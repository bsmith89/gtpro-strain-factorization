{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpyro as npyro\n",
    "import numpyro.distributions as npyro_dist\n",
    "import jax\n",
    "\n",
    "#npyro.enable_x64(True)\n",
    "\n",
    "def model(\n",
    "    s,\n",
    "    m,\n",
    "    gamma_hyper=1.0,\n",
    "    pi_hyper=1.0,\n",
    "    rho_hyper=1.0,\n",
    "    epsilon_hyper=0.01,\n",
    "    alpha_hyper=1000.0,\n",
    "    y=None,\n",
    "):\n",
    "\n",
    "    n, g = m.shape\n",
    "\n",
    "    with npyro.plate(\"position\", g, dim=-1):\n",
    "        with npyro.plate(\"strain\", s, dim=-2):\n",
    "            gamma = npyro.sample(\"gamma\", npyro_dist.Beta(gamma_hyper, gamma_hyper))\n",
    "    # gamma.shape == (s, g)\n",
    "\n",
    "    rho = npyro.sample(\n",
    "        \"rho\",\n",
    "        npyro_dist.Dirichlet(jnp.ones(s) * rho_hyper),\n",
    "    )\n",
    "\n",
    "    with npyro.plate(\"sample\", n, dim=-1):\n",
    "        pi = npyro.sample(\"pi\", npyro_dist.Dirichlet(rho * s * pi_hyper))\n",
    "        alpha = npyro.sample(\"alpha\", npyro_dist.Gamma(alpha_hyper, 1.0)).reshape(\n",
    "            (-1, 1)\n",
    "        )\n",
    "        epsilon = npyro.sample(\n",
    "            \"epsilon\", npyro_dist.Beta(1.0, 1 / epsilon_hyper)\n",
    "        ).reshape((-1, 1))\n",
    "    # pi.shape == (n, s)\n",
    "    # alpha.shape == epsilon.shape == (n,)\n",
    "\n",
    "    p_noerr = npyro.deterministic(\"p_noerr\", pi @ gamma)\n",
    "    p = npyro.deterministic(\n",
    "        \"p\", (1 - epsilon / 2) * (p_noerr) + (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    # p.shape == (n, g)\n",
    "\n",
    "    y = npyro.sample(\n",
    "        \"y\",\n",
    "        npyro_dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m,\n",
    "        ),\n",
    "        obs=y\n",
    "    )\n",
    "    # y.shape == (n, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_m = 100 * jnp.ones((10, 20), dtype=int)\n",
    "\n",
    "prior = partial(\n",
    "    model,\n",
    "    s=4, m=sim_m, gamma_hyper=1e-2, pi_hyper=0.1, rho_hyper=0.5\n",
    ")\n",
    "\n",
    "rng0 = jax.random.PRNGKey(1)\n",
    "sim = (\n",
    "    npyro.infer.Predictive(prior, num_samples=1)\n",
    "    (rng0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim['pi'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim['gamma'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim['y']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "posterior = npyro.handlers.condition(\n",
    "    prior(y=sim['y']),\n",
    "    data=dict(y=sim['y'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from this source of randomness. We will split keys for subsequent operations.\n",
    "rng1, rng2 = jax.random.split(rng0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_warmup, num_samples = 1000, 2000\n",
    "\n",
    "# Run NUTS.\n",
    "kernel = npyro.infer.NUTS(posterior)\n",
    "mcmc = npyro.infer.MCMC(kernel, num_warmup, num_samples)\n",
    "mcmc.run(rng1)\n",
    "mcmc.print_summary()\n",
    "samples_1 = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "posterior_pred = (\n",
    "    anpyro.infer.Predictive(prior, posterior_samples=samples_1)\n",
    "    (jax.random.PRNGKey(1), m=sim_m)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sim['y'].squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "posterior_pred['y'].squeeze()[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(samples_1['pi'].mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import autoguide\n",
    "from jax import lax\n",
    "\n",
    "guide = npyro.infer.autoguide.AutoDiagonalNormal(prior)\n",
    "opt = npyro.optim.ClippedAdam(step_size=0.1, clip_norm=1000.)\n",
    "svi = npyro.infer.SVI(prior, guide, opt, loss=npyro.infer.Trace_ELBO(), y=sim['y'])\n",
    "\n",
    "svi_init = svi.init(rng1)\n",
    "svi_state = svi_init\n",
    "# svi_state, losses = lax.scan(lambda state, i: svi.update(state), svi_state, jnp.arange(2000))\n",
    "\n",
    "\n",
    "pbar = tqdm(range(2000))\n",
    "history = []\n",
    "delta_history = []\n",
    "# trace_epsilon_interval = []\n",
    "# trace_gamma_a = []\n",
    "# trace_gamma_b = []\n",
    "# trace_gamma_loc = []\n",
    "# trace_alpha_log = []\n",
    "# trace_pi_simplex = []\n",
    "for i in pbar:\n",
    "    svi_state, elbo = svi.update(svi_state)\n",
    "    \n",
    "    if np.isnan(elbo):\n",
    "        break\n",
    "\n",
    "    # Fit tracking\n",
    "    history.append(elbo)\n",
    "    \n",
    "    # Reporting/Breaking\n",
    "    if (i % 1 == 0):\n",
    "        if i > 1:\n",
    "            pbar.set_postfix({'ELBO': history[-1], 'delta': history[-2] - history[-1]})\n",
    "#         trace_epsilon_interval.append(pyro.get_param_store()['epsilon_interval'].detach().numpy().copy())\n",
    "#         trace_gamma_a.append(pyro.get_param_store()['gamma_a'].detach().numpy().copy())\n",
    "#         trace_gamma_b.append(pyro.get_param_store()['gamma_b'].detach().numpy().copy())\n",
    "# #         trace_gamma_loc.append(pyro.get_param_store()['gamma_loc'].detach().numpy().copy())\n",
    "#         trace_alpha_log.append(pyro.get_param_store()['alpha_log'].detach().numpy().copy())\n",
    "#         trace_pi_simplex.append(pyro.get_param_store()['pi_simplex'].detach().numpy().copy())\n",
    "#     if np.mean(delta_history[-1000:]) < 0.0001:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide(svi_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide(svi.init(rng1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide(svi_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.sample_posterior(rng0, guide.get_transform(svi_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_point = npyro.infer.Predictive(posterior, guide=svi_state, num_samples=1, return_sites=['pi', 'gamma'])\n",
    "mapest = {k: v\n",
    "                 for k, v\n",
    "                 in svi_point(rng1).items()}\n",
    "#posterior_predictive = svi_predictive()['y']\n",
    "\n",
    "#fit_pi = fit_pi.rename(columns=lambda i: f\"fit_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim['gamma'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(mapest['gamma'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = pyro.infer.autoguide.AutoNormal(model_fit, )\n",
    "\n",
    "opt = pyro.optim.Adamax({\"lr\": 1e-1}, {\"clip_norm\": 100.})\n",
    "#opt = pyro.optim.RMSprop({\"lr\": 0.001})\n",
    "\n",
    "svi = pyro.infer.SVI(\n",
    "    model_fit,\n",
    "    _guide,\n",
    "    opt,\n",
    "    loss=pyro.infer.JitTrace_ELBO()\n",
    ")\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "pbar = tqdm(range(10000))\n",
    "history = []\n",
    "delta_history = []\n",
    "# trace_epsilon_interval = []\n",
    "# trace_gamma_a = []\n",
    "# trace_gamma_b = []\n",
    "# trace_gamma_loc = []\n",
    "# trace_alpha_log = []\n",
    "# trace_pi_simplex = []\n",
    "for i in pbar:\n",
    "    elbo = svi.step(\n",
    "        y=y_obs,\n",
    "    )\n",
    "    \n",
    "    if np.isnan(elbo):\n",
    "        break\n",
    "\n",
    "    # Fit tracking\n",
    "    history.append(elbo)\n",
    "    \n",
    "    # Reporting/Breaking\n",
    "    if (i % 1 == 0):\n",
    "        if i > 1:\n",
    "            pbar.set_postfix({'ELBO': history[-1], 'delta': history[-2] - history[-1]})\n",
    "#         trace_epsilon_interval.append(pyro.get_param_store()['epsilon_interval'].detach().numpy().copy())\n",
    "#         trace_gamma_a.append(pyro.get_param_store()['gamma_a'].detach().numpy().copy())\n",
    "#         trace_gamma_b.append(pyro.get_param_store()['gamma_b'].detach().numpy().copy())\n",
    "# #         trace_gamma_loc.append(pyro.get_param_store()['gamma_loc'].detach().numpy().copy())\n",
    "#         trace_alpha_log.append(pyro.get_param_store()['alpha_log'].detach().numpy().copy())\n",
    "#         trace_pi_simplex.append(pyro.get_param_store()['pi_simplex'].detach().numpy().copy())\n",
    "#     if np.mean(delta_history[-1000:]) < 0.0001:\n",
    "#         break\n",
    "\n",
    "        \n",
    "pbar.refresh()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %load scripts/strain_facts.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.util import info, idxwhere\n",
    "import numpy as np\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def rss(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    q = 1 - p\n",
    "    return -p * np.log2(p) - q * np.log2(q)\n",
    "\n",
    "\n",
    "def genotype_distance(x, y):\n",
    "    x = x * 2 - 1\n",
    "    y = y * 2 - 1\n",
    "    return ((x * y) ** 2 * ((x - y) / 2) ** 2).mean()\n",
    "\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    min_loss = loss_history.min()\n",
    "    plt.plot(loss_history - min_loss)\n",
    "    plt.plot(\n",
    "        np.linspace(0, len(loss_history), num=1000),\n",
    "        np.linspace(len(loss_history), 0, num=1000),\n",
    "        lw=1,\n",
    "        linestyle=\"--\",\n",
    "        color=\"grey\",\n",
    "    )\n",
    "    plt.title(f\"+{min_loss:0.3e}\")\n",
    "    plt.yscale(\"log\")\n",
    "    return plt.gca()\n",
    "\n",
    "\n",
    "def mean_residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    out = np.abs(((frac_obs - expect_frac)))\n",
    "    out[np.isnan(out)] = 0\n",
    "    return (out * m).sum() / m.sum()\n",
    "\n",
    "\n",
    "def _load_input_data(allpaths):\n",
    "    data = []\n",
    "    for path in allpaths:\n",
    "        info(path)\n",
    "        d = xr.open_dataarray(path).squeeze()\n",
    "        info(f\"Shape: {d.sizes}.\")\n",
    "        data.append(d)\n",
    "    info(\"Concatenating data from {} files.\".format(len(data)))\n",
    "    data = xr.concat(data, \"library_id\", fill_value=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def select_informative_positions(data, incid_thresh):\n",
    "    minor_allele_incid = (data > 0).mean(\"library_id\").min(\"allele\")\n",
    "    informative_positions = idxwhere(\n",
    "        minor_allele_incid.to_series() > incid_thresh\n",
    "    )\n",
    "    return informative_positions\n",
    "\n",
    "\n",
    "def model(\n",
    "    s,\n",
    "    m,\n",
    "    y=None,\n",
    "    gamma_hyper=1.0,\n",
    "    pi0=1.0,\n",
    "    rho0=1.0,\n",
    "    epsilon0=0.01,\n",
    "    alpha0=1000.0,\n",
    "    dtype=torch.float32,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "\n",
    "    # Cast inputs and set device\n",
    "    m, gamma_hyper, pi0, rho0, epsilon0, alpha0 = [\n",
    "        torch.tensor(v, dtype=dtype, device=device)\n",
    "        for v in [m, gamma_hyper, pi0, rho0, epsilon0, alpha0]\n",
    "    ]\n",
    "    if y is not None:\n",
    "        y = torch.tensor(y)\n",
    "\n",
    "    n, g = m.shape\n",
    "\n",
    "    with pyro.plate(\"position\", g, dim=-1):\n",
    "        with pyro.plate(\"strain\", s, dim=-2):\n",
    "            gamma = pyro.sample(\"gamma\", dist.Beta(gamma_hyper, gamma_hyper))\n",
    "    # gamma.shape == (s, g)\n",
    "\n",
    "    rho_hyper = pyro.sample(\"rho_hyper\", dist.Gamma(rho0, 1.0))\n",
    "    rho = pyro.sample(\n",
    "        \"rho\",\n",
    "        dist.Dirichlet(torch.ones(s, dtype=dtype, device=device) * rho_hyper),\n",
    "    )\n",
    "\n",
    "    epsilon_hyper = pyro.sample(\"epsilon_hyper\", dist.Beta(1.0, 1 / epsilon0))\n",
    "    alpha_hyper = pyro.sample(\"alpha_hyper\", dist.Gamma(alpha0, 1.0))\n",
    "    pi_hyper = pyro.sample(\"pi_hyper\", dist.Gamma(pi0, 1.0))\n",
    "\n",
    "    with pyro.plate(\"sample\", n, dim=-1):\n",
    "        pi = pyro.sample(\"pi\", dist.Dirichlet(rho * s * pi_hyper))\n",
    "        alpha = pyro.sample(\"alpha\", dist.Gamma(alpha_hyper, 1.0)).unsqueeze(\n",
    "            -1\n",
    "        )\n",
    "        epsilon = pyro.sample(\n",
    "            \"epsilon\", dist.Beta(1.0, 1 / epsilon_hyper)\n",
    "        ).unsqueeze(-1)\n",
    "    # pi.shape == (n, s)\n",
    "    # alpha.shape == epsilon.shape == (n,)\n",
    "\n",
    "    p_noerr = pyro.deterministic(\"p_noerr\", pi @ gamma)\n",
    "    p = pyro.deterministic(\n",
    "        \"p\", (1 - epsilon / 2) * (p_noerr) + (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    # p.shape == (n, g)\n",
    "\n",
    "    y = pyro.sample(\n",
    "        \"y\",\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m,\n",
    "        ),\n",
    "        obs=y,\n",
    "    )\n",
    "    # y.shape == (n, g)\n",
    "    return y\n",
    "\n",
    "\n",
    "def conditioned_model(\n",
    "    model, data={}, dtype=torch.float32, device=\"cpu\", **kwargs,\n",
    "):\n",
    "    data = {\n",
    "        k: torch.tensor(v, dtype=dtype, device=device) for k, v in data.items()\n",
    "    }\n",
    "    return partial(\n",
    "        pyro.condition(model, data=data), dtype=dtype, device=device, **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def find_map(\n",
    "    model,\n",
    "    lag=10,\n",
    "    stop_at=1.0,\n",
    "    max_iter=int(1e5),\n",
    "    learning_rate=1e-0,\n",
    "    clip_norm=100.0,\n",
    "):\n",
    "    guide = pyro.infer.autoguide.AutoLaplaceApproximation(model)\n",
    "    svi = pyro.infer.SVI(\n",
    "        model,\n",
    "        guide,\n",
    "        pyro.optim.Adamax(\n",
    "            optim_args={\"lr\": learning_rate},\n",
    "            clip_args={\"clip_norm\": clip_norm},\n",
    "        ),\n",
    "        loss=pyro.infer.JitTrace_ELBO(),\n",
    "    )\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    pbar = tqdm(range(max_iter), position=0, leave=True)\n",
    "    history = []\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                break\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if i < 2:\n",
    "                pbar.set_postfix({\"ELBO\": history[-1]})\n",
    "            elif i < lag + 1:\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"ELBO\": history[-1],\n",
    "                        \"delta_1\": history[-2] - history[-1],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                delta_lag = (history[-lag] - history[-1]) / lag\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"ELBO\": history[-1],\n",
    "                        \"delta_1\": history[-2] - history[-1],\n",
    "                        f\"delta_{lag}\": delta_lag,\n",
    "                    }\n",
    "                )\n",
    "                if delta_lag < stop_at:\n",
    "                    info(\"Optimization converged\")\n",
    "                    break\n",
    "    except KeyboardInterrupt:\n",
    "        info(\"Optimization interrupted\")\n",
    "    pbar.refresh()\n",
    "    assert delta_lag < stop_at, (\n",
    "        f\"Reached {args.max_iter} iterations with a per-step improvement of \"\n",
    "        f\"{args.delta_lag}. Consider setting --max-iter \"\n",
    "        f\"or --stop-at larger; increasing --learning-rate may also help, \"\n",
    "        f\"although it could also lead to numerical issues.\"\n",
    "    )\n",
    "    # Gather MAP from parameter-store\n",
    "    mapest = {\n",
    "        k: v.detach().cpu().numpy().squeeze()\n",
    "        for k, v in pyro.infer.Predictive(\n",
    "            model, guide=guide, num_samples=1\n",
    "        )().items()\n",
    "    }\n",
    "    return mapest, np.array(history)\n",
    "\n",
    "\n",
    "def parse_args(argv):\n",
    "    p = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    # Input\n",
    "    p.add_argument(\n",
    "        \"pileup\",\n",
    "        nargs=\"+\",\n",
    "        help=\"\"\"\n",
    "Single, fully processed, pileup table in NetCDF format\n",
    "with the following dimensions:\n",
    "    * library_id\n",
    "    * position\n",
    "    * allele\n",
    "                        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Shape of the model\n",
    "    p.add_argument(\"--nstrains\", metavar=\"INT\", type=int, default=1000)\n",
    "    p.add_argument(\n",
    "        \"--npos\",\n",
    "        metavar=\"INT\",\n",
    "        default=2000,\n",
    "        type=int,\n",
    "        help=(\"Number of positions to sample for model fitting.\"),\n",
    "    )\n",
    "\n",
    "    # Data filtering\n",
    "    p.add_argument(\n",
    "        \"--incid-thresh\",\n",
    "        metavar=\"FLOAT\",\n",
    "        type=float,\n",
    "        default=0.02,\n",
    "        help=(\n",
    "            \"Minimum fraction of samples that must have the minor allele \"\n",
    "            \"for the position to be considered 'informative'.\"\n",
    "        ),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--cvrg-thresh\",\n",
    "        metavar=\"FLOAT\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=(\n",
    "            \"Minimum fraction of 'informative' positions with counts \"\n",
    "            \"necessary for sample to be included.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Regularization\n",
    "    p.add_argument(\n",
    "        \"--gamma-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e-2,\n",
    "        type=float,\n",
    "        help=(\"Ambiguity regularization parameter.\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--pi-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e-1,\n",
    "        type=float,\n",
    "        help=(\n",
    "            \"Heterogeneity regularization parameter (will be scaled by 1 / s).\"\n",
    "        ),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--rho-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e0,\n",
    "        type=float,\n",
    "        help=(\"Diversity regularization parameter.\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--epsilon-hyper\", metavar=\"FLOAT\", default=0.01, type=float\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--epsilon\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=None,\n",
    "        type=float,\n",
    "        help=(\"Fixed error rate for all samples.\"),\n",
    "    )\n",
    "    p.add_argument(\"--alpha-hyper\", metavar=\"FLOAT\", default=100.0, type=float)\n",
    "    p.add_argument(\n",
    "        \"--alpha\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=None,\n",
    "        type=float,\n",
    "        help=(\"Fixed concentration for all samples.\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--collapse\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=0.0,\n",
    "        type=float,\n",
    "        help=(\"Merge strains with a cosine distance of less than this value.\"),\n",
    "    )\n",
    "\n",
    "    # Fitting\n",
    "    p.add_argument(\"--random-seed\", default=0, type=int, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--max-iter\", default=10000, type=int, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--lag\", default=50, type=int, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--stop-at\", default=5.0, type=float, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--learning-rate\", default=1e-0, type=float, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--clip-norm\", default=100.0, type=float, help=(\"FIXME\"))\n",
    "\n",
    "    # Hardware\n",
    "    p.add_argument(\"--device\", default=\"cpu\", help=(\"PyTorch device name.\"))\n",
    "\n",
    "    # Output\n",
    "    p.add_argument(\n",
    "        \"--outpath\",\n",
    "        metavar=\"PATH\",\n",
    "        help=(\"Path for genotype fraction output.\"),\n",
    "    )\n",
    "\n",
    "    args = p.parse_args(argv)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\n",
    "        \"error\",\n",
    "        message=\"Encountered NaN: loss\",\n",
    "        category=UserWarning,\n",
    "        # module=\"trace_elbo\",  # FIXME: What is the correct regex for module?\n",
    "        lineno=217,\n",
    "    )\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"CUDA initialization: Found no NVIDIA\",\n",
    "        category=UserWarning,\n",
    "        lineno=130,\n",
    "    )\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"torch.tensor results are registered as constants\",\n",
    "        category=torch.jit.TracerWarning,\n",
    "        # module=\"trace_elbo\",  # FIXME: What is the correct regex for module?\n",
    "        lineno=95,\n",
    "    )\n",
    "\n",
    "    args = parse_args(sys.argv[1:])\n",
    "    info(args)\n",
    "\n",
    "    info(f\"Setting random seed: {args.random_seed}\")\n",
    "    np.random.seed(args.random_seed)\n",
    "\n",
    "    info(\"Loading input data.\")\n",
    "    data = _load_input_data(args.pileup)\n",
    "    info(f\"Full data shape: {data.sizes}.\")\n",
    "\n",
    "    info(\"Filtering positions.\")\n",
    "    informative_positions = select_informative_positions(\n",
    "        data, args.incid_thresh\n",
    "    )\n",
    "    npos_available = len(informative_positions)\n",
    "    info(\n",
    "        f\"Found {npos_available} informative positions with minor \"\n",
    "        f\"allele incidence of >{args.incid_thresh}\"\n",
    "    )\n",
    "    npos = min(args.npos, npos_available)\n",
    "    info(f\"Randomly sampling {npos} positions.\")\n",
    "    position_ss = np.random.choice(\n",
    "        informative_positions, size=npos, replace=False,\n",
    "    )\n",
    "\n",
    "    info(\"Filtering libraries.\")\n",
    "    suff_cvrg_samples = idxwhere(\n",
    "        (\n",
    "            (\n",
    "                data.sel(position=informative_positions).sum([\"allele\"]) > 0\n",
    "            ).mean(\"position\")\n",
    "            > args.cvrg_thresh\n",
    "        ).to_series()\n",
    "    )\n",
    "    nlibs = len(suff_cvrg_samples)\n",
    "    info(\n",
    "        f\"Found {nlibs} libraries with >{args.cvrg_thresh:0.1%} \"\n",
    "        f\"of informative positions covered.\"\n",
    "    )\n",
    "\n",
    "    info(\"Constructing input data.\")\n",
    "    data_fit = data.sel(library_id=suff_cvrg_samples, position=position_ss)\n",
    "    m_ss = data_fit.sum(\"allele\")\n",
    "    n, g_ss = m_ss.shape\n",
    "    y_obs_ss = data_fit.sel(allele=\"alt\")\n",
    "    s = args.nstrains\n",
    "    info(f\"Model shape: n={n}, g={g_ss}, s={s}\")\n",
    "\n",
    "    info(\n",
    "        \"Fitting intial strain fractions and genotypes with ambiguity \"\n",
    "        \"regularization.\"\n",
    "    )\n",
    "    conditioning_data = dict(\n",
    "        alpha_hyper=args.alpha_hyper,\n",
    "        epsilon_hyper=args.epsilon_hyper,\n",
    "        pi_hyper=args.pi_hyper / s,\n",
    "        rho_hyper=args.rho_hyper,\n",
    "        y=y_obs_ss.values,\n",
    "    )\n",
    "    if args.alpha:\n",
    "        conditioning_data[\"alpha\"] = np.ones(n) * args.alpha\n",
    "    if args.epsilon:\n",
    "        conditioning_data[\"epsilon\"] = np.ones(n) * args.epsilon\n",
    "    info(f\"Conditioning model on: {conditioning_data.keys()}\")\n",
    "\n",
    "    model_fit = conditioned_model(\n",
    "        model,\n",
    "        data=conditioning_data,\n",
    "        s=s,\n",
    "        m=m_ss.values,\n",
    "        gamma_hyper=args.gamma_hyper,\n",
    "        dtype=torch.float32,\n",
    "        device=args.device,\n",
    "    )\n",
    "\n",
    "    info(\"Optimizing model parameters.\")\n",
    "    mapest1, history1 = find_map(\n",
    "        model_fit,\n",
    "        lag=args.lag,\n",
    "        stop_at=args.stop_at,\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_iter=args.max_iter,\n",
    "        clip_norm=args.clip_norm,\n",
    "    )\n",
    "    if args.device.startswith(\"cuda\"):\n",
    "        info(\n",
    "            \"CUDA available mem: {}\".format(\n",
    "                torch.cuda.get_device_properties(0).total_memory\n",
    "            ),\n",
    "        )\n",
    "        info(\"CUDA reserved mem: {}\".format(torch.cuda.memory_reserved(0)))\n",
    "        info(\"CUDA allocated mem: {}\".format(torch.cuda.memory_allocated(0)))\n",
    "        info(\n",
    "            \"CUDA free mem: {}\".format(\n",
    "                torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)\n",
    "            )\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    info(\n",
    "        \"Re-fitting genotypes, this time without ambiguity regularization, \"\n",
    "        \"conditioned on strain fractions from Round 1.\"\n",
    "    )\n",
    "    conditioning_data = dict(\n",
    "        alpha_hyper=args.alpha_hyper,\n",
    "        epsilon_hyper=args.epsilon_hyper,\n",
    "        pi=mapest1[\"pi\"],\n",
    "        y=y_obs_ss.values,\n",
    "    )\n",
    "    if args.alpha:\n",
    "        conditioning_data[\"alpha\"] = np.ones(n) * args.alpha\n",
    "    if args.epsilon:\n",
    "        conditioning_data[\"epsilon\"] = np.ones(n) * args.epsilon\n",
    "    info(f\"Conditioning model on: {conditioning_data.keys()}\")\n",
    "\n",
    "    model_fit = conditioned_model(\n",
    "        model,\n",
    "        data=conditioning_data,\n",
    "        s=s,\n",
    "        m=m_ss.values,\n",
    "        gamma_hyper=1e0,\n",
    "        dtype=torch.float32,\n",
    "        device=args.device,\n",
    "    )\n",
    "\n",
    "    info(\"Optimizing model parameters.\")\n",
    "    mapest2, history2 = find_map(\n",
    "        model_fit,\n",
    "        lag=args.lag,\n",
    "        stop_at=args.stop_at,\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_iter=args.max_iter,\n",
    "        clip_norm=args.clip_norm,\n",
    "    )\n",
    "    if args.device.startswith(\"cuda\"):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if args.collapse > 0:\n",
    "        # Collapse genotypes\n",
    "        info(\n",
    "            f\"Collapsing genotype clusters with maximum-distances \"\n",
    "            f\"<{args.collapse}.\"\n",
    "        )\n",
    "        info(f\"See the definition of the function: 'genotype_distance'.\")\n",
    "        info(\n",
    "            \"Calculating strain distances. \"\n",
    "            \"This can take a while for large numbers of strains.\"\n",
    "        )\n",
    "        geno_dist = squareform(\n",
    "            pdist(mapest2[\"gamma\"], metric=genotype_distance)\n",
    "        )\n",
    "\n",
    "        info(\"Clustering.\")\n",
    "        clust = pd.Series(\n",
    "            AgglomerativeClustering(\n",
    "                n_clusters=None,\n",
    "                affinity=\"precomputed\",\n",
    "                linkage=\"complete\",\n",
    "                distance_threshold=args.collapse,\n",
    "            )\n",
    "            .fit(geno_dist)\n",
    "            .labels_\n",
    "        )\n",
    "        pi_collapse = (\n",
    "            pd.DataFrame(mapest2[\"pi\"])\n",
    "            .groupby(clust, axis=\"columns\")\n",
    "            .sum()\n",
    "            .values\n",
    "        )\n",
    "        s_collapse = pi_collapse.shape[1]\n",
    "        info(f\"Collapsed {s} genotypes into {s_collapse}.\")\n",
    "\n",
    "        # Re-fit genotypes based on collapsed strain abundances in order\n",
    "        # to fully integrate abundances details into genotypes.\n",
    "        info(\"Fitting genotypes based on collapsed strain fractions.\")\n",
    "        conditioning_data = dict(\n",
    "            alpha_hyper=args.alpha_hyper,\n",
    "            epsilon_hyper=args.epsilon_hyper,\n",
    "            pi=pi_collapse,\n",
    "            y=y_obs_ss.values,\n",
    "        )\n",
    "        if args.alpha:\n",
    "            conditioning_data[\"alpha\"] = np.ones(n) * args.alpha\n",
    "        if args.epsilon:\n",
    "            conditioning_data[\"epsilon\"] = np.ones(n) * args.epsilon\n",
    "        info(f\"Conditioning model on: {conditioning_data.keys()}\")\n",
    "\n",
    "        model_fit = conditioned_model(\n",
    "            model,\n",
    "            data=conditioning_data,\n",
    "            s=s_collapse,\n",
    "            m=m_ss.values,\n",
    "            gamma_hyper=1e0,\n",
    "            dtype=torch.float32,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "        info(\"Optimizing model parameters.\")\n",
    "        mapest3, history3 = find_map(\n",
    "            model_fit,\n",
    "            lag=args.lag,\n",
    "            stop_at=args.stop_at,\n",
    "            learning_rate=args.learning_rate,\n",
    "            max_iter=args.max_iter,\n",
    "            clip_norm=args.clip_norm,\n",
    "        )\n",
    "        if args.device.startswith(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    info(\"Finished fitting model.\")\n",
    "    result = xr.Dataset(\n",
    "        {\n",
    "            \"gamma\": ([\"strain\", \"position\"], mapest3[\"gamma\"]),\n",
    "            \"rho\": ([\"strain\"], mapest3[\"rho\"]),\n",
    "            \"alpha_hyper\": ([], mapest3[\"alpha_hyper\"]),\n",
    "            \"pi\": ([\"library_id\", \"strain\"], mapest3[\"pi\"]),\n",
    "            \"epsilon\": ([\"library_id\"], mapest3[\"epsilon\"]),\n",
    "            \"rho_hyper\": ([], mapest3[\"rho_hyper\"]),\n",
    "            \"epsilon_hyper\": ([], mapest3[\"epsilon_hyper\"]),\n",
    "            \"pi_hyper\": ([], mapest3[\"pi_hyper\"]),\n",
    "            \"alpha\": ([\"library_id\"], mapest3[\"alpha\"]),\n",
    "            \"p_noerr\": ([\"library_id\", \"position\"], mapest3[\"p_noerr\"]),\n",
    "            \"p\": ([\"library_id\", \"position\"], mapest3[\"p\"]),\n",
    "            \"y\": ([\"library_id\", \"position\"], y_obs_ss),\n",
    "            \"m\": ([\"library_id\", \"position\"], m_ss),\n",
    "            \"elbo_trace\": ([\"iteration\"], history1),\n",
    "        },\n",
    "        coords=dict(\n",
    "            strain=np.arange(s_collapse),\n",
    "            position=data_fit.position,\n",
    "            library_id=data_fit.library_id,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if args.outpath:\n",
    "        info(\"Saving results.\")\n",
    "        result.to_netcdf(\n",
    "            args.outpath,\n",
    "            encoding=dict(\n",
    "                gamma=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                pi=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                p_noerr=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                p=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                y=dict(dtype=\"uint16\", zlib=True, complevel=6),\n",
    "                m=dict(dtype=\"uint16\", zlib=True, complevel=6),\n",
    "                elbo_trace=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}