{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/__init__.py\n",
    "\n",
    "from sfacts.logging_util import *\n",
    "from sfacts.pyro_util import *\n",
    "from sfacts.metagenotype_model import *\n",
    "from sfacts.genotype import *\n",
    "from sfacts.plot import *\n",
    "from sfacts.estimation import *\n",
    "from sfacts.evaluation import *\n",
    "from sfacts.workflow import *\n",
    "\n",
    "gamma_to_sign = prob_to_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/logging_util.py\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "\n",
    "def info(*msg):\n",
    "    now = datetime.now()\n",
    "    print(f'[{now}]', *msg, file=sys.stderr, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyro_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/pyro_util.py\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "\n",
    "def as_torch(x, dtype=None, device=None):\n",
    "    # Cast inputs and set device\n",
    "    return torch.tensor(x, dtype=dtype, device=device)\n",
    "\n",
    "def all_torch(dtype=None, device=None, **kwargs):\n",
    "    # Cast inputs and set device\n",
    "    return {k: as_torch(kwargs[k], dtype=dtype, device=device) for k in kwargs}\n",
    "\n",
    "def shape_info(model, *args, **kwargs):\n",
    "    _trace = pyro.poutine.trace(model).get_trace(*args, **kwargs)\n",
    "    _trace.compute_log_prob()\n",
    "    info(_trace.format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metagenotype_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/metagenotype_model.py\n",
    "\n",
    "from sfacts.pyro_util import as_torch, all_torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def NegativeBinomialReparam(mu, r, eps=1e-5):\n",
    "    p = torch.clamp(1. / ((r / mu) + 1.), min=eps, max=1. - eps)\n",
    "    return dist.NegativeBinomial(\n",
    "        total_count=r,\n",
    "        probs=p\n",
    "    )\n",
    "\n",
    "def model(\n",
    "    n,\n",
    "    g,\n",
    "    s,\n",
    "    gamma_hyper=1.,\n",
    "    delta_hyper_temp=0.1,\n",
    "    delta_hyper_p=0.9,\n",
    "    rho_hyper=1.,\n",
    "    pi_hyper=1.,\n",
    "    alpha_hyper_hyper_mean=100.,\n",
    "    alpha_hyper_hyper_scale=1.,\n",
    "    alpha_hyper_scale=0.5,\n",
    "    epsilon_hyper_alpha=1.5,\n",
    "    epsilon_hyper_beta=1.5 / 0.01,\n",
    "    mu_hyper_mean=1.,\n",
    "    mu_hyper_scale=1.,\n",
    "    m_hyper_r=1.,\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "):\n",
    "    \n",
    "    (\n",
    "        gamma_hyper,\n",
    "        delta_hyper_temp,\n",
    "        delta_hyper_p,\n",
    "        rho_hyper,\n",
    "        pi_hyper,\n",
    "        alpha_hyper_hyper_mean,\n",
    "        alpha_hyper_hyper_scale,\n",
    "        alpha_hyper_scale,\n",
    "        epsilon_hyper_alpha,\n",
    "        epsilon_hyper_beta,\n",
    "        mu_hyper_mean,\n",
    "        mu_hyper_scale,\n",
    "        m_hyper_r,\n",
    "    ) = (\n",
    "        as_torch(x, dtype=dtype, device=device)\n",
    "        for x in [\n",
    "            gamma_hyper,\n",
    "            delta_hyper_temp,\n",
    "            delta_hyper_p,\n",
    "            rho_hyper,\n",
    "            pi_hyper,\n",
    "            alpha_hyper_hyper_mean,\n",
    "            alpha_hyper_hyper_scale,\n",
    "            alpha_hyper_scale,\n",
    "            epsilon_hyper_alpha,\n",
    "            epsilon_hyper_beta,\n",
    "            mu_hyper_mean,\n",
    "            mu_hyper_scale,\n",
    "            m_hyper_r,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Genotypes\n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.RelaxedBernoulli(temperature=gamma_hyper, logits=torch.zeros((1,), dtype=dtype, device=device).squeeze())\n",
    "            )\n",
    "            # Position presence/absence\n",
    "            delta = pyro.sample(\n",
    "                'delta', dist.RelaxedBernoulli(temperature=delta_hyper_temp, probs=delta_hyper_p)\n",
    "            )\n",
    "    \n",
    "    # Meta-community composition\n",
    "    rho = pyro.sample('rho', dist.Dirichlet(rho_hyper * torch.ones(s, dtype=dtype, device=device)))\n",
    "\n",
    "    alpha_hyper_mean = pyro.sample('alpha_hyper_mean', dist.LogNormal(loc=torch.log(alpha_hyper_hyper_mean), scale=alpha_hyper_hyper_scale))\n",
    "    alpha_hyper_scale = pyro.sample('alpha_hyper_scale', dist.LogNormal(loc=0, scale=1))\n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        # Community composition\n",
    "        pi = pyro.sample('pi', dist.RelaxedOneHotCategorical(temperature=pi_hyper, probs=rho))\n",
    "        # Sample coverage\n",
    "        mu = pyro.sample('mu', dist.LogNormal(loc=torch.log(mu_hyper_mean), scale=mu_hyper_scale))\n",
    "        # Sequencing error\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta)).unsqueeze(-1)\n",
    "        alpha = pyro.sample('alpha', dist.LogNormal(loc=torch.log(alpha_hyper_mean), scale=alpha_hyper_scale)).unsqueeze(-1)\n",
    "        \n",
    "    # Depth at each position\n",
    "    nu = pyro.deterministic(\"nu\", pi @ delta)\n",
    "    m = pyro.sample('m', NegativeBinomialReparam(nu * mu.reshape((-1,1)), m_hyper_r).to_event())\n",
    "\n",
    "    # Expected fractions of each allele at each position\n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ (gamma * delta) / nu)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    \n",
    "    # Observation\n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ).to_event(),\n",
    "    )\n",
    "    \n",
    "def condition_model(model, data=None, device='cpu', dtype=torch.float32, **model_kwargs):\n",
    "    if data is None:\n",
    "        data = {}\n",
    "        \n",
    "    conditioned_model = partial(\n",
    "        pyro.condition(\n",
    "            model,\n",
    "            data=all_torch(**data, dtype=dtype, device=device),\n",
    "        ),\n",
    "        **model_kwargs,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    return conditioned_model\n",
    "    \n",
    "def simulate(model):\n",
    "    obs = pyro.infer.Predictive(model, num_samples=1)()\n",
    "    obs = {\n",
    "        k: obs[k].detach().cpu().numpy().squeeze()\n",
    "        for k in obs.keys()\n",
    "    }\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/genotype.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import scipy as sp\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prob_to_sign(gamma):\n",
    "    return gamma * 2 - 1\n",
    "\n",
    "def genotype_distance(x, y):\n",
    "    x = prob_to_sign(x)\n",
    "    y = prob_to_sign(y)\n",
    "    dist = ((x - y) / 2) ** 2\n",
    "    weight = (x * y) ** 2\n",
    "    wmean_dist = ((weight * dist).mean()) / ((weight.mean()))\n",
    "    return wmean_dist\n",
    "\n",
    "def sign_genotype_distance(x, y):\n",
    "    dist = ((x - y) / 2) ** 2\n",
    "    weight = (x * y) ** 2\n",
    "    wmean_dist = ((weight * dist).mean()) / ((weight.mean()))\n",
    "    return wmean_dist\n",
    "\n",
    "def genotype_pdist(gamma, progress=False):\n",
    "    metric = sign_genotype_distance\n",
    "    X = np.asarray(gamma * 2 - 1)\n",
    "    m = X.shape[0]\n",
    "    dm = np.empty((m * (m - 1)) // 2)\n",
    "    k = 0\n",
    "    with tqdm(total=len(dm), disable=(not progress)) as pbar:\n",
    "        for i in range(0, m - 1):\n",
    "            for j in range(i + 1, m):\n",
    "                dm[k] = metric(X[i], X[j])\n",
    "                k = k + 1\n",
    "                pbar.update()\n",
    "    return dm\n",
    "    \n",
    "\n",
    "def counts_to_p_estimate(y, m, pseudo=1):\n",
    "    return (y + pseudo) / (m + pseudo * 2)\n",
    "\n",
    "def genotype_linkage(gamma, progress=False, **kwargs):\n",
    "    dmat = genotype_pdist(gamma, progress=progress)\n",
    "    kw = dict(method='complete')\n",
    "    kw.update(kwargs)\n",
    "    return linkage(dmat, **kw), dmat\n",
    "\n",
    "def mask_missing_genotype(gamma, delta):\n",
    "    return sp.special.expit(sp.special.logit(gamma) * delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/plot.py\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sfacts.genotype import genotype_linkage, prob_to_sign\n",
    "from scipy.spatial.distance import squareform\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_clustermap_dims(nx, ny, scalex=0.15, scaley=0.02, dwidth=0.2, dheight=1.0):\n",
    "    mwidth = nx * scalex\n",
    "    mheight = ny * scaley\n",
    "    fwidth = mwidth + dwidth\n",
    "    fheight = mheight + dheight\n",
    "    dendrogram_ratio = (dwidth / fwidth, dheight / fheight)\n",
    "    return fwidth, fheight, dendrogram_ratio\n",
    "    \n",
    "\n",
    "def plot_genotype(gamma, linkage_kw=None, **kwargs):\n",
    "    if linkage_kw is None:\n",
    "        linkage_kw = {}\n",
    "    linkage, _ = genotype_linkage(gamma, **linkage_kw)\n",
    "    \n",
    "    gamma_t = gamma.T\n",
    "    ny, nx = gamma_t.shape\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=0.15, scaley=0.02, dwidth=0.2, dheight=1.0\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        cmap='coolwarm',\n",
    "        dendrogram_ratio=dendrogram_ratio,\n",
    "        col_linkage=linkage,\n",
    "        figsize=(fwidth, fheight),\n",
    "        xticklabels=1,\n",
    "        yticklabels=0,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    sns.clustermap(prob_to_sign(gamma_t), **kw)\n",
    "    \n",
    "def plot_missing(delta, **kwargs):\n",
    "    delta_t = delta.T\n",
    "    ny, nx = delta_t.shape\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=0.15, scaley=0.02, dwidth=0.2, dheight=1.0\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        vmin=0, vmax=1, dendrogram_ratio=dendrogram_ratio, figsize=(fwidth, fheight), xticklabels=1, yticklabels=0,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    sns.clustermap(delta_t, **kw)\n",
    "    \n",
    "def plot_community(pi, **kwargs):\n",
    "    ny, nx = pi.shape\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=0.2, scaley=0.1, dwidth=0.2, dheight=1.0\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        metric='cosine', vmin=0, vmax=1, dendrogram_ratio=dendrogram_ratio, figsize=(fwidth, fheight), xticklabels=1,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    sns.clustermap(pi, **kw)\n",
    "    \n",
    "def plot_genotype_similarity(gamma, linkage_kw=None, **kwargs):\n",
    "    if linkage_kw is None:\n",
    "        linkage_kw = {}\n",
    "    linkage, dmat = genotype_linkage(gamma, **linkage_kw)\n",
    "    dmat = squareform(dmat)\n",
    "    \n",
    "    nx = ny = gamma.shape[0]\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=0.15, scaley=0.15, dwidth=0.5, dheight=0.5\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        vmin=0, vmax=1, dendrogram_ratio=dendrogram_ratio, row_linkage=linkage, col_linkage=linkage, figsize=(fwidth, fheight), xticklabels=1, yticklabels=1,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    sns.clustermap(1 - dmat, **kw)\n",
    "    \n",
    "def plot_genotype_comparison(data=None, **kwargs):\n",
    "    stacked = pd.concat([\n",
    "        pd.DataFrame(data[k], index=[f'{k}_{i}' for i in range(data[k].shape[0])])\n",
    "        for k in data\n",
    "    ])\n",
    "    kw = dict(xticklabels=1)\n",
    "    kw.update(kwargs)\n",
    "    plot_genotype(stacked, **kw)\n",
    "\n",
    "def plot_community_comparison(data=None, **kwargs):\n",
    "    stacked = pd.concat([\n",
    "        pd.DataFrame(data[k], columns=[f'{k}_{i}' for i in range(data[k].shape[1])])\n",
    "        for k in data\n",
    "    ], axis=1)\n",
    "    kw = dict(xticklabels=1)\n",
    "    kw.update(kwargs)\n",
    "    plot_community(stacked, **kw)\n",
    "\n",
    "def plot_loss_history(trace):\n",
    "    trace = np.array(trace)\n",
    "    plt.plot((trace - trace.min()))\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/estimation.py\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sfacts.genotype import genotype_pdist, mask_missing_genotype\n",
    "from sfacts.pyro_util import all_torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "from sfacts.metagenotype_model import condition_model\n",
    "\n",
    "def cluster_genotypes(\n",
    "    gamma, thresh, progress=False\n",
    "):\n",
    "    \n",
    "\n",
    "    clust = pd.Series(\n",
    "        AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            affinity=\"precomputed\",\n",
    "            linkage=\"complete\",\n",
    "            distance_threshold=thresh,\n",
    "        )\n",
    "        .fit(squareform(genotype_pdist(gamma, progress=progress)))\n",
    "        .labels_\n",
    "    )\n",
    "\n",
    "    return clust\n",
    "\n",
    "def initialize_parameters_by_clustering_samples(\n",
    "    y, m, thresh, additional_strains_factor=0.5, progress=False,\n",
    "):\n",
    "    n, g = y.shape\n",
    "\n",
    "    sample_genotype = (y + 1) / (m + 2)\n",
    "    clust = cluster_genotypes(sample_genotype, thresh=thresh, progress=progress)\n",
    "\n",
    "    y_total = (\n",
    "        pd.DataFrame(pd.DataFrame(y))\n",
    "        .groupby(clust)\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    m_total = (\n",
    "        pd.DataFrame(pd.DataFrame(m))\n",
    "        .groupby(clust)\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    clust_genotype = (y_total + 1) / (m_total + 2)\n",
    "    additional_haplotypes = int(\n",
    "        additional_strains_factor * clust_genotype.shape[0]\n",
    "    )\n",
    "\n",
    "\n",
    "    gamma_init = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(clust_genotype),\n",
    "            pd.DataFrame(np.ones((additional_haplotypes, g)) * 0.5),\n",
    "        ]\n",
    "    ).values\n",
    "\n",
    "    s_init = gamma_init.shape[0]\n",
    "    pi_init = np.ones((n, s_init))\n",
    "    for i in range(n):\n",
    "        pi_init[i, clust[i]] = s_init - 1\n",
    "    pi_init /= pi_init.sum(1, keepdims=True)\n",
    "\n",
    "    assert (~np.isnan(gamma_init)).all()\n",
    "\n",
    "    return gamma_init, pi_init\n",
    "\n",
    "\n",
    "def estimate_parameters(\n",
    "    model,\n",
    "    data,\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "    initialize_params=None,\n",
    "    maxiter=10000,\n",
    "    lag=100,\n",
    "    lr=1e-0,\n",
    "    clip_norm=100,\n",
    "    progress=True,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    conditioned_model = condition_model(\n",
    "        model,\n",
    "        data=data,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    if initialize_params is None:\n",
    "        initialize_params = {}\n",
    "\n",
    "    _guide = pyro.infer.autoguide.AutoLaplaceApproximation(\n",
    "        conditioned_model,\n",
    "        init_loc_fn=pyro.infer.autoguide.initialization.init_to_value(\n",
    "            values=all_torch(**initialize_params, dtype=dtype, device=device)\n",
    "        ),\n",
    "    )\n",
    "    opt = pyro.optim.Adamax({\"lr\": lr}, {\"clip_norm\": clip_norm})\n",
    "    svi = pyro.infer.SVI(\n",
    "        conditioned_model,\n",
    "        _guide,\n",
    "        opt,\n",
    "        loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    history = []\n",
    "    pbar = tqdm(range(maxiter), disable=(not progress))\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                raise RuntimeError(\"ELBO NaN?\")\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if (i % 10 == 0):\n",
    "                if i > lag:\n",
    "                    delta = history[-2] - history[-1]\n",
    "                    delta_lag = (history[-lag] - history[-1]) / lag\n",
    "                    if delta_lag <= 0:\n",
    "                        if progress:\n",
    "                            info(\"Converged\")\n",
    "                        break\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta': delta,\n",
    "                        f'lag{lag}': delta_lag,\n",
    "                    })\n",
    "    except KeyboardInterrupt:\n",
    "        info(\"Interrupted\")\n",
    "        pass\n",
    "    finally:         \n",
    "        est = pyro.infer.Predictive(conditioned_model, guide=_guide, num_samples=1)()\n",
    "        est = {\n",
    "            k: est[k].detach().cpu().numpy().mean(0).squeeze()\n",
    "            for k in est.keys()\n",
    "        }\n",
    "    return est, history\n",
    "\n",
    "\n",
    "def merge_similar_genotypes(\n",
    "    gamma, pi, thresh, delta=None,\n",
    "):\n",
    "    if delta is None:\n",
    "        delta = np.ones_like(gamma)\n",
    "        \n",
    "    gamma_adjust = mask_missing_genotype(gamma, delta)\n",
    "\n",
    "    clust = cluster_genotypes(gamma_adjust, thresh=thresh)\n",
    "    gamma_mean = (\n",
    "        pd.DataFrame(pd.DataFrame(gamma_adjust))\n",
    "        .groupby(clust)\n",
    "        .apply(lambda x: sp.special.expit(sp.special.logit(x)).mean(0))\n",
    "        .values\n",
    "    )\n",
    "    delta_mean = (\n",
    "        pd.DataFrame(pd.DataFrame(delta))\n",
    "        .groupby(clust)\n",
    "        .mean()\n",
    "        .values\n",
    "    )\n",
    "    pi_sum = (\n",
    "        pd.DataFrame(pd.DataFrame(pi))\n",
    "        .groupby(clust, axis='columns')\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    \n",
    "    return gamma_mean, pi_sum, delta_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/evaluation.py\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    q = 1 - p\n",
    "    ent = -(p * np.log2(p) + q * np.log2(q))\n",
    "    return ent\n",
    "\n",
    "def sum_binary_entropy(p, normalize=False, axis=None):\n",
    "    q = 1 - p\n",
    "    ent = np.sum(-(p * np.log2(p) + q * np.log2(q)), axis=axis)\n",
    "    if normalize:\n",
    "        ent = ent / p.shape[axis]\n",
    "    return ent\n",
    "\n",
    "def mean_masked_genotype_entropy(gamma, delta):\n",
    "    return (binary_entropy(gamma) * delta).mean(1)\n",
    "\n",
    "def sample_mean_masked_genotype_entropy(pi, gamma, delta):\n",
    "    return (pi @ mean_masked_genotype_entropy(gamma, delta).reshape((-1, 1))).squeeze()\n",
    "\n",
    "def match_genotypes(gammaA, gammaB):\n",
    "    g = gammaA.shape[1]\n",
    "    dist = pd.DataFrame(cdist(gammaA, gammaB, metric='cityblock'))\n",
    "    return dist.idxmin(axis=1), dist.min(axis=1) / g\n",
    "\n",
    "def _rmse(x, y):\n",
    "    return np.sqrt(np.square(x - y).mean())\n",
    "\n",
    "def community_accuracy_test(pi_sim, pi_fit, reps=99):\n",
    "    bc_sim = 1 - pdist(pi_sim, metric='braycurtis')\n",
    "    bc_fit = 1 - pdist(pi_fit, metric='braycurtis')\n",
    "    err = _rmse(bc_sim, bc_fit)\n",
    "    \n",
    "    null = []\n",
    "    n = len(bc_sim)\n",
    "    for i in range(reps):\n",
    "        bc_sim_permute = np.random.permutation(bc_sim)\n",
    "        null.append(_rmse(bc_sim, bc_sim_permute))\n",
    "    null = np.array(null)\n",
    "    \n",
    "    return err, null, err / np.mean(null), (np.sort(null) < err).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/workflow.py\n",
    "\n",
    "import pyro\n",
    "from sfacts.metagenotype_model import model, simulate, condition_model\n",
    "from sfacts.estimation import (\n",
    "    initialize_parameters_by_clustering_samples,\n",
    "    estimate_parameters,\n",
    "    merge_similar_genotypes\n",
    ")\n",
    "from sfacts.genotype import mask_missing_genotype\n",
    "from sfacts.evaluation import match_genotypes, sample_mean_masked_genotype_entropy, community_accuracy_test\n",
    "import time\n",
    "\n",
    "def fit_to_data(\n",
    "    y,\n",
    "    m,\n",
    "    fit_kwargs,\n",
    "    preclust=True,\n",
    "    preclust_kwargs=None,\n",
    "    postclust=True,\n",
    "    postclust_kwargs=None,\n",
    "    seed=1,\n",
    "):\n",
    "    n, g = y.shape\n",
    "    pyro.util.set_rng_seed(seed)\n",
    "    if preclust:\n",
    "        assert preclust_kwargs is not None\n",
    "        gamma_init, pi_init = initialize_parameters_by_clustering_samples(\n",
    "            y,\n",
    "            m,\n",
    "            **preclust_kwargs\n",
    "        )\n",
    "        initialize_params=dict(gamma=gamma_init, pi=pi_init)\n",
    "        s = gamma_init.shape[0]\n",
    "    else:\n",
    "        initialize_params = None\n",
    "        s = fit_kwargs.pop('s')\n",
    "\n",
    "    fit, history = estimate_parameters(\n",
    "        model,\n",
    "        data=dict(y=y, m=m),\n",
    "        n=n,\n",
    "        g=g,\n",
    "        s=s,\n",
    "        initialize_params=initialize_params,\n",
    "        **fit_kwargs,\n",
    "    )\n",
    "    \n",
    "    if postclust:\n",
    "        merge_gamma, merge_pi, merge_delta = merge_similar_genotypes(\n",
    "            fit['gamma'],\n",
    "            fit['pi'],\n",
    "            delta=fit['delta'],\n",
    "            **postclust_kwargs,\n",
    "        )\n",
    "        mrg = fit.copy()\n",
    "        mrg['gamma'] = merge_gamma\n",
    "        mrg['pi'] = merge_pi\n",
    "        mrg['delta'] = merge_delta\n",
    "    else:\n",
    "        mrg = fit\n",
    "        \n",
    "    return mrg, fit, history\n",
    "\n",
    "def simulate_fit_and_evaluate(\n",
    "    s_sim,\n",
    "    n_sim,\n",
    "    g_sim,\n",
    "    n_fit,\n",
    "    g_fit,\n",
    "    sim_kwargs,\n",
    "    fit_kwargs,\n",
    "    seed=1,\n",
    "    preclust=True,\n",
    "    preclust_kwargs=None,\n",
    "    postclust=True,\n",
    "    postclust_kwargs=None,\n",
    "    return_raw=False,\n",
    "):\n",
    "    pyro.util.set_rng_seed(seed)\n",
    "    sim = simulate(\n",
    "        condition_model(\n",
    "            model,\n",
    "            n=n_sim,\n",
    "            g=g_sim,\n",
    "            s=s_sim,\n",
    "            **sim_kwargs,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mrg, fit, history = fit_to_data(\n",
    "        sim['y'][:n_fit, :g_fit],\n",
    "        sim['m'][:n_fit, :g_fit],\n",
    "        fit_kwargs=fit_kwargs,\n",
    "        preclust=preclust,\n",
    "        preclust_kwargs=preclust_kwargs,\n",
    "        postclust=postclust,\n",
    "        postclust_kwargs=postclust_kwargs,\n",
    "        seed=seed,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    s_mrg = mrg['gamma'].shape[0]\n",
    "    \n",
    "    sim_gamma_adj = mask_missing_genotype(sim['gamma'][:, :g_fit], sim['delta'][:, :g_fit])\n",
    "    mrg_gamma_adj = mask_missing_genotype(mrg['gamma'], mrg['delta'])\n",
    "    best_hit, best_dist = match_genotypes(sim_gamma_adj, mrg_gamma_adj)\n",
    "    weighted_mean_genotype_error = (best_dist * sim['pi'][:n_fit].mean(0)).sum()\n",
    "    runtime = end_time - start_time\n",
    "    \n",
    "    _, _, beta_diversity_error_ratio, _ = (\n",
    "        community_accuracy_test(sim['pi'][:n_fit], mrg['pi'])\n",
    "    )\n",
    "    \n",
    "    strain_count_error = s_mrg - s_sim\n",
    "    \n",
    "    mean_sample_weighted_genotype_entropy = (\n",
    "        sample_mean_masked_genotype_entropy(mrg['pi'], mrg['gamma'], mrg['delta']).mean()\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        weighted_mean_genotype_error,\n",
    "        beta_diversity_error_ratio,\n",
    "        strain_count_error,\n",
    "        mean_sample_weighted_genotype_entropy,\n",
    "        runtime,\n",
    "        sim,\n",
    "        mrg\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfacts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from torch.jit import TracerWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\",\n",
    "    category=TracerWarning,\n",
    "#     module=\"trace_elbo\",  # FIXME: What is the correct regex for module?\n",
    "#     lineno=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "delta_hyper_temp, delta_hyper_p = 0.1, 0.9\n",
    "x = pyro.sample('delta', dist.Beta(delta_hyper_temp * (delta_hyper_p), delta_hyper_temp * (1 - delta_hyper_p)).expand([10000])).cpu().numpy()\n",
    "plt.hist(x, bins=100)\n",
    "print(x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_hyper_alpha, epsilon_hyper_beta = 1.5, 1.5 / 0.01\n",
    "plt.hist(pyro.sample('epsilon_hyper', dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta).expand([10000])).cpu().numpy(), bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_info(model, n=100, g=200, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimShape-1: Small study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "pyro.util.set_rng_seed(seed)\n",
    "\n",
    "n_sim = 100\n",
    "g_sim = 5000\n",
    "s_sim = 20\n",
    "\n",
    "sim1 = simulate(\n",
    "    condition_model(\n",
    "        model,\n",
    "        data=dict(\n",
    "            alpha_hyper_mean=100.\n",
    "        ),\n",
    "        n=n_sim,\n",
    "        g=g_sim,\n",
    "        s=s_sim,\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_p=0.7,\n",
    "        pi_hyper=0.5,\n",
    "        rho_hyper=2.,\n",
    "        mu_hyper_mean=2.,\n",
    "        mu_hyper_scale=0.5,\n",
    "        m_hyper_r=10.,\n",
    "        alpha_hyper_scale=0.5,\n",
    "        epsilon_hyper_alpha=1.5,\n",
    "        epsilon_hyper_beta=1.5/0.01,\n",
    "        device='cpu'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plt = 100\n",
    "g_plt = 200\n",
    "s_plt = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(counts_to_p_estimate(sim1['y'][:n_plt, :g_plt], sim1['m'][:n_plt, :g_plt]), linkage_kw=dict(progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_community(sim1['pi'][:s_plt, :n_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype_similarity(counts_to_p_estimate(sim1['y'][:n_plt, :g_plt], sim1['m'][:n_plt, :g_plt]), linkage_kw=dict(progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1['gamma'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(sim1['delta'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(sim1['nu'][:n_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(sim1['m'][:n_plt, :g_plt], norm=mpl.colors.SymLogNorm(linthresh=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype_similarity(sim1['gamma'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1['epsilon'], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1['alpha'], bins=20)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fit = 1000  # sim1['y'].shape[1]\n",
    "n_fit = sim1['y'].shape[0]\n",
    "\n",
    "sim1_gamma_init, sim1_pi_init = initialize_parameters_by_clustering_samples(\n",
    "    sim1['y'][:n_fit, :g_fit],\n",
    "    sim1['m'][:n_fit, :g_fit],\n",
    "    thresh=0.1,\n",
    "    additional_strains_factor=0.,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "print(sim1_pi_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1_gamma_init[:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype_similarity(sim1_gamma_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_community(sim1_pi_init[:n_plt, :s_plt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fit = sim1_gamma_init.shape[0]\n",
    "initialize_params = dict(gamma=sim1_gamma_init, pi=sim1_pi_init)\n",
    "\n",
    "sim1_fit1, history = estimate_parameters(\n",
    "    model,\n",
    "    data=dict(y=sim1['y'][:, :g_fit], m=sim1['m'][:, :g_fit]),\n",
    "    n=n_fit,\n",
    "    g=g_fit,\n",
    "    s=s_fit,\n",
    "    gamma_hyper=0.1,\n",
    "    pi_hyper=1.0,\n",
    "    rho_hyper=0.5,\n",
    "    mu_hyper_mean=5,\n",
    "    mu_hyper_scale=5.,\n",
    "    m_hyper_r=10.,\n",
    "    delta_hyper_temp=0.1,\n",
    "    delta_hyper_p=0.9,\n",
    "    alpha_hyper_hyper_mean=100.,\n",
    "    alpha_hyper_hyper_scale=10.,\n",
    "    alpha_hyper_scale=0.5,\n",
    "    epsilon_hyper_alpha=1.5,\n",
    "    epsilon_hyper_beta=1.5 / 0.01,\n",
    "    initialize_params=initialize_params,\n",
    "    device='cpu',\n",
    "    lag=100,\n",
    "    lr=1e-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1_fit1_gamma_merge, sim1_fit1_pi_merge, sim1_fit1_delta_merge  = merge_similar_genotypes(\n",
    "    sim1_fit1['gamma'],\n",
    "    sim1_fit1['pi'],\n",
    "    delta=sim1_fit1['delta'],\n",
    "    thresh=0.1,\n",
    ")\n",
    "\n",
    "# print(sim1_gamma_init.shape[0], sim1_fit1['gamma'].shape[0], sim1_fit1_gamma_merge.shape[0])\n",
    "print(sim1_fit1['gamma'].shape[0], sim1_fit1_gamma_merge.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1_gamma_adjusted = mask_missing_genotype(sim1['gamma'][:, :g_fit], sim1['delta'][:, :g_fit])\n",
    "sim1_fit1_gamma_adjusted = mask_missing_genotype(sim1_fit1['gamma'], sim1_fit1['delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype_comparison(\n",
    "    data=dict(\n",
    "        true=sim1_gamma_adjusted[:, :g_plt],\n",
    "#         fit=sim1_fit1['gamma'][:, :g_plt],\n",
    "        adj=sim1_fit1_gamma_adjusted[:, :g_plt],\n",
    "#         init=sim1_gamma_init,\n",
    "#         merg=sim1_fit1_gamma_merge,\n",
    "    ),\n",
    "    linkage_kw=dict(progress=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_community_comparison(\n",
    "    data=dict(\n",
    "        true=sim1['pi'],\n",
    "        fit=sim1_fit1['pi'],\n",
    "#         init=sim1_pi_init,\n",
    "#         merg=sim1_fit1_pi_merge,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['epsilon'], sim1_fit1['epsilon'])\n",
    "plt.plot([0, 0.04], [0, 0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['alpha'], sim1_fit1['alpha'])\n",
    "plt.plot([0, 200], [0, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim1_fit1['delta'], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['mu'], sim1_fit1['mu'])\n",
    "plt.plot([0, 40], [0, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot comparing genotype accuracy to true strain abundance\n",
    "# colored by mean entropy of the estimated genotype masked by delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1_fit1['alpha'], sample_mean_masked_genotype_entropy(sim1_fit1['pi'], sim1_fit1['gamma'], sim1_fit1['delta']))\n",
    "sample_mean_masked_genotype_entropy(sim1_fit1['pi'], sim1_fit1['gamma'], sim1_fit1['delta']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hit, best_dist = match_genotypes(sim1_gamma_adjusted[:, :g_fit], sim1_fit1_gamma_adjusted[:, :g_fit])\n",
    "\n",
    "print('weighted_mean_distance:', (best_dist * sim1['pi'].mean(0)).sum())\n",
    "plt.scatter((sim1['pi'] * sim1['mu'].reshape(-1, 1)).sum(0), best_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_sim = 1 - pdist(sim1['pi'], metric='braycurtis')\n",
    "bc_fit = 1 - pdist(sim1_fit1['pi'], metric='braycurtis')\n",
    "plt.scatter(\n",
    "    bc_sim,\n",
    "    bc_fit,\n",
    "    marker='.',\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "community_accuracy_test(sim1['pi'], sim1_fit1['pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strains that are not representative of true haplotypes\n",
    "# are high entropy (even after masking with delta)\n",
    "# and have low estimated total coverage.\n",
    "\n",
    "best_true_strain, best_true_strain_dist = match_genotypes(sim1_fit1_gamma_adjusted[:, :g_fit], sim1_gamma_adjusted[:, :g_fit])\n",
    "best_true_strain_dist\n",
    "\n",
    "plt.scatter((sim1_fit1['pi'] * sim1_fit1['mu'].reshape((-1, 1))).sum(0), best_true_strain_dist, c=mean_masked_genotype_entropy(sim1_fit1['gamma'], sim1_fit1['delta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1_fit1_gamma_adjusted[:, :g_plt], linkage_kw=dict(progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_community(sim1_fit1['pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_masked_genotype_entropy(sim1_fit1['gamma'][:, :g_plt], sim1_fit1['delta'][:, :g_plt]))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1_fit1['alpha'], bins=20)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1_fit1['gamma'][mean_masked_genotype_entropy(sim1_fit1['gamma'], sim1_fit1['delta']) < 0.1, :g_fit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Average and variation in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for seed in range(10):\n",
    "    generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "        s_sim=20,\n",
    "        n_sim=100,\n",
    "        g_sim=500,\n",
    "        n_fit=100,\n",
    "        g_fit=500,\n",
    "        sim_kwargs=dict(\n",
    "            data=dict(\n",
    "                alpha_hyper_mean=100.\n",
    "            ),\n",
    "            gamma_hyper=0.01,\n",
    "            delta_hyper_temp=0.01,\n",
    "            delta_hyper_p=0.7,\n",
    "            pi_hyper=0.5,\n",
    "            rho_hyper=2.,\n",
    "            mu_hyper_mean=1.,\n",
    "            mu_hyper_scale=0.5,\n",
    "            m_hyper_r=10.,\n",
    "            alpha_hyper_scale=0.5,\n",
    "            epsilon_hyper_alpha=1.5,\n",
    "            epsilon_hyper_beta=1.5/0.01,\n",
    "            device='cpu'\n",
    "        ),\n",
    "        preclust_kwargs=dict(\n",
    "            thresh=0.1,\n",
    "            additional_strains_factor=0.1,\n",
    "            progress=False,\n",
    "        ),\n",
    "        fit_kwargs=dict(\n",
    "            gamma_hyper=0.01,\n",
    "            pi_hyper=1.0,\n",
    "            rho_hyper=0.5,\n",
    "            mu_hyper_mean=5,\n",
    "            mu_hyper_scale=5.,\n",
    "            m_hyper_r=10.,\n",
    "            delta_hyper_temp=0.1,\n",
    "            delta_hyper_p=0.9,\n",
    "            alpha_hyper_hyper_mean=100.,\n",
    "            alpha_hyper_hyper_scale=10.,\n",
    "            alpha_hyper_scale=0.5,\n",
    "            epsilon_hyper_alpha=1.5,\n",
    "            epsilon_hyper_beta=1.5 / 0.01,\n",
    "            device='cpu',\n",
    "            lag=10,\n",
    "            lr=1e-0,\n",
    "            progress=False\n",
    "        ),\n",
    "        postclust_kwargs=dict(\n",
    "            thresh=0.1,\n",
    "        ),\n",
    "        seed=seed,\n",
    "        return_raw=True,\n",
    "    )\n",
    "    results.append((seed, generr, comperr, scounter, entropy, runtime))\n",
    "    print(seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results1 = pd.DataFrame(results, columns=['seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    ax.hist(results1[stat])\n",
    "    ax.set_title(stat)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Benefits of increasing sample data (preclust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n_fit in [20, 50, 100, 150, 200, 500]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=500,\n",
    "            g_sim=500,\n",
    "            n_fit=n_fit,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((n_fit, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(n_fit, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results2 = pd.DataFrame(results, columns=['n_fit', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results2.set_index(['n_fit', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    if stat == 'generr':\n",
    "        ax.set_yscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Benefits of increasing sample data (no preclust, `s` known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n_fit in [20, 50, 100, 150, 200, 500]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=500,\n",
    "            g_sim=500,\n",
    "            n_fit=n_fit,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust=False,\n",
    "            fit_kwargs=dict(\n",
    "                s=20,\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((n_fit, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(n_fit, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results3 = pd.DataFrame(results, columns=['n_fit', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results3.set_index(['n_fit', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    if stat == 'generr':\n",
    "        ax.set_yscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Benefits of increasing depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for mu_hyper_mean_sim in [0.5, 1., 2., 5., 10., 100.]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=500,\n",
    "            n_fit=100,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=mu_hyper_mean_sim,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5.,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((mu_hyper_mean_sim, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(mu_hyper_mean_sim, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results4 = pd.DataFrame(results, columns=['mu_hyper_mean_sim', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results4.set_index(['mu_hyper_mean_sim', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    ax.set_xscale('log')\n",
    "    if stat == 'generr':\n",
    "        ax.set_yscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: Benefits of increasing genotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for g_fit in [100, 250, 500, 1000, 2000]:\n",
    "    replicates = 0\n",
    "    for seed in [2, 3, 4, 5, 7]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=2000,\n",
    "            n_fit=100,\n",
    "            g_fit=g_fit,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((g_fit, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(g_fit, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results5 = pd.DataFrame(results, columns=['g_fit', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results5.set_index(['g_fit', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: Strain-number estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for s_fit in [5, 10, 15, 20, 25, 30, 50]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=500,\n",
    "            n_fit=100,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust=False,\n",
    "            fit_kwargs=dict(\n",
    "                s=s_fit,\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((s_fit, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(s_fit, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results6 = pd.DataFrame(results, columns=['s_fit', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results6.set_index(['s_fit', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: Effects of genotype fuzzyness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for gamma_hyper_fit in [1e-8, 1e-5, 1e-3, 1e-2, 5e-2, 1e-1, 5e-1]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=500,\n",
    "            n_fit=100,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=gamma_hyper_fit,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "                postclust_kwargs=dict(\n",
    "                    thresh=0.1,\n",
    "                ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((gamma_hyper_fit, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(gamma_hyper_fit, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results7 = pd.DataFrame(results, columns=['gamma_hyper_fit', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results7.set_index(['gamma_hyper_fit', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    ax.set_xscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8: Effects of diversity regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for rho_hyper_fit in [0.01, 0.05, 0.1, 0.25, 0.5, 1.0]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=500,\n",
    "            n_fit=100,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust=False,\n",
    "            fit_kwargs=dict(\n",
    "                s=30,\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=rho_hyper_fit,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((rho_hyper_fit, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(rho_hyper_fit, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results8 = pd.DataFrame(results, columns=['rho_hyper_fit', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results8.set_index(['rho_hyper_fit', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    ax.set_xscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 9: Effects of heterogeneity regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for pi_hyper_fit in [0.1, 0.25, 0.5, 1.0, 1.5]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=500,\n",
    "            n_fit=100,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=pi_hyper_fit,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((pi_hyper_fit, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(pi_hyper_fit, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results9 = pd.DataFrame(results, columns=['pi_hyper_fit', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results9.set_index(['pi_hyper_fit', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    ax.set_xscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 10: Effects of preclustering threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for preclust_thresh in [0.03, 0.05, 0.08, 0.1, 0.12, 0.15, 0.2]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=500,\n",
    "            n_fit=100,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=preclust_thresh,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((preclust_thresh, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(preclust_thresh, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results10 = pd.DataFrame(results, columns=['preclust_thresh', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results10.set_index(['preclust_thresh', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    if stat == 'comperr':\n",
    "        ax.set_yscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 11: Effects of strain merging (postclustering) threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for postclust_thresh in [0.03, 0.05, 0.08, 0.1, 0.12, 0.15, 0.2]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_fit_and_evaluate(\n",
    "            s_sim=20,\n",
    "            n_sim=100,\n",
    "            g_sim=500,\n",
    "            n_fit=100,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=postclust_thresh,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((postclust_thresh, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(postclust_thresh, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results11 = pd.DataFrame(results, columns=['postclust_thresh', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results11.set_index(['postclust_thresh', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    if stat == 'comperr':\n",
    "        ax.set_yscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Big matrix plot.\n",
    "\n",
    "fig, axs = plt.subplots(5, 10, figsize=(2*11, 2*5), sharex='col', sharey='row')\n",
    "\n",
    "for (stat, scale_y), row in zip([\n",
    "    ('generr', 'log'),\n",
    "    ('comperr', 'log'),\n",
    "    ('scounterr', 'symlog'),\n",
    "    ('entropy', 'linear'),\n",
    "    ('runtime', 'log')\n",
    "], axs):\n",
    "    for (results, indexer, scale_x, title), ax in zip([\n",
    "        (results2, 'n_fit', 'log', 2),\n",
    "        (results3, 'n_fit', 'log', 3),\n",
    "        (results4, 'mu_hyper_mean_sim', 'log', 4),\n",
    "        (results5, 'g_fit', 'log', 5),\n",
    "        (results6, 's_fit', 'linear', 6),\n",
    "        (results7, 'gamma_hyper_fit', 'log', 7),\n",
    "        (results8, 'rho_hyper_fit', 'log', 8),\n",
    "        (results9, 'pi_hyper_fit', 'log', 9),\n",
    "        (results10, 'preclust_thresh', 'linear', 10),\n",
    "        (results11, 'postclust_thresh', 'linear', 11),\n",
    "    ], row):\n",
    "        results.set_index([indexer, 'seed'])[stat].unstack().plot(ax=ax)\n",
    "        ax.set_ylabel(stat)\n",
    "        ax.set_xlabel(indexer)\n",
    "        ax.legend_.set_visible(False)\n",
    "        ax.set_xscale(scale_x)\n",
    "        ax.set_yscale(scale_y)\n",
    "        ax.set_title(title)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Experiment 12: TODO: check similar stuff on a large-data example."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = []\n",
    "for postclust_thresh in [0.03, 0.05, 0.08, 0.1, 0.12, 0.15, 0.2]:\n",
    "    replicates = 0\n",
    "    for seed in [0, 1, 3, 4, 5]:\n",
    "        generr, comperr, scounter, entropy, runtime, sim, fit = simulate_then_run_workflow(\n",
    "            s_sim=100,\n",
    "            n_sim=1000,\n",
    "            g_sim=500,\n",
    "            n_fit=1000,\n",
    "            g_fit=500,\n",
    "            sim_kwargs=dict(\n",
    "                data=dict(\n",
    "                    alpha_hyper_mean=100.\n",
    "                ),\n",
    "                gamma_hyper=0.01,\n",
    "                delta_hyper_temp=0.01,\n",
    "                delta_hyper_p=0.7,\n",
    "                pi_hyper=0.5,\n",
    "                rho_hyper=2.,\n",
    "                mu_hyper_mean=1.,\n",
    "                mu_hyper_scale=0.5,\n",
    "                m_hyper_r=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5/0.01,\n",
    "                device='cpu'\n",
    "            ),\n",
    "            preclust_kwargs=dict(\n",
    "                thresh=0.1,\n",
    "                additional_strains_factor=0.1,\n",
    "                progress=False,\n",
    "            ),\n",
    "            fit_kwargs=dict(\n",
    "                gamma_hyper=0.01,\n",
    "                pi_hyper=1.0,\n",
    "                rho_hyper=0.5,\n",
    "                mu_hyper_mean=5,\n",
    "                mu_hyper_scale=5.,\n",
    "                m_hyper_r=10.,\n",
    "                delta_hyper_temp=0.1,\n",
    "                delta_hyper_p=0.9,\n",
    "                alpha_hyper_hyper_mean=100.,\n",
    "                alpha_hyper_hyper_scale=10.,\n",
    "                alpha_hyper_scale=0.5,\n",
    "                epsilon_hyper_alpha=1.5,\n",
    "                epsilon_hyper_beta=1.5 / 0.01,\n",
    "                device='cpu',\n",
    "                lag=10,\n",
    "                lr=1e-0,\n",
    "                progress=False\n",
    "            ),\n",
    "            postclust_kwargs=dict(\n",
    "                thresh=postclust_thresh,\n",
    "            ),\n",
    "            seed=seed,\n",
    "            return_raw=True,\n",
    "        )\n",
    "        results.append((postclust_thresh, seed, generr, comperr, scounter, entropy, runtime))\n",
    "        print(postclust_thresh, seed, generr, comperr, scounter, entropy, runtime, sep='\\t')\n",
    "         \n",
    "results12 = pd.DataFrame(results, columns=['postclust_thresh', 'seed', 'generr', 'comperr', 'scounterr', 'entropy', 'runtime'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "for stat, ax in zip(['generr', 'comperr', 'scounterr', 'entropy', 'runtime'], axs.flatten()):\n",
    "    results12.set_index(['postclust_thresh', 'seed'])[stat].unstack().plot(ax=ax)\n",
    "    ax.set_title(stat)\n",
    "    ax.legend_.set_visible(False)\n",
    "    if stat == 'comperr':\n",
    "        ax.set_yscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "82px",
    "width": "168px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}