{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sfacts.logging_util import *\n",
    "from sfacts.pyro_util import *\n",
    "from sfacts.model import *\n",
    "from sfacts.genotype import *\n",
    "from sfacts.plot import *\n",
    "from sfacts.estimation import *\n",
    "from sfacts.evaluation import *\n",
    "from sfacts.workflow import *\n",
    "from sfacts.data import *\n",
    "from sfacts.pandas_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sfacts as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from torch.jit import TracerWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls sfacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/__init__.py\n",
    "from sfacts import (\n",
    "    logging_util,\n",
    "    pyro_util,\n",
    "    pandas_util,\n",
    "    model,\n",
    "    model_zoo,\n",
    "    plot,\n",
    "    estimation,\n",
    "    evaluation,\n",
    "#     workflow,\n",
    "    data,\n",
    "#     app,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyro_util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/pyro_util.py\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from sfacts.logging_util import info\n",
    "import warnings\n",
    "\n",
    "\n",
    "def as_torch(x, dtype=None, device=None):\n",
    "    # Cast inputs and set device\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return torch.tensor(x.numpy(), dtype=dtype, device=device)\n",
    "    else:\n",
    "        return torch.tensor(x, dtype=dtype, device=device)\n",
    "\n",
    "\n",
    "def all_torch(dtype=None, device=None, **kwargs):\n",
    "    # Cast inputs and set device\n",
    "    return {k: as_torch(kwargs[k], dtype=dtype, device=device) for k in kwargs}\n",
    "\n",
    "\n",
    "def shape_info(model, *args, **kwargs):\n",
    "    _trace = pyro.poutine.trace(model).get_trace(*args, **kwargs)\n",
    "    _trace.compute_log_prob()\n",
    "    info(_trace.format_shapes())\n",
    "\n",
    "def set_random_seed(seed, warn=True):\n",
    "    if seed is not None:\n",
    "        pyro.set_rng_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/data.py\n",
    "from sfacts.logging_util import info\n",
    "from sfacts.pandas_util import idxwhere\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import scipy as sp\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def _on_2_simplex(d):\n",
    "    return (d.min() >= 0) and (d.max() <= 1.0)\n",
    "\n",
    "\n",
    "def _strictly_positive(d):\n",
    "    return d.min() > 0\n",
    "\n",
    "\n",
    "def _positive_counts(d):\n",
    "    return (d.astype(int) == d).all()\n",
    "\n",
    "\n",
    "class WrappedDataArrayMixin():\n",
    "    constraints = {}\n",
    "    \n",
    "    # The following are all white-listed and\n",
    "    # transparently passed through to self.data, but with\n",
    "    # different symantics for the return value.\n",
    "    dims = ()\n",
    "    safe_unwrapped = ['shape', 'sizes', 'to_pandas', 'to_dataframe', 'min', 'max', 'sum', 'mean', 'median', 'values', 'pipe', 'to_series']\n",
    "    safe_lifted = ['isel', 'sel']\n",
    "    variable_name = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_ndarray(cls, x, coords=None):\n",
    "        if coords is None:\n",
    "            coords = {k: None for k in cls.dims}\n",
    "        shapes = {k: x.shape[i] for i, k in enumerate(cls.dims)}\n",
    "        for k in coords:\n",
    "            if coords[k] is None:\n",
    "                coords[k] = range(shapes[k])\n",
    "        data = xr.DataArray(\n",
    "            x,\n",
    "            dims=cls.dims,\n",
    "            coords=coords,\n",
    "        )\n",
    "        return cls(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def stack(cls, mapping, dim, prefix=False, validate=True):\n",
    "        if not len(cls.dims) == 2:\n",
    "            raise NotImplementedError(\"Generic stacking has only been implemented for 2D wrapped DataArrays\")\n",
    "        axis = cls.dims.index(dim)\n",
    "        data = []\n",
    "        for k, d in mapping.items():\n",
    "            if prefix:\n",
    "                d = d.to_pandas().rename(lambda s: f\"{k}_{s}\", axis=axis).stack().to_xarray()\n",
    "            else:\n",
    "                d = d.data\n",
    "            data.append(d)\n",
    "        out = cls(xr.concat(data, dim=dim))\n",
    "        if validate:\n",
    "            out.validate_constraints()\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.validate_fast()\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.dims:\n",
    "            return getattr(self.data, name)\n",
    "        elif name in self.safe_unwrapped:\n",
    "            return getattr(self.data, name)\n",
    "        elif name in self.safe_lifted:\n",
    "            return lambda *args, **kwargs: self.__class__(getattr(self.data, name)(*args, **kwargs))\n",
    "        else:\n",
    "            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}' \"\n",
    "                                 f\"and this name is not found in '{self.__class__.__name__}.dims', \"\n",
    "                                 f\"'{self.__class__.__name__}.safe_unwrapped', \"\n",
    "                                 f\"or '{self.__class__.__name__}.safe_lifted'. \"\n",
    "                                 f\"Consider working with the '{self.__class__.__name__}.data' \"\n",
    "                                 f\"xr.DataArray object directly.\")\n",
    "    \n",
    "    def validate_fast(self):\n",
    "        assert len(self.data.shape) == len(self.dims)\n",
    "        assert self.data.dims == self.dims\n",
    "        \n",
    "    def validate_constraints(self):\n",
    "        self.validate_fast()\n",
    "        for name in self.constraints:\n",
    "            assert self.constraints[name](self.data), f\"Failed constraint: {name}\"\n",
    "        \n",
    "    def lift(self, func, *args, **kwargs):\n",
    "        return self.__class__(self.data.pipe(func, *args, **kwargs))\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}({self.data})'\n",
    "    \n",
    "    @classmethod\n",
    "    def concat(cls, data, dim):\n",
    "        out_data = []\n",
    "        new_coords = []\n",
    "        for name in data:\n",
    "            d = data[name].data\n",
    "            out_data.append(d)\n",
    "            new_coords.extend([f\"{name}_{i}\" for i in d[dim].values])\n",
    "        out_data = xr.concat(out_data, dim)\n",
    "        out_data[dim] = new_coords\n",
    "        return cls(out_data)\n",
    "    \n",
    "    def to_world(self):\n",
    "        return World(self.data.to_dataset())\n",
    "\n",
    "\n",
    "class Metagenotypes(WrappedDataArrayMixin):\n",
    "    dims = ('sample', 'position', 'allele')\n",
    "    constraints = dict(\n",
    "        positive_counts = _positive_counts\n",
    "    )\n",
    "    variable_name = 'metagenotypes'\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename_or_obj, validate=True):\n",
    "        data = xr.open_dataarray(filename_or_obj).rename({'library_id': 'sample'}).squeeze(drop=True)\n",
    "        data.name = 'metagenotypes'\n",
    "        result = cls(data)\n",
    "        if validate:\n",
    "            result.validate_constraints()\n",
    "        return result\n",
    "    \n",
    "    @classmethod\n",
    "    def from_counts_and_totals(cls, y, m, coords=None):\n",
    "        if coords is None:\n",
    "            coords = {}\n",
    "        if not 'allele' in coords:\n",
    "            coords['allele'] = ['alt', 'ref']\n",
    "        x = np.stack([y, m - y], axis=-1)\n",
    "        return cls.from_ndarray(x, coords=coords)\n",
    "        \n",
    "    def dump(self, path, validate=True):\n",
    "        if validate:\n",
    "            self.validate_constraints()\n",
    "        self.data.astype(np.uint8).to_dataset(name=\"tally\").to_netcdf(\n",
    "            path,\n",
    "            encoding=dict(tally=dict(zlib=True, complevel=6))\n",
    "        )\n",
    "\n",
    "    def select_variable_positions(self, incid_thresh, allele_thresh=0):\n",
    "        # TODO: Consider using .lift() to do this.\n",
    "        x = self.data\n",
    "        minor_allele_incid = (x > allele_thresh).mean(\"sample\").min(\"allele\")\n",
    "        variable_positions = idxwhere(minor_allele_incid.to_series() > incid_thresh)\n",
    "        return self.__class__(x.sel(position=variable_positions))\n",
    "\n",
    "    def select_samples_with_coverage(self, cvrg_thresh):\n",
    "        # TODO: Consider using .lift() to do this.\n",
    "        x = self.data\n",
    "        covered_samples = (x.sum(\"allele\") > 0).mean(\"position\") > cvrg_thresh\n",
    "        return self.__class__(x.sel(sample=covered_samples))\n",
    "\n",
    "    def frequencies(self, pseudo=0.):\n",
    "        \"Convert metagenotype counts to a frequencies with optional pseudocount.\"\n",
    "        return (self.data + pseudo) / (self.data.sum('allele') + pseudo * self.sizes['allele'])\n",
    "    \n",
    "    def dominant_allele_fraction(self, pseudo=0.):\n",
    "        \"Convert metagenotype counts to a frequencies with optional pseudocount.\"\n",
    "        return self.frequencies(pseudo=pseudo).max('allele')\n",
    "\n",
    "    def to_genotype_estimates(self, pseudo=1.):\n",
    "        data = self.frequencies(pseudo=pseudo).sel(allele='alt').rename({'sample': 'strain'})\n",
    "        return Genotypes(data)\n",
    "\n",
    "    def to_counts_and_totals(self, binary_allele='alt'):\n",
    "        return dict(y=self.data.sel(allele=binary_allele).values, m=self.data.sum('allele').values)\n",
    "    \n",
    "    def pdist(self, dim='strain', **kwargs):\n",
    "        return self.to_genotype_estimates().pdist(dim=dim, pseudo=pseudo, **kwargs)\n",
    "    \n",
    "    def linkage(self, dim='strain', **kwargs):\n",
    "        return self.to_genotype_estimates().linkage(dim=dim, **kwargs)\n",
    "\n",
    "\n",
    "class Genotypes(WrappedDataArrayMixin):\n",
    "    dims = ('strain', 'position')\n",
    "    constraints = dict(\n",
    "        on_2_simplex = _on_2_simplex\n",
    "    )\n",
    "    variable_name = 'genotypes'\n",
    "    \n",
    "    def fuzz_missing(self, missingness, eps=1e-10):\n",
    "        clip = partial(np.clip, a_min=eps, a_max=(1 - eps))\n",
    "        return self.lift(lambda g, m: sp.special.expit(sp.special.logit(clip(g)) * clip(m)), m=missingness.data)\n",
    "    \n",
    "    # TODO: Move distance metrics to a new module?\n",
    "    @staticmethod\n",
    "    def _convert_to_sign_representation(p):\n",
    "        \"Alternative representation of binary genotype on a [-1, 1] interval.\"\n",
    "        return p * 2 - 1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _genotype_sign_representation_dissimilarity(x, y, pseudo=0.):\n",
    "        \"Dissimilarity between 1D genotypes, accounting for fuzzyness.\"\n",
    "        dist = ((x - y) / 2) ** 2\n",
    "        weight = (x * y) ** 2\n",
    "        wmean_dist = ((weight * dist).mean()) / ((weight.mean() + pseudo))\n",
    "        return wmean_dist\n",
    "\n",
    "    @staticmethod\n",
    "    def _genotype_dissimilarity(x, y, pseudo=0.):\n",
    "        return self._genotype_sign_representation_dissimilarity(\n",
    "            self._genotype_p_to_s(x), self._genotype_p_to_s(y)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _genotype_dissimilarity_cdmat(unwrapped_values, pseudo=0., quiet=True):\n",
    "        g_sign = Genotypes._convert_to_sign_representation(unwrapped_values)\n",
    "        s, _ = g_sign.shape\n",
    "        cdmat = np.empty((s * (s - 1)) // 2)\n",
    "        k = 0\n",
    "        with tqdm(total=len(cdmat), disable=quiet) as pbar:\n",
    "            for i in range(0, s - 1):\n",
    "                for j in range(i + 1, s):\n",
    "                    cdmat[k] = Genotypes._genotype_sign_representation_dissimilarity(g_sign[i], g_sign[j], pseudo=pseudo)\n",
    "                    k = k + 1\n",
    "                    pbar.update()\n",
    "        return cdmat\n",
    "\n",
    "    def pdist(self, dim='strain', pseudo=0., quiet=True):\n",
    "        index = getattr(self, dim)\n",
    "        if dim == 'strain':\n",
    "            unwrapped_values = self.values\n",
    "            cdmat = self._genotype_dissimilarity_cdmat(unwrapped_values, quiet=quiet, pseudo=pseudo)\n",
    "        elif dim == 'position':\n",
    "            unwrapped_values = self.values.T\n",
    "            cdmat = pdist(self._convert_to_sign_representation(self.values.T), metric='cosine')\n",
    "        # Reboxing\n",
    "        dmat = pd.DataFrame(squareform(cdmat), index=index, columns=index)\n",
    "        return dmat\n",
    "    \n",
    "    def linkage(self, dim='strain', pseudo=0., quiet=True, method=\"complete\", optimal_ordering=True, **kwargs):\n",
    "        dmat = self.pdist(dim=dim, pseudo=pseudo, quiet=quiet)\n",
    "        cdmat = squareform(dmat)\n",
    "        return linkage(cdmat, method=method, optimal_ordering=optimal_ordering, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def entropy(self):\n",
    "        p = self.data\n",
    "        q = 1 - p\n",
    "        ent = -(p * np.log2(p) + q * np.log2(q))\n",
    "        return ent.sum(\"position\").rename(\"entropy\")\n",
    "\n",
    "\n",
    "class Missingness(WrappedDataArrayMixin):\n",
    "    dims = ('strain', 'position')\n",
    "    constraints = dict(\n",
    "        on_2_simplex = _on_2_simplex\n",
    "    )\n",
    "    variable_name = 'missingness'\n",
    "        \n",
    "        \n",
    "class Communities(WrappedDataArrayMixin):\n",
    "    dims = ('sample', 'strain')\n",
    "    constraints = dict(\n",
    "        strains_sum_to_1 = lambda d: (d.sum('strain') == 1.0).all()\n",
    "    )\n",
    "    variable_name = 'communities'\n",
    "    \n",
    "    def pdist(self, dim='strain', quiet=True):\n",
    "        index = getattr(self, dim)\n",
    "        if dim == 'strain':\n",
    "            unwrapped_values = self.values.T\n",
    "            cdmat = pdist(unwrapped_values, metric='cosine') \n",
    "        elif dim == 'sample':\n",
    "            unwrapped_values = self.values\n",
    "            cdmat = pdist(unwrapped_values, metric='braycurtis') \n",
    "        # Reboxing\n",
    "        dmat = pd.DataFrame(squareform(cdmat), index=index, columns=index)\n",
    "        return dmat\n",
    "    \n",
    "    def linkage(self, dim='strain', quiet=True, method=\"average\", optimal_ordering=True, **kwargs):\n",
    "        dmat = self.pdist(dim=dim, quiet=quiet)\n",
    "        cdmat = squareform(dmat)\n",
    "        return linkage(cdmat, method=method, optimal_ordering=optimal_ordering, **kwargs)\n",
    "\n",
    "\n",
    "class Overdispersion(WrappedDataArrayMixin):\n",
    "    dims = ('sample',)\n",
    "    constraints = dict(\n",
    "        strains_sum_to_1 = _strictly_positive\n",
    "    )\n",
    "    variable_name = 'overdispersion'\n",
    "\n",
    "\n",
    "class ErrorRate(WrappedDataArrayMixin):\n",
    "    dims = ('sample',)\n",
    "    constraints = dict(\n",
    "        on_2_simplex = _on_2_simplex\n",
    "    )\n",
    "    variable_name = 'error_rate'\n",
    "    \n",
    "\n",
    "class World():\n",
    "    safe_lifted = ['isel', 'sel']\n",
    "    safe_unwrapped = ['sizes']\n",
    "    dims = ('sample', 'position', 'strain', 'allele')\n",
    "    variables = [Genotypes, Missingness, Communities, Metagenotypes]\n",
    "    _variable_wrapper_map = {wrapper.variable_name: wrapper for wrapper in variables}\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data  # self._align_dims(data)\n",
    "        self.validate_fast()\n",
    "        \n",
    "#     @classmethod\n",
    "#     def _align_dims(cls, data):\n",
    "#         missing_dims = [k for k in cls.dims if k not in data.dims]\n",
    "#         return data.expand_dims(missing_dims).transpose(*cls.dims)\n",
    "        \n",
    "    def validate_fast(self):\n",
    "        assert not (set(self.data.dims) - set(self.dims)), f\"Found data dims that shouldn't exist: {self.data.dims}\"\n",
    "        \n",
    "    def validate_constraints(self):\n",
    "        self.validate_fast()\n",
    "        for variable_name in _variable_wrapper_map:\n",
    "            if variable_name in self.data:\n",
    "                wrapped_variable = getattr(self, name)\n",
    "                wrapped_variable.validate_constraints()\n",
    "    \n",
    "    @property\n",
    "    def fuzzed_genotypes(self):\n",
    "        return self.genotypes.fuzz_missing(self.missingness)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.dims:\n",
    "            # Return dims for those registered in self.dims.\n",
    "            return getattr(self.data, name)\n",
    "        if name in self._variable_wrapper_map:\n",
    "            # Return wrapped variables for those registered in self.variables.\n",
    "            return self._variable_wrapper_map[name](self.data[name])\n",
    "        elif name in self.safe_unwrapped:\n",
    "            # Return a naked version of the variables registered in self.safe_unwrapped\n",
    "            return getattr(self.data, name)\n",
    "        elif name in self.safe_lifted:\n",
    "            # Return a lifted version of the the attributes registered in safe_lifted\n",
    "            return lambda *args, **kwargs: self.__class__(getattr(self.data, name)(*args, **kwargs))\n",
    "        else:\n",
    "            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}' \"\n",
    "                                 f\"and this name is not found in '{self.__class__.__name__}.dims', \"\n",
    "                                 f\"'{self.__class__.__name__}.safe_unwrapped', \"\n",
    "                                 f\"or '{self.__class__.__name__}.safe_lifted'. \"\n",
    "                                 f\"Consider working with the '{self.__class__.__name__}.data' object directly.\")\n",
    "            \n",
    "    @classmethod\n",
    "    def concat(cls, data, dim):\n",
    "        out_data = []\n",
    "        new_coords = []\n",
    "        for name in data:\n",
    "            d = data[name].data\n",
    "            d['_concat_from'] = xr.DataArray(name, dims=(dim,), coords={dim: d[dim]})\n",
    "            out_data.append(d)\n",
    "            new_coords.extend([f\"{name}_{i}\" for i in d[dim].values])\n",
    "        out_data = xr.concat(out_data, dim, data_vars='minimal', coords='minimal', compat='override')\n",
    "        out_data[dim] = new_coords\n",
    "        return cls(out_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/plot.py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.spatial.distance import squareform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sfacts as sf\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def _calculate_clustermap_sizes(\n",
    "    nx, ny, scalex=0.15, scaley=0.02, cwidth=0, cheight=0, dwidth=0.2, dheight=1.0\n",
    "):\n",
    "    # TODO: Incorporate colors.\n",
    "    mwidth = nx * scalex\n",
    "    mheight = ny * scaley\n",
    "    fwidth = mwidth + cwidth + dwidth\n",
    "    fheight = mheight + cheight + dheight\n",
    "    dendrogram_ratio = (dwidth / fwidth, dheight / fheight)\n",
    "    colors_ratio = (cwidth / fwidth, cheight / fheight)\n",
    "    return (fwidth, fheight), dendrogram_ratio, colors_ratio\n",
    "\n",
    "\n",
    "def _min_max_normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "def _scale_to_max_of_one(x):\n",
    "    return x / x.max()\n",
    "\n",
    "\n",
    "def dictionary_union(*args):\n",
    "    out = args[0].copy()\n",
    "    for a in args:\n",
    "        out.update(a)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_generic_clustermap_factory(\n",
    "    matrix_func,\n",
    "    col_colors_func=None,\n",
    "    row_colors_func=None,\n",
    "    col_linkage_func=None,\n",
    "    row_linkage_func=None,\n",
    "    row_col_annotation_cmap=mpl.cm.viridis,\n",
    "    scalex=0.05,\n",
    "    scaley=0.05,\n",
    "    cwidth=0.4,\n",
    "    cheight=0.4,\n",
    "    dwidth=1.0,\n",
    "    dheight=1.0,\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    cmap=None,\n",
    "    norm=mpl.colors.PowerNorm(1.),\n",
    "    xticklabels=0,\n",
    "    yticklabels=0,\n",
    "    metric='correlation',\n",
    "    cbar_pos=None,\n",
    "):\n",
    "\n",
    "\n",
    "    def _plot_func(\n",
    "        world,\n",
    "        matrix_func=matrix_func,\n",
    "        col_linkage_func=col_linkage_func,\n",
    "        row_linkage_func=row_linkage_func,\n",
    "        col_colors_func=col_colors_func,\n",
    "        row_colors_func=row_colors_func,\n",
    "        row_col_annotation_cmap=row_col_annotation_cmap,\n",
    "        scalex=scalex,\n",
    "        scaley=scaley,\n",
    "        cwidth=cwidth,\n",
    "        cheight=cheight,\n",
    "        dwidth=dwidth,\n",
    "        dheight=dheight,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        xticklabels=xticklabels,\n",
    "        yticklabels=yticklabels,\n",
    "        metric=metric,\n",
    "        cbar_pos=cbar_pos,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if col_linkage_func is None:\n",
    "            col_linkage = None\n",
    "        else:\n",
    "            col_linkage = col_linkage_func(world)\n",
    "\n",
    "        if row_linkage_func is None:\n",
    "            row_linkage = None\n",
    "        else:\n",
    "            row_linkage = row_linkage_func(world)\n",
    "        \n",
    "        if col_colors_func is None:\n",
    "            col_colors = None\n",
    "        else:\n",
    "            col_colors = col_colors_func(world).pipe(_scale_to_max_of_one).to_dataframe().applymap(row_col_annotation_cmap)\n",
    "\n",
    "        if row_colors_func is None:\n",
    "            row_colors = None    \n",
    "        else:\n",
    "            row_colors = row_colors_func(world).pipe(_scale_to_max_of_one).to_dataframe().applymap(row_col_annotation_cmap)\n",
    "\n",
    "        matrix_data = matrix_func(world)\n",
    "        \n",
    "        ny, nx = matrix_data.shape\n",
    "        figsize, dendrogram_ratio, colors_ratio = _calculate_clustermap_sizes(\n",
    "            nx,\n",
    "            ny,\n",
    "            scalex=scalex,\n",
    "            scaley=scaley,\n",
    "            cwidth=cwidth,\n",
    "            cheight=cheight,\n",
    "            dwidth=dwidth,\n",
    "            dheight=dheight,\n",
    "        )\n",
    "    #     sf.logging_util.info(matrix_data.shape, applied_scale_kwargs, figsize, dendrogram_ratio, colors_ratio)\n",
    "    \n",
    "        clustermap_kwargs = dict(\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            norm=norm,\n",
    "            cmap=cmap,\n",
    "            xticklabels=xticklabels,\n",
    "            yticklabels=yticklabels,\n",
    "            col_linkage=col_linkage,\n",
    "            row_linkage=row_linkage,\n",
    "            row_colors=row_colors,\n",
    "            col_colors=col_colors,\n",
    "            figsize=figsize,\n",
    "            dendrogram_ratio=dendrogram_ratio,\n",
    "            colors_ratio=colors_ratio,\n",
    "            metric=metric,\n",
    "            cbar_pos=cbar_pos,\n",
    "        )\n",
    "        clustermap_kwargs.update(kwargs)\n",
    "        \n",
    "        grid = sns.clustermap(\n",
    "            matrix_data,\n",
    "            **clustermap_kwargs\n",
    "        )\n",
    "        return grid\n",
    "    return _plot_func\n",
    "    \n",
    "\n",
    "plot_metagenotype = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.metagenotypes.to_genotype_estimates().to_pandas().T,\n",
    "    col_linkage_func=lambda w: w.metagenotypes.linkage(dim='strain', pseudo=1.),\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w\n",
    "            .metagenotypes\n",
    "            .sum('allele')\n",
    "            .mean('position')\n",
    "            .pipe(np.sqrt)\n",
    "            .rename('mean_depth')\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "plot_genotype = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.genotypes.to_pandas().T,\n",
    "    col_linkage_func=lambda w: w.genotypes.linkage(dim='strain'),\n",
    "    row_linkage_func=lambda w: w.genotypes.linkage(dim='position'),\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w\n",
    "            .genotypes\n",
    "            .entropy\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "plot_fuzzed_genotype = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.fuzzed_genotypes.to_pandas().T,\n",
    "    col_linkage_func=lambda w: w.fuzzed_genotypes.linkage(dim='strain'),\n",
    "    row_linkage_func=lambda w: w.genotypes.linkage(dim='position'),\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=mpl.cm.coolwarm,\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w\n",
    "            .genotypes\n",
    "            .entropy\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "plot_missing = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.missingness.to_pandas().T,\n",
    "    col_linkage_func=lambda w: w.genotypes.linkage(dim='strain'),\n",
    "    row_linkage_func=lambda w: w.genotypes.linkage(dim='position'),\n",
    "    metric='cosine',\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=None,\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            1 - w\n",
    "            .missingness\n",
    "            .mean('position')\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "plot_community = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.communities.to_pandas(),\n",
    "    col_linkage_func=lambda w: w.genotypes.linkage(dim='strain'),\n",
    "    metric='cosine',\n",
    "    scalex=0.15,\n",
    "    scaley=0.14,\n",
    "    dheight=1.0,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=None,\n",
    "    norm=mpl.colors.PowerNorm(1/2),\n",
    "    xticklabels=1,\n",
    "    yticklabels=1,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w\n",
    "            .communities\n",
    "            .sum('sample')\n",
    "            .pipe(np.sqrt)\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "def plot_loss_history(trace):\n",
    "    trace = np.array(trace)\n",
    "    plt.plot((trace - trace.min()))\n",
    "    plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model.py\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "import xarray as xr\n",
    "import sfacts as sf\n",
    "from sfacts.pyro_util import all_torch\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "\n",
    "class Structure():\n",
    "    def __init__(self, generative, dims, description, default_hyperparameters=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        *generative* :: Pyro generative model function(shape_dim_0, shape_dim_1, shape_dim_2, ..., **hyper_parameters)\n",
    "        *dims* :: Sequence of names for dim_0, dim_1, dim_2, ...\n",
    "        *description* :: Mapping from model variable to its dims.\n",
    "        *default_hyperparamters* :: Values to use for hyperparameters when not explicitly set.\n",
    "        \"\"\"\n",
    "        if default_hyperparameters is None:\n",
    "            default_hyperparameters = {}\n",
    "        \n",
    "        self.generative = generative\n",
    "        self.dims = dims\n",
    "        self.description = description\n",
    "        self.default_hyperparameters = default_hyperparameters\n",
    "#         _ = self(self._dummy_shape, **all_torch(**self.default_hyperparameters))\n",
    "\n",
    "#         info(f\"New Structure({self.generative}, {self.default_hyperparameters})\")\n",
    "        \n",
    "    def __call__(self, shape, data, hyperparameters):\n",
    "        assert len(shape) == len(self.dims)\n",
    "        conditioned_generative = pyro.condition(self.generative, data)\n",
    "        return conditioned_generative(*shape, **hyperparameters)\n",
    "    \n",
    "#     def condition(self, **data):\n",
    "#         new_data = self.data.copy()\n",
    "#         new_data.update(data)\n",
    "#         return self.__class__(\n",
    "#             generative=self.generative,\n",
    "#             dims=self.dims,\n",
    "#             description=self.description,\n",
    "#             default_hyperparameters=self.default_hyperparameters,\n",
    "#             data=new_data,\n",
    "#         )\n",
    "    \n",
    "    @property\n",
    "    def _dummy_shape(self):\n",
    "        shape = range(1, len(self.dims) + 1)\n",
    "        return shape\n",
    "\n",
    "    def explain_shapes(self, shape=None):\n",
    "        if shape is None:\n",
    "            shape = self._dummy_shape\n",
    "        info(dict(zip(self.dims, shape)))\n",
    "        sf.pyro_util.shape_info(self(shape, **self.default_hyperparameters))\n",
    "\n",
    "    \n",
    "# For decorator use.\n",
    "def structure(dims, description, default_hyperparameters=None):\n",
    "    return partial(Structure, dims=dims, description=description, default_hyperparameters=default_hyperparameters)\n",
    "\n",
    "\n",
    "class ParameterizedModel():\n",
    "    def __init__(self, structure, coords, dtype=torch.float32, device='cpu', data=None, hyperparameters=None):\n",
    "        if hyperparameters is None:\n",
    "            hyperparameters = {}\n",
    "            \n",
    "        if data is None:\n",
    "            data = {}\n",
    "       \n",
    "        self.structure = structure\n",
    "        self.coords = {k: self._coords_or_range(coords[k]) for k in self.structure.dims}\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.hyperparameters = self.structure.default_hyperparameters.copy()\n",
    "        self.hyperparameters.update(hyperparameters)\n",
    "        self.data = data\n",
    "\n",
    "    @property\n",
    "    def sizes(self):\n",
    "        return {k: len(self.coords[k]) for k in self.structure.dims}\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return tuple(self.sizes.values())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"{self.__class__.__name__}\"\n",
    "                f\"({self.structure}, \"\n",
    "                f\"coords={self.coords}, \"\n",
    "                f\"dtype={self.dtype}, \"\n",
    "                f\"device={self.device}, \"\n",
    "                f\"hyperparameters={self.hyperparameters}, \"\n",
    "                f\"data={self.data})\")\n",
    "        \n",
    "    def __call__(self):\n",
    "        # Here's where all the action happens.\n",
    "        # All parameters are cast based on dtype and device.\n",
    "        # The model is conditioned on the\n",
    "        # data, and then called with the shape tuple\n",
    "        # and cast hyperparameters.\n",
    "        return self.structure(\n",
    "            self.shape,\n",
    "            data=all_torch(**self.data, dtype=self.dtype, device=self.device),\n",
    "            hyperparameters=all_torch(**self.hyperparameters, dtype=self.dtype, device=self.device),\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _coords_or_range(coords):\n",
    "        if type(coords) == int:\n",
    "            return range(coords)\n",
    "        else:\n",
    "            return coords\n",
    "        \n",
    "    def with_hyperparameters(self, **hyperparameters):\n",
    "        new_hyperparameters = self.hyperparameters.copy()\n",
    "        new_hyperparameters.update(hyperparameters)\n",
    "        return self.__class__(\n",
    "            structure=self.structure,\n",
    "            coords=self.coords,\n",
    "            dtype=self.dtype,\n",
    "            device=self.device,\n",
    "            hyperparameters=new_hyperparameters,\n",
    "            data=self.data,\n",
    "        )\n",
    "    \n",
    "    def condition(self, **data):\n",
    "        new_data = self.data.copy()\n",
    "        new_data.update(data)\n",
    "        return self.__class__(\n",
    "            structure=self.structure,\n",
    "            coords=self.coords,\n",
    "            dtype=self.dtype,\n",
    "            device=self.device,\n",
    "            hyperparameters=self.hyperparameters,\n",
    "            data=new_data,\n",
    "        )\n",
    "\n",
    "    def format_world(self, data):\n",
    "        out = {}\n",
    "        for k in self.structure.description:\n",
    "            out[k] = xr.DataArray(\n",
    "                data[k],\n",
    "                dims=self.structure.description[k],\n",
    "                coords={dim: self.coords[dim] for dim in self.structure.description[k]},\n",
    "            )\n",
    "        return sf.data.World(xr.Dataset(out))\n",
    "\n",
    "    def simulate(self, n=1, seed=None):\n",
    "        sf.pyro_util.set_random_seed(seed)\n",
    "        obs = pyro.infer.Predictive(self, num_samples=n)()\n",
    "        obs = {k: obs[k].detach().cpu().numpy().squeeze() for k in obs.keys()}\n",
    "        return obs\n",
    "    \n",
    "    def simulate_world(self, seed=None):\n",
    "        return self.format_world(self.simulate(n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_zoo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model_zoo.py\n",
    "import sfacts as sf\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "\n",
    "\n",
    "SHARED_DIMS = ('sample', 'position', 'strain', 'allele')\n",
    "SHARED_DESCRIPTIONS = dict(\n",
    "    gamma=('strain', 'position'),\n",
    "    delta=('strain', 'position'),\n",
    "    rho=('strain',),\n",
    "    pi=('sample', 'strain'),\n",
    "    epsilon=('sample',),\n",
    "    m_hyper_r=(),\n",
    "    mu=('sample',),\n",
    "    nu=('sample', 'position'),\n",
    "    p_noerr=('sample', 'position'),\n",
    "    p=('sample', 'position'),\n",
    "    alpha_hyper_mean=(),\n",
    "    alpha=('sample',),\n",
    "    m=('sample', 'position'),\n",
    "    y=('sample', 'position'),\n",
    "    genotypes=('strain', 'position'),\n",
    "    missingness=('strain', 'position'),\n",
    "    communities=('sample', 'strain'),\n",
    "    metagenotypes=('sample', 'position', 'allele'),\n",
    ")\n",
    "\n",
    "def _mapping_subset(mapping, keys):\n",
    "    return {k: mapping[k] for k in keys}\n",
    "\n",
    "def stickbreaking_betas_to_probs(beta):\n",
    "    beta1m_cumprod = (1 - beta).cumprod(-1)\n",
    "    return torch_pad(beta, (0, 1), value=1) * torch_pad(beta1m_cumprod, (1, 0), value=1)\n",
    "\n",
    "\n",
    "def NegativeBinomialReparam(mu, r):\n",
    "    p = 1.0 / ((r / mu) + 1.0)\n",
    "    logits = torch.logit(p)\n",
    "#     p = torch.clamp(p, eps, 1 - eps)\n",
    "    return dist.NegativeBinomial(\n",
    "        total_count=r,\n",
    "        logits=logits,\n",
    "    )\n",
    "\n",
    "def unit_interval_power_transformation(p, alpha, beta):\n",
    "    log_p = torch.log(p)\n",
    "    log_q = torch.log1p(-p)\n",
    "    log_p_raised = log_p * alpha\n",
    "    log_q_raised = log_q * beta\n",
    "    return torch.exp(\n",
    "        log_p_raised -\n",
    "        torch.logsumexp(torch.stack([log_p_raised, log_q_raised]), dim=0)\n",
    "    )\n",
    "\n",
    "def _pp_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "    # Genotypes\n",
    "    #     delta_hyper_p = pyro.sample('delta_hyper_p', dist.Beta(1., 1.))\n",
    "    with pyro.plate(\"position\", g, dim=-1):\n",
    "        with pyro.plate(\"strain\", s, dim=-2):\n",
    "            _gamma = pyro.sample(\n",
    "                \"_gamma\", dist.Beta(1., 1.)\n",
    "            )\n",
    "            gamma = pyro.deterministic(\n",
    "                'gamma',\n",
    "                unit_interval_power_transformation(_gamma, 1 / gamma_hyper, 1 / gamma_hyper))\n",
    "#                 Position presence/absence\n",
    "            _delta = pyro.sample(\n",
    "                \"_delta\", dist.Beta(1., 1.)\n",
    "            )\n",
    "            delta = pyro.deterministic(\n",
    "                'delta',\n",
    "                unit_interval_power_transformation(\n",
    "                    _delta,\n",
    "                    2 * (1 - delta_hyper_r) / delta_hyper_temp,\n",
    "                    2 * delta_hyper_r / delta_hyper_temp\n",
    "                )\n",
    "            )\n",
    "\n",
    "#                 delta = pyro.sample(\n",
    "#                     'delta',\n",
    "#                     dist.RelaxedBernoulli(\n",
    "#                         temperature=delta_hyper_temp, probs=delta_hyper_p\n",
    "#                     ),\n",
    "#                 )\n",
    "\n",
    "    # These deterministics are accessed by PointMixin class properties.\n",
    "    pyro.deterministic(\"genotypes\", gamma)\n",
    "    pyro.deterministic(\"missingness\", delta)\n",
    "    return gamma, delta\n",
    "\n",
    "\n",
    "def _gsm_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "    with pyro.plate(\"position\", g, dim=-1):\n",
    "        with pyro.plate(\"strain\", s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma',\n",
    "                dist.RelaxedBernoulli(\n",
    "                    temperature=gamma_hyper, probs=0.5,\n",
    "                ),\n",
    "            )\n",
    "            delta = pyro.sample(\n",
    "                'delta',\n",
    "                dist.RelaxedBernoulli(\n",
    "                    temperature=delta_hyper_temp, probs=delta_hyper_r\n",
    "                ),\n",
    "            )\n",
    "    pyro.deterministic(\"genotypes\", gamma)\n",
    "    pyro.deterministic(\"missingness\", delta)\n",
    "    return gamma, delta\n",
    "\n",
    "\n",
    "def _beta_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "    with pyro.plate(\"position\", g, dim=-1):\n",
    "        with pyro.plate(\"strain\", s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma',\n",
    "                dist.Beta(\n",
    "                    gamma_hyper, gamma_hyper,\n",
    "                ),\n",
    "            )\n",
    "            delta = pyro.sample(\n",
    "                'delta',\n",
    "                dist.RelaxedBernoulli(\n",
    "                    delta_hyper_r * delta_hyper_temp,\n",
    "                    (1 - delta_hyper_r) * delta_hyper_temp,\n",
    "                ),\n",
    "            )\n",
    "    pyro.deterministic(\"genotypes\", gamma)\n",
    "    pyro.deterministic(\"missingness\", delta)\n",
    "    return gamma, delta\n",
    "\n",
    "\n",
    "def _hybrid_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "    with pyro.plate(\"position\", g, dim=-1):\n",
    "        with pyro.plate(\"strain\", s, dim=-2):\n",
    "            _gamma = pyro.sample(\n",
    "                \"_gamma\", dist.Beta(1., 1.)\n",
    "            )\n",
    "            gamma = pyro.deterministic(\n",
    "                'gamma',\n",
    "                unit_interval_power_transformation(_gamma, 1 / gamma_hyper, 1 / gamma_hyper))\n",
    "#                 Position presence/absence\n",
    "            delta = pyro.sample(\n",
    "                'delta',\n",
    "                dist.RelaxedBernoulli(\n",
    "                    temperature=delta_hyper_temp, probs=delta_hyper_r\n",
    "                ),\n",
    "            )\n",
    "    pyro.deterministic(\"genotypes\", gamma)\n",
    "    pyro.deterministic(\"missingness\", delta)\n",
    "    return gamma, delta\n",
    "\n",
    "\n",
    "def _dp_rho_module(s, rho_hyper):\n",
    "#         # TODO: Will torch.ones(s) fail when I try to run this on the GPU because it's, by default on the CPU?\n",
    "#         rho = pyro.sample(\n",
    "#             \"rho\", dist.Dirichlet(rho_hyper * torch.ones(s))\n",
    "#         )\n",
    "    # Meta-community composition\n",
    "    rho_betas = pyro.sample('rho_betas', dist.Beta(1., rho_hyper).expand([s - 1]).to_event())\n",
    "    rho = pyro.deterministic('rho', stickbreaking_betas_to_probs(rho_betas))\n",
    "    pyro.deterministic(\"metacommunity\", rho)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "    n, pi_hyper, rho, epsilon_hyper_alpha, epsilon_hyper_beta, alpha_hyper_mean, alpha_hyper_scale, mu_hyper_mean, mu_hyper_scale\n",
    "):\n",
    "    with pyro.plate(\"sample\", n, dim=-1):\n",
    "        # Community composition\n",
    "        pi = pyro.sample(\"pi\", dist.Dirichlet(pi_hyper * rho, validate_args=False))\n",
    "        # Sequencing error\n",
    "        epsilon = pyro.sample(\n",
    "            \"epsilon\", dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta)\n",
    "        ).unsqueeze(-1)\n",
    "        alpha = pyro.sample(\n",
    "            \"alpha\",\n",
    "            dist.LogNormal(loc=torch.log(alpha_hyper_mean), scale=alpha_hyper_scale),\n",
    "        ).unsqueeze(-1)\n",
    "        # Sample coverage\n",
    "        mu = pyro.sample(\n",
    "            \"mu\", dist.LogNormal(loc=torch.log(mu_hyper_mean), scale=mu_hyper_scale)\n",
    "        )\n",
    "    pyro.deterministic(\"communities\", pi)\n",
    "    return pi, epsilon, alpha, mu\n",
    "\n",
    "\n",
    "def _lognormal_alpha_hyper_mean_module(alpha_hyper_hyper_mean, alpha_hyper_hyper_scale):\n",
    "    alpha_hyper_mean = pyro.sample(\n",
    "        \"alpha_hyper_mean\",\n",
    "        dist.LogNormal(\n",
    "            loc=torch.log(alpha_hyper_hyper_mean), scale=alpha_hyper_hyper_scale\n",
    "        ),\n",
    "    )\n",
    "    return alpha_hyper_mean\n",
    "\n",
    "\n",
    "def _betabinomial_observation_module(\n",
    "    pi, gamma, delta, m_hyper_r_mu, m_hyper_r_scale, mu, epsilon, alpha\n",
    "):\n",
    "    # Depth at each position\n",
    "    nu = pyro.deterministic(\"nu\", pi @ delta)\n",
    "    m_hyper_r = pyro.sample(\"m_hyper_r\", dist.LogNormal(loc=m_hyper_r_mu, scale=m_hyper_r_scale))\n",
    "    # TODO: Consider using pyro.distributions.GammaPoisson parameterization?\n",
    "    m = pyro.sample(\n",
    "        \"m\",\n",
    "        NegativeBinomialReparam(\n",
    "            nu * mu.reshape((-1, 1)), m_hyper_r\n",
    "        ).to_event(),\n",
    "    )\n",
    "\n",
    "    # Expected fractions of each allele at each position\n",
    "    p_noerr = pyro.deterministic(\"p_noerr\", pi @ (gamma * delta) / nu)\n",
    "    p = pyro.deterministic(\n",
    "        \"p\", (1 - epsilon / 2) * (p_noerr) + (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "\n",
    "    # Observation\n",
    "    y = pyro.sample(\n",
    "        \"y\",\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m,\n",
    "#             validate_args=False,\n",
    "        ).to_event(),\n",
    "    )\n",
    "    # TODO: Check that dim=0 works?\n",
    "    metagenotypes = pyro.deterministic(\"metagenotypes\", torch.stack([y, m - y], dim=-1))\n",
    "\n",
    "\n",
    "@sf.model.structure(\n",
    "    dims=SHARED_DIMS,\n",
    "    description=_mapping_subset(\n",
    "        SHARED_DESCRIPTIONS,\n",
    "        ['rho', 'epsilon', 'm_hyper_r', 'mu',\n",
    "         'nu', 'p_noerr', 'p', 'm', 'y',\n",
    "         'alpha_hyper_mean', 'alpha',\n",
    "         'genotypes', 'missingness',\n",
    "         'communities', 'metagenotypes']\n",
    "    ),\n",
    "    default_hyperparameters=dict(\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_r=0.9,\n",
    "        rho_hyper=5.0,\n",
    "        pi_hyper=0.2,\n",
    "        epsilon_hyper_alpha=1.5,\n",
    "        epsilon_hyper_beta=1.5 / 0.01,\n",
    "        mu_hyper_mean=1.0,\n",
    "        mu_hyper_scale=10.0,\n",
    "        m_hyper_r_mu=1.,\n",
    "        m_hyper_r_scale=1.,\n",
    "        alpha_hyper_hyper_mean=100.0,\n",
    "        alpha_hyper_hyper_scale=1.0,\n",
    "        alpha_hyper_scale=0.5,\n",
    "    ),\n",
    ")\n",
    "def pp_fuzzy_missing_dp_betabinomial_metagenotype(\n",
    "        n,\n",
    "        g,\n",
    "        s,\n",
    "        a,\n",
    "        gamma_hyper,\n",
    "        delta_hyper_r,\n",
    "        delta_hyper_temp,\n",
    "        rho_hyper,  #=1.0,\n",
    "        pi_hyper,  #=1.0,\n",
    "        alpha_hyper_hyper_mean,  #=100.0,\n",
    "        alpha_hyper_hyper_scale,  #=1.0,\n",
    "        alpha_hyper_scale,  #=0.5,\n",
    "        epsilon_hyper_alpha,  #=1.5,\n",
    "        epsilon_hyper_beta,  #=1.5 / 0.01,\n",
    "        mu_hyper_mean,  #=1.0,\n",
    "        mu_hyper_scale,  #=1.0,\n",
    "        m_hyper_r_mu,\n",
    "        m_hyper_r_scale,\n",
    "    ):\n",
    "    gamma, delta = _pp_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp)\n",
    "    rho = _dp_rho_module(s, rho_hyper)\n",
    "    alpha_hyper_mean = _lognormal_alpha_hyper_mean_module(\n",
    "        alpha_hyper_hyper_mean, alpha_hyper_hyper_scale\n",
    "    )\n",
    "    pi, epsilon, alpha, mu = _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "        n, pi_hyper, rho, epsilon_hyper_alpha, epsilon_hyper_beta, alpha_hyper_mean, alpha_hyper_scale, mu_hyper_mean, mu_hyper_scale\n",
    "    )\n",
    "    _betabinomial_observation_module(\n",
    "        pi, gamma, delta, m_hyper_r_mu, m_hyper_r_scale, mu, epsilon, alpha\n",
    "    )\n",
    "    \n",
    "@sf.model.structure(\n",
    "    dims=SHARED_DIMS,\n",
    "    description=_mapping_subset(\n",
    "        SHARED_DESCRIPTIONS,\n",
    "        ['rho', 'epsilon', 'm_hyper_r', 'mu',\n",
    "         'nu', 'p_noerr', 'p', 'm', 'y',\n",
    "         'alpha_hyper_mean', 'alpha',\n",
    "         'genotypes', 'missingness',\n",
    "         'communities', 'metagenotypes']\n",
    "    ),\n",
    "    default_hyperparameters=dict(\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_r=0.9,\n",
    "        rho_hyper=5.0,\n",
    "        pi_hyper=0.2,\n",
    "        epsilon_hyper_alpha=1.5,\n",
    "        epsilon_hyper_beta=1.5 / 0.01,\n",
    "        mu_hyper_mean=1.0,\n",
    "        mu_hyper_scale=10.0,\n",
    "        m_hyper_r_mu=1.,\n",
    "        m_hyper_r_scale=1.,\n",
    "        alpha_hyper_hyper_mean=100.0,\n",
    "        alpha_hyper_hyper_scale=1.0,\n",
    "        alpha_hyper_scale=0.5,\n",
    "    ),\n",
    ")\n",
    "def gsm_fuzzy_missing_dp_betabinomial_metagenotype(\n",
    "        n,\n",
    "        g,\n",
    "        s,\n",
    "        a,\n",
    "        gamma_hyper,\n",
    "        delta_hyper_r,\n",
    "        delta_hyper_temp,\n",
    "        rho_hyper,  #=1.0,\n",
    "        pi_hyper,  #=1.0,\n",
    "        alpha_hyper_hyper_mean,  #=100.0,\n",
    "        alpha_hyper_hyper_scale,  #=1.0,\n",
    "        alpha_hyper_scale,  #=0.5,\n",
    "        epsilon_hyper_alpha,  #=1.5,\n",
    "        epsilon_hyper_beta,  #=1.5 / 0.01,\n",
    "        mu_hyper_mean,  #=1.0,\n",
    "        mu_hyper_scale,  #=1.0,\n",
    "        m_hyper_r_mu,\n",
    "        m_hyper_r_scale,\n",
    "    ):\n",
    "    gamma, delta = _gsm_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp)\n",
    "    rho = _dp_rho_module(s, rho_hyper)\n",
    "    alpha_hyper_mean = _lognormal_alpha_hyper_mean_module(\n",
    "        alpha_hyper_hyper_mean, alpha_hyper_hyper_scale\n",
    "    )\n",
    "    pi, epsilon, alpha, mu = _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "        n, pi_hyper, rho, epsilon_hyper_alpha, epsilon_hyper_beta, alpha_hyper_mean, alpha_hyper_scale, mu_hyper_mean, mu_hyper_scale\n",
    "    )\n",
    "    _betabinomial_observation_module(\n",
    "        pi, gamma, delta, m_hyper_r_mu, m_hyper_r_scale, mu, epsilon, alpha\n",
    "    )\n",
    "    \n",
    "    \n",
    "@sf.model.structure(\n",
    "    dims=SHARED_DIMS,\n",
    "    description=_mapping_subset(\n",
    "        SHARED_DESCRIPTIONS,\n",
    "        ['rho', 'epsilon', 'm_hyper_r', 'mu',\n",
    "         'nu', 'p_noerr', 'p', 'm', 'y',\n",
    "         'alpha_hyper_mean', 'alpha',\n",
    "         'genotypes', 'missingness',\n",
    "         'communities', 'metagenotypes']\n",
    "    ),\n",
    "    default_hyperparameters=dict(\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_r=0.9,\n",
    "        rho_hyper=5.0,\n",
    "        pi_hyper=0.2,\n",
    "        epsilon_hyper_alpha=1.5,\n",
    "        epsilon_hyper_beta=1.5 / 0.01,\n",
    "        mu_hyper_mean=1.0,\n",
    "        mu_hyper_scale=10.0,\n",
    "        m_hyper_r_mu=1.,\n",
    "        m_hyper_r_scale=1.,\n",
    "        alpha_hyper_hyper_mean=100.0,\n",
    "        alpha_hyper_hyper_scale=1.0,\n",
    "        alpha_hyper_scale=0.5,\n",
    "    ),\n",
    ")\n",
    "def hybrid_fuzzy_missing_dp_betabinomial_metagenotype(\n",
    "        n,\n",
    "        g,\n",
    "        s,\n",
    "        a,\n",
    "        gamma_hyper,\n",
    "        delta_hyper_r,\n",
    "        delta_hyper_temp,\n",
    "        rho_hyper,  #=1.0,\n",
    "        pi_hyper,  #=1.0,\n",
    "        alpha_hyper_hyper_mean,  #=100.0,\n",
    "        alpha_hyper_hyper_scale,  #=1.0,\n",
    "        alpha_hyper_scale,  #=0.5,\n",
    "        epsilon_hyper_alpha,  #=1.5,\n",
    "        epsilon_hyper_beta,  #=1.5 / 0.01,\n",
    "        mu_hyper_mean,  #=1.0,\n",
    "        mu_hyper_scale,  #=1.0,\n",
    "        m_hyper_r_mu,\n",
    "        m_hyper_r_scale,\n",
    "    ):\n",
    "    gamma, delta = _hybrid_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp)\n",
    "    rho = _dp_rho_module(s, rho_hyper)\n",
    "    alpha_hyper_mean = _lognormal_alpha_hyper_mean_module(\n",
    "        alpha_hyper_hyper_mean, alpha_hyper_hyper_scale\n",
    "    )\n",
    "    pi, epsilon, alpha, mu = _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "        n, pi_hyper, rho, epsilon_hyper_alpha, epsilon_hyper_beta, alpha_hyper_mean, alpha_hyper_scale, mu_hyper_mean, mu_hyper_scale\n",
    "    )\n",
    "    _betabinomial_observation_module(\n",
    "        pi, gamma, delta, m_hyper_r_mu, m_hyper_r_scale, mu, epsilon, alpha\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/estimation.py\n",
    "import sfacts as sf\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.decomposition import non_negative_factorization\n",
    "# from sfacts.genotype import genotype_pdist, adjust_genotype_by_missing\n",
    "from sfacts.pyro_util import all_torch\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy as sp\n",
    "# from scipy.spatial.distance import squareform\n",
    "import pyro\n",
    "# import pyro.distributions as dist\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sfacts.logging_util import info\n",
    "# from sfacts.model import condition_model\n",
    "\n",
    "\n",
    "# def cluster_genotypes(gamma, thresh, quiet=True, precomputed_pdist=None):\n",
    "# \n",
    "#     if precomputed_pdist is None:\n",
    "#         compressed_dmat = genotype_pdist(gamma, quiet=quiet)\n",
    "#     else:\n",
    "#         compressed_dmat = precomputed_pdist\n",
    "# \n",
    "#     clust = pd.Series(\n",
    "#         AgglomerativeClustering(\n",
    "#             n_clusters=None,\n",
    "#             affinity=\"precomputed\",\n",
    "#             linkage=\"complete\",\n",
    "#             distance_threshold=thresh,\n",
    "#         )\n",
    "#         .fit(squareform(compressed_dmat))\n",
    "#         .labels_\n",
    "#     )\n",
    "# \n",
    "#     return clust, compressed_dmat\n",
    "# \n",
    "# \n",
    "# def initialize_parameters_by_clustering_samples(\n",
    "#     metagenotype,\n",
    "#     thresh=None,\n",
    "#     additional_strains_factor=0.5,\n",
    "#     quiet=True,\n",
    "#     precomputed_pdist=None,\n",
    "# ):\n",
    "#     n, g, a = metagenotype.shape\n",
    "# \n",
    "#     sample_genotype = sf.genotype.metagenotype_to_genotype(metagenotype)\n",
    "#     clust, cdmat = cluster_genotypes(\n",
    "#         sample_genotype,\n",
    "#         thresh=thresh,\n",
    "#         quiet=quiet,\n",
    "#         precomputed_pdist=precomputed_pdist,\n",
    "#     )\n",
    "#     s_clust = len(clust.value_counts())\n",
    "#     \n",
    "#     # FIXME: This probably doesn't work using xarray.\n",
    "#     # How to use pandas style aggregation here?\n",
    "#     clust_metagenotype = metagenotype.groupby(sample=clust)\n",
    "#     s_additional_haplotypes = int(additional_strains_factor * s_clust)\n",
    "#     s_init = s_clust + s_additional_haplotypes\n",
    "# \n",
    "#     # FIXME: This probably doesn't work using xarray.\n",
    "#     # The \"additional_haplotypes\" cells will need to be indexed.\n",
    "#     # Consider doing the matrix building in numpy space,\n",
    "#     # and then apply genotype.build_genotype(gamma).\n",
    "#     gamma_init = xr.concat(\n",
    "#         [\n",
    "#             clust_metagenotype,\n",
    "#             np.ones((s_additional_haplotypes, g)) * 0.5,\n",
    "#         ]\n",
    "#     ).values\n",
    "# \n",
    "#     pi_init = np.ones((n, s_init))\n",
    "#     for i in range(n):\n",
    "#         pi_init[i, clust[i]] = s_init - 1\n",
    "#     pi_init /= pi_init.sum(1, keepdims=True)\n",
    "# \n",
    "#     assert (~np.isnan(gamma_init)).all()\n",
    "# \n",
    "#     return gamma_init, pi_init, cdmat\n",
    "# \n",
    "# \n",
    "# def initialize_parameters_by_nmf(\n",
    "#     y, m, s, quiet=True, solver=\"mu\", alpha=100.0, l1_ratio=1.0, tol=1e-2, **kwargs\n",
    "# ):\n",
    "#     n, g = y.shape\n",
    "# \n",
    "#     # Fit to counts of both reference and alternative alleles by stacking them.\n",
    "#     stacked_metagenotype = np.concatenate([y, m - y], axis=1)\n",
    "#     pi_unnorm, gamma_unnorm, _ = non_negative_factorization(\n",
    "#         stacked_metagenotype,\n",
    "#         n_components=s,\n",
    "#         solver=solver,\n",
    "#         verbose=int(not quiet),\n",
    "#         alpha=alpha,\n",
    "#         l1_ratio=l1_ratio,\n",
    "#         tol=tol,\n",
    "#         **kwargs,\n",
    "#     )\n",
    "# \n",
    "#     # TODO: Find a more principled way to convert pi_unnorm into pi_init.\n",
    "#     pi_init = (pi_unnorm + 1) / (pi_unnorm + 1).sum(1, keepdims=True)\n",
    "#     gamma_init = (gamma_unnorm[:, :g] + 1) / (\n",
    "#         gamma_unnorm[:, :g] + gamma_unnorm[:, -g:] + 2\n",
    "#     )\n",
    "# \n",
    "#     return gamma_init, pi_init, None\n",
    "\n",
    "\n",
    "def estimate_parameters(\n",
    "    model,\n",
    "#     data,\n",
    "    dtype=torch.float32,\n",
    "    device=\"cpu\",\n",
    "    initialize_params=None,\n",
    "    maxiter=10000,\n",
    "    lagA=20,\n",
    "    lagB=100,\n",
    "    opt=pyro.optim.Adamax({\"lr\": 1e-2}, {\"clip_norm\": 100}),\n",
    "    quiet=False,\n",
    "    seed=None,\n",
    "#     **model_kwargs,\n",
    "):\n",
    "    if initialize_params is None:\n",
    "        initialize_params = {}\n",
    "        \n",
    "    sf.pyro_util.set_random_seed(seed, warn=(not quiet))\n",
    "\n",
    "#     conditioned_model = model.with_hyperparameters(**hyperparameters).condition(**data)\n",
    "    \n",
    "#     condition_model(\n",
    "#         model,\n",
    "#         data=data,\n",
    "#         dtype=dtype,\n",
    "#         device=device,\n",
    "#         **model_kwargs,\n",
    "#     )\n",
    "\n",
    "    _guide = pyro.infer.autoguide.AutoLaplaceApproximation(\n",
    "        model,\n",
    "        init_loc_fn=pyro.infer.autoguide.initialization.init_to_value(\n",
    "            values=all_torch(**initialize_params, dtype=dtype, device=device)\n",
    "        ),\n",
    "    )\n",
    "    svi = pyro.infer.SVI(\n",
    "        model, _guide, opt, loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    history = []\n",
    "    pbar = tqdm(range(maxiter), disable=quiet)\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                pbar.close()\n",
    "                raise RuntimeError(\"ELBO NaN?\")\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if i % 10 == 0:\n",
    "                if i > lagB:\n",
    "                    delta = history[-2] - history[-1]\n",
    "                    delta_lagA = (history[-lagA] - history[-1]) / lagA\n",
    "                    delta_lagB = (history[-lagB] - history[-1]) / lagB\n",
    "                    pbar.set_postfix(\n",
    "                        {\n",
    "                            \"ELBO\": history[-1],\n",
    "                            \"delta\": delta,\n",
    "                            f\"lag{lagA}\": delta_lagA,\n",
    "                            f\"lag{lagB}\": delta_lagB,\n",
    "                        }\n",
    "                    )\n",
    "                    if (delta_lagA <= 0) and (delta_lagB <= 0):\n",
    "                        pbar.close()\n",
    "                        info(\"Converged\", quiet=quiet)\n",
    "                        break\n",
    "    except KeyboardInterrupt:\n",
    "        pbar.close()\n",
    "        info(\"Interrupted\", quiet=quiet)\n",
    "        pass\n",
    "    est = pyro.infer.Predictive(model, guide=_guide, num_samples=1)()\n",
    "    est = {k: est[k].detach().cpu().numpy().mean(0).squeeze() for k in est.keys()}\n",
    "\n",
    "    if device.startswith(\"cuda\"):\n",
    "        #         info(\n",
    "        #             \"CUDA available mem: {}\".format(\n",
    "        #                 torch.cuda.get_device_properties(0).total_memory\n",
    "        #             ),\n",
    "        #         )\n",
    "        #         info(\"CUDA reserved mem: {}\".format(torch.cuda.memory_reserved(0)))\n",
    "        #         info(\"CUDA allocated mem: {}\".format(torch.cuda.memory_allocated(0)))\n",
    "        #         info(\n",
    "        #             \"CUDA free mem: {}\".format(\n",
    "        #                 torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)\n",
    "        #             )\n",
    "        #         )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model.format_world(est), history\n",
    "\n",
    "\n",
    "# def merge_similar_genotypes(\n",
    "#     gamma,\n",
    "#     pi,\n",
    "#     thresh,\n",
    "#     delta=None,\n",
    "#     progress=False,\n",
    "# ):\n",
    "#     if delta is None:\n",
    "#         delta = np.ones_like(gamma)\n",
    "\n",
    "#     gamma_adjust = adjust_genotype_by_missing(gamma, delta)\n",
    "\n",
    "#     clust, dmat = cluster_genotypes(gamma_adjust, thresh=thresh, progress=progress)\n",
    "#     gamma_mean = (\n",
    "#         pd.DataFrame(pd.DataFrame(gamma_adjust))\n",
    "#         .groupby(clust)\n",
    "#         .apply(lambda x: sp.special.expit(sp.special.logit(x)).mean(0))\n",
    "#         .values\n",
    "#     )\n",
    "#     delta_mean = pd.DataFrame(pd.DataFrame(delta)).groupby(clust).mean().values\n",
    "#     pi_sum = pd.DataFrame(pd.DataFrame(pi)).groupby(clust, axis=\"columns\").sum().values\n",
    "\n",
    "#     return gamma_mean, pi_sum, delta_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/evaluation.py\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "# def binary_entropy(p):\n",
    "#     q = 1 - p\n",
    "#     ent = -(p * np.log2(p) + q * np.log2(q))\n",
    "#     return ent\n",
    "\n",
    "\n",
    "# def sum_binary_entropy(p, normalize=False, axis=None):\n",
    "#     q = 1 - p\n",
    "#     ent = np.sum(-(p * np.log2(p) + q * np.log2(q)), axis=axis)\n",
    "#     if normalize:\n",
    "#         ent = ent / p.shape[axis]\n",
    "#     return ent\n",
    "\n",
    "\n",
    "# def mean_masked_genotype_entropy(gamma, delta):\n",
    "#     return (binary_entropy(gamma) * delta).mean(1)\n",
    "\n",
    "\n",
    "# def sample_mean_masked_genotype_entropy(pi, gamma, delta):\n",
    "#     return (pi @ mean_masked_genotype_entropy(gamma, delta).reshape((-1, 1))).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "def _rmse(x, y):\n",
    "    return np.sqrt(np.square(x - y).mean())\n",
    "\n",
    "\n",
    "def _rss(x, y):\n",
    "    return np.sqrt(np.square(x - y).sum())\n",
    "\n",
    "\n",
    "def match_genotypes(worldA, worldB):\n",
    "    gammaA = worldA.genotypes.data.to_pandas()\n",
    "    gammaB = worldB.genotypes.data.to_pandas()\n",
    "\n",
    "    g = gammaA.shape[1]\n",
    "    dist = pd.DataFrame(cdist(gammaA, gammaB, metric=\"cityblock\"))\n",
    "    return dist.idxmin(axis=1), dist.min(axis=1) / g\n",
    "\n",
    "\n",
    "def weighted_genotype_error(worldA, worldB):\n",
    "    _, accuracy = match_genotypes(worldA, worldB)\n",
    "    error = xr.DataArray(accuracy, dims=('strain',), coords=dict(strain=worldA.strain))\n",
    "    total_coverage = (worldA.data.mu * worldA.data.communities).sum(\"sample\")\n",
    "    return float((error * total_coverage).sum() / total_coverage.sum())\n",
    "\n",
    "\n",
    "def community_error(worldA, worldB, reps=99):\n",
    "    piA = worldA.communities.to_pandas()\n",
    "    piB = worldB.communities.to_pandas()\n",
    "    bcA = 1 - pdist(piA, metric=\"braycurtis\")\n",
    "    bcB = 1 - pdist(piB, metric=\"braycurtis\")\n",
    "    return _rmse(bcA, bcB)\n",
    "\n",
    "\n",
    "def community_error_test(worldA, worldB, reps=99):\n",
    "    pi_sim = worldA.communities.to_pandas()\n",
    "    pi_fit = worldB.communities.to_pandas()\n",
    "    \n",
    "    bc_sim = 1 - pdist(pi_sim, metric=\"braycurtis\")\n",
    "    bc_fit = 1 - pdist(pi_fit, metric=\"braycurtis\")\n",
    "    err = _rmse(bc_sim, bc_fit)\n",
    "\n",
    "    null = []\n",
    "    n = len(bc_sim)\n",
    "    for i in range(reps):\n",
    "        bc_sim_permute = np.random.permutation(bc_sim)\n",
    "        null.append(_rmse(bc_sim, bc_sim_permute))\n",
    "    null = np.array(null)\n",
    "\n",
    "    return err, null, err / np.mean(null), (np.sort(null) < err).mean()\n",
    "\n",
    "\n",
    "# def metacommunity_composition_rss(worldA, worldB):\n",
    "#     pi_sim = worldA.communities.to_dataframe()\n",
    "#     pi_fit = worldB.communities.to_dataframe()\n",
    "#     mean_sim = pi_sim.mean(0)\n",
    "#     mean_fit = pi_fit.mean(0)\n",
    "#     s_sim = mean_sim.shape[0]\n",
    "#     s_fit = mean_fit.shape[0]\n",
    "#     s = max(s_sim, s_fit)\n",
    "#     mean_sim = np.sort(np.pad(mean_sim, pad_width=(0, s - s_sim)))\n",
    "#     mean_fit = np.sort(np.pad(mean_fit, pad_width=(0, s - s_fit)))\n",
    "#     return _rss(mean_sim, mean_fit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data / Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check on sfacts/data.py\n",
    "obs = (\n",
    "    sf.data.Metagenotypes.load('data/ucfmt.sp-100022.gtpro-pileup.nc')\n",
    "    .select_variable_positions(incid_thresh=0.2)\n",
    "    .select_samples_with_coverage(0.1)\n",
    ")\n",
    "\n",
    "obs.genotypes.validate_constraints()\n",
    "\n",
    "print(obs.sizes)\n",
    "# sf.plot.plot_genotype(d.genotypes.lift(lambda d: d.isel(strain=range(30), position=range(1000))))\n",
    "sf.plot.plot_genotype(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_similarity(d.genotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unit_interval_power_transformation(p, alpha, beta):\n",
    "#     log_p = torch.log(p)\n",
    "#     log_q = torch.log1p(-p)\n",
    "#     log_p_raised = log_p * alpha\n",
    "#     log_q_raised = log_q * beta\n",
    "#     return torch.exp(\n",
    "#         log_p_raised -\n",
    "#         torch.logsumexp(torch.stack([log_p_raised, log_q_raised]), dim=0)\n",
    "#     )\n",
    "\n",
    "x = pyro.sample('x', dist.Beta(1., 1.).expand([10000]))\n",
    "b = pyro.sample('b', dist.Beta(0.2, 0.8).expand([10000]))\n",
    "\n",
    "r = 0.6\n",
    "temp = 1e-1\n",
    "x_pt = sf.model_zoo.unit_interval_power_transformation(x, 2 * (1 - r) / temp, 2 * r / temp)\n",
    "\n",
    "bins = np.linspace(0., 1.0, num=10)\n",
    "\n",
    "plt.hist(x.numpy(), bins=bins, density=True)\n",
    "# plt.hist(b.numpy(), bins=bins, alpha=0.5, density=True)\n",
    "plt.hist(x_pt.numpy(), bins=bins, density=True, alpha=0.5)\n",
    "print(x_pt.numpy().mean(), x_pt.numpy().min(), x_pt.numpy().max())\n",
    "# plt.yscale('log')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sim = sf.model.ParameterizedModel(\n",
    "    sf.model_zoo.pp_fuzzy_missing_dp_betabinomial_metagenotype,\n",
    "    coords=dict(\n",
    "        sample=100,\n",
    "        position=500,\n",
    "        strain=20,\n",
    "        allele=['alt', 'ref'],\n",
    "    ),\n",
    "    hyperparameters=dict(\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_r=0.9,\n",
    "        delta_hyper_temp=0.01,\n",
    "        rho_hyper=4.,\n",
    "        pi_hyper=0.4,\n",
    "#         alpha_hyper_hyper_mean=100.0,\n",
    "#         alpha_hyper_hyper_scale=1.0,\n",
    "#         alpha_hyper_scale=0.5,\n",
    "#         epsilon_hyper_alpha=1.5,\n",
    "#         epsilon_hyper_beta=1.5 / 0.01,\n",
    "        mu_hyper_mean=50.0,\n",
    "        mu_hyper_scale=2.0,\n",
    "        m_hyper_r_mu=5.,\n",
    "        m_hyper_r_scale=2.,\n",
    "    )\n",
    ").simulate_world()\n",
    "\n",
    "sf.plot.plot_genotype(sim.metagenotypes)\n",
    "sf.plot.plot_community(sim)\n",
    "sf.plot.plot_genotype(sim)\n",
    "sf.plot.plot_missing(sim)\n",
    "None\n",
    "# sf.plot.plot_genotype(sf.data.Metagenotypes.from_counts_and_totals(sim0.data['y'], sim0.data['m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(sim.fuzzed_genotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "obs = sf.data.Metagenotypes.load('data/ucfmt.sp-100022.gtpro-pileup.nc').select_variable_positions(incid_thresh=0.1).select_samples_with_coverage(cvrg_thresh=0.5)\n",
    "print(obs.sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgen = obs.metagenotypes\n",
    "\n",
    "print(mgen.sizes)\n",
    "\n",
    "model_fit = (\n",
    "    sf.model.ParameterizedModel(\n",
    "        sf.model_zoo.pp_fuzzy_missing_dirichlet_binomial_metagenotype,\n",
    "        coords=dict(\n",
    "            sample=mgen.sample.values,\n",
    "            position=mgen.position.values,\n",
    "            allele=mgen.allele.values,\n",
    "            strain=range(20),\n",
    "        ),\n",
    "        hyperparameters=dict(\n",
    "            gamma_hyper=0.1,\n",
    "            delta_hyper_r=0.9,\n",
    "            delta_hyper_temp=0.1,\n",
    "            rho_hyper=1.,\n",
    "            pi_hyper=0.5,\n",
    "            epsilon_hyper_alpha=1.5,\n",
    "            epsilon_hyper_beta=1.5 / 0.01,\n",
    "            mu_hyper_mean=10.0,\n",
    "            mu_hyper_scale=10.0,\n",
    "        )\n",
    "    )\n",
    "    .condition(\n",
    "        **mgen.to_counts_and_totals()\n",
    "#         y=metagenotype_sim.data.sel(allele='alt').values,\n",
    "#         m=metagenotype_sim.data.sum('allele').values\n",
    "    )\n",
    ")\n",
    "\n",
    "est, history = sf.estimation.estimate_parameters(\n",
    "    model_fit,\n",
    "    opt=pyro.optim.Adamax({\"lr\": 1e-0}, {\"clip_norm\": 100})\n",
    ")\n",
    "\n",
    "sf.plot.plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(est)\n",
    "sf.plot.plot_community(est)\n",
    "sf.plot.plot_missing(est)\n",
    "\n",
    "# sf.plot.plot_missing_comparison(dict(sim=sim0, est=est))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgen = sim.metagenotypes\n",
    "\n",
    "print(mgen.sizes)\n",
    "\n",
    "model_fit = (\n",
    "    sf.model.ParameterizedModel(\n",
    "        sf.model_zoo.pp_fuzzy_missing_dp_betabinomial_metagenotype,\n",
    "        coords=dict(\n",
    "            sample=mgen.sample.values,\n",
    "            position=mgen.position.values,\n",
    "            allele=mgen.allele.values,\n",
    "            strain=range(20),\n",
    "        ),\n",
    "        hyperparameters=dict(\n",
    "            gamma_hyper=0.1,\n",
    "            delta_hyper_r=0.9,\n",
    "            delta_hyper_temp=0.1,\n",
    "            rho_hyper=1.,\n",
    "            pi_hyper=0.5,\n",
    "#             alpha_hyper_hyper_mean=100.0,\n",
    "#             alpha_hyper_hyper_scale=1.0,\n",
    "#             alpha_hyper_scale=0.5,\n",
    "            epsilon_hyper_alpha=1.5,\n",
    "            epsilon_hyper_beta=1.5 / 0.01,\n",
    "            mu_hyper_mean=10.0,\n",
    "            mu_hyper_scale=10.0,\n",
    "\n",
    "        )\n",
    "    )\n",
    "    .condition(\n",
    "        **mgen.to_counts_and_totals()\n",
    "#         y=metagenotype_sim.data.sel(allele='alt').values,\n",
    "#         m=metagenotype_sim.data.sum('allele').values\n",
    "    )\n",
    ")\n",
    "\n",
    "est, history = sf.estimation.estimate_parameters(\n",
    "    model_fit,\n",
    "    opt=pyro.optim.Adamax({\"lr\": 1e-0}, {\"clip_norm\": 100})\n",
    ")\n",
    "\n",
    "sf.plot.plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_comparison(dict(sim=sim, est=est))\n",
    "sf.plot.plot_community_comparison(dict(sim=sim, est=est))\n",
    "# sf.plot.plot_missing_comparison(dict(sim=sim0, est=est))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.evaluation.community_accuracy_test(sim.communities.data.values, est.communities.data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "delta_hyper_temp, delta_hyper_p = 0.1, 0.9\n",
    "x = pyro.sample('delta', dist.Beta(delta_hyper_temp * (delta_hyper_p), delta_hyper_temp * (1 - delta_hyper_p)).expand([10000])).cpu().numpy()\n",
    "plt.hist(x, bins=100)\n",
    "print(x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_hyper_alpha, epsilon_hyper_beta = 1.5, 1.5 / 0.01\n",
    "plt.hist(pyro.sample('epsilon_hyper', dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta).expand([10000])).cpu().numpy(), bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pyro.sample('test', sf.model.NegativeBinomialReparam(torch.tensor(10.), r=torch.tensor(1.), eps=torch.tensor(1e-5)).expand([1000])).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.pyro_util.shape_info(sf.model.model, n=100, g=200, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimShape-1: Small study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "pyro.util.set_rng_seed(seed)\n",
    "\n",
    "n_sim = 100\n",
    "g_sim = 5000\n",
    "s_sim = 20\n",
    "\n",
    "sim1 = sf.model.simulate(\n",
    "    sf.model.condition_model(\n",
    "        sf.model.model,\n",
    "        data=dict(\n",
    "            alpha_hyper_mean=100.\n",
    "        ),\n",
    "        n=n_sim,\n",
    "        g=g_sim,\n",
    "        s=s_sim,\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_p=0.7,\n",
    "        pi_hyper=0.5,\n",
    "        rho_hyper=10.,\n",
    "        mu_hyper_mean=2.,\n",
    "        mu_hyper_scale=0.5,\n",
    "        m_hyper_r=10.,\n",
    "        alpha_hyper_scale=0.5,\n",
    "        epsilon_hyper_alpha=1.5,\n",
    "        epsilon_hyper_beta=1.5/0.01,\n",
    "        device='cpu'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plt = 100\n",
    "g_plt = 200\n",
    "s_plt = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_community(sim1['pi'][:s_plt, :n_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(\n",
    "    sf.genotype.counts_to_p_estimate(\n",
    "        sim1['y'][:n_plt, :g_plt],\n",
    "        sim1['m'][:n_plt, :g_plt]),\n",
    "    linkage_kw=dict(progress=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_similarity(sf.genotype.counts_to_p_estimate(sim1['y'][:n_plt, :g_plt], sim1['m'][:n_plt, :g_plt]), linkage_kw=dict(progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(sim1['gamma'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_missing(sim1['delta'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_missing(sim1['nu'][:n_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(sim1['m'][:n_plt, :g_plt], norm=mpl.colors.SymLogNorm(linthresh=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_similarity(sim1['gamma'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1['epsilon'], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1['alpha'], bins=20)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fit = 1000  # sim1['y'].shape[1]\n",
    "n_fit = sim1['y'].shape[0]\n",
    "\n",
    "sim1_gamma_init, sim1_pi_init, sim1_dmat = sf.estimation.initialize_parameters_by_clustering_samples(\n",
    "    sim1['y'][:n_fit, :g_fit],\n",
    "    sim1['m'][:n_fit, :g_fit],\n",
    "    thresh=0.05,\n",
    "    additional_strains_factor=0.,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "print(sim1_pi_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(sim1_gamma_init[:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_similarity(sim1_gamma_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_community(sim1_pi_init[:n_plt, :s_plt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fit = sim1_gamma_init.shape[0]\n",
    "initialize_params = dict(gamma=sim1_gamma_init, pi=sim1_pi_init)\n",
    "\n",
    "sim1_fit1, history = sf.estimation.estimate_parameters(\n",
    "    sf.model.model,\n",
    "    data=dict(y=sim1['y'][:, :g_fit], m=sim1['m'][:, :g_fit]),\n",
    "    n=n_fit,\n",
    "    g=g_fit,\n",
    "    s=s_fit,\n",
    "    gamma_hyper=0.1,\n",
    "    pi_hyper=1.0,\n",
    "    rho_hyper=0.5,\n",
    "    mu_hyper_mean=5,\n",
    "    mu_hyper_scale=5.,\n",
    "    m_hyper_r=10.,\n",
    "    delta_hyper_temp=0.1,\n",
    "    delta_hyper_p=0.9,\n",
    "    alpha_hyper_hyper_mean=100.,\n",
    "    alpha_hyper_hyper_scale=10.,\n",
    "    alpha_hyper_scale=0.5,\n",
    "    epsilon_hyper_alpha=1.5,\n",
    "    epsilon_hyper_beta=1.5 / 0.01,\n",
    "    initialize_params=initialize_params,\n",
    "    device='cpu',\n",
    "    lag=100,\n",
    "    lr=1e-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1_fit1_gamma_merge, sim1_fit1_pi_merge, sim1_fit1_delta_merge  = sf.estimation.merge_similar_genotypes(\n",
    "    sim1_fit1['gamma'],\n",
    "    sim1_fit1['pi'],\n",
    "    delta=sim1_fit1['delta'],\n",
    "    thresh=0.1,\n",
    ")\n",
    "\n",
    "# print(sim1_gamma_init.shape[0], sim1_fit1['gamma'].shape[0], sim1_fit1_gamma_merge.shape[0])\n",
    "print(sim1_fit1['gamma'].shape[0], sim1_fit1_gamma_merge.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1_gamma_adjusted = sf.genotype.mask_missing_genotype(sim1['gamma'][:, :g_fit], sim1['delta'][:, :g_fit])\n",
    "sim1_fit1_gamma_adjusted = sf.genotype.mask_missing_genotype(sim1_fit1['gamma'], sim1_fit1['delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_comparison(\n",
    "    data=dict(\n",
    "        true=sim1_gamma_adjusted[:, :g_plt],\n",
    "#         fit=sim1_fit1['gamma'][:, :g_plt],\n",
    "        adj=sim1_fit1_gamma_adjusted[:, :g_plt],\n",
    "#         init=sim1_gamma_init,\n",
    "#         merg=sim1_fit1_gamma_merge,\n",
    "    ),\n",
    "    linkage_kw=dict(progress=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_community_comparison(\n",
    "    data=dict(\n",
    "        true=sim1['pi'],\n",
    "        fit=sim1_fit1['pi'],\n",
    "#         init=sim1_pi_init,\n",
    "#         merg=sim1_fit1_pi_merge,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['epsilon'], sim1_fit1['epsilon'])\n",
    "plt.plot([0, 0.04], [0, 0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['alpha'], sim1_fit1['alpha'])\n",
    "plt.plot([0, 200], [0, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim1_fit1['delta'], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['mu'], sim1_fit1['mu'])\n",
    "plt.plot([0, 40], [0, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot comparing genotype accuracy to true strain abundance\n",
    "# colored by mean entropy of the estimated genotype masked by delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1_fit1['alpha'], sample_mean_masked_genotype_entropy(sim1_fit1['pi'], sim1_fit1['gamma'], sim1_fit1['delta']))\n",
    "sample_mean_masked_genotype_entropy(sim1_fit1['pi'], sim1_fit1['gamma'], sim1_fit1['delta']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hit, best_dist = match_genotypes(sim1_gamma_adjusted[:, :g_fit], sim1_fit1_gamma_adjusted[:, :g_fit])\n",
    "\n",
    "print('weighted_mean_distance:', (best_dist * sim1['pi'].mean(0)).sum())\n",
    "plt.scatter((sim1['pi'] * sim1['mu'].reshape(-1, 1)).sum(0), best_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_sim = 1 - pdist(sim1['pi'], metric='braycurtis')\n",
    "bc_fit = 1 - pdist(sim1_fit1['pi'], metric='braycurtis')\n",
    "plt.scatter(\n",
    "    bc_sim,\n",
    "    bc_fit,\n",
    "    marker='.',\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "community_accuracy_test(sim1['pi'], sim1_fit1['pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strains that are not representative of true haplotypes\n",
    "# are high entropy (even after masking with delta)\n",
    "# and have low estimated total coverage.\n",
    "\n",
    "best_true_strain, best_true_strain_dist = match_genotypes(sim1_fit1_gamma_adjusted[:, :g_fit], sim1_gamma_adjusted[:, :g_fit])\n",
    "best_true_strain_dist\n",
    "\n",
    "plt.scatter((sim1_fit1['pi'] * sim1_fit1['mu'].reshape((-1, 1))).sum(0), best_true_strain_dist, c=mean_masked_genotype_entropy(sim1_fit1['gamma'], sim1_fit1['delta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1_fit1_gamma_adjusted[:, :g_plt], linkage_kw=dict(progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_community(sim1_fit1['pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_masked_genotype_entropy(sim1_fit1['gamma'][:, :g_plt], sim1_fit1['delta'][:, :g_plt]))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1_fit1['alpha'], bins=20)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1_fit1['gamma'][mean_masked_genotype_entropy(sim1_fit1['gamma'], sim1_fit1['delta']) < 0.1, :g_fit])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "82px",
    "width": "168px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}