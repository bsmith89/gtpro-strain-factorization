{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sfacts.logging_util import *\n",
    "from sfacts.pyro_util import *\n",
    "from sfacts.model import *\n",
    "from sfacts.genotype import *\n",
    "from sfacts.plot import *\n",
    "from sfacts.estimation import *\n",
    "from sfacts.evaluation import *\n",
    "from sfacts.workflow import *\n",
    "from sfacts.data import *\n",
    "from sfacts.pandas_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sfacts as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from torch.jit import TracerWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = torch.tensor(1 - 1e-8, requires_grad=True)\n",
    "\n",
    "torch.allclose(\n",
    "    sf.model.stickbreaking_betas_to_probs(torch.ones(n) * p),\n",
    "    sf.model.stickbreaking_betas_to_probs2(torch.ones(n) * p),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    torch.autograd.grad(sf.model.stickbreaking_betas_to_probs(torch.ones(n) * p)[2], inputs=p)[0],\n",
    "    torch.autograd.grad(sf.model.stickbreaking_betas_to_probs2(torch.ones(n) * p)[2], inputs=p)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/__init__.py\n",
    "\n",
    "from sfacts import (logging_util, pyro_util, pandas_util, model, genotype, plot, estimation, evaluation, workflow, data, app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/logging_util.py\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "\n",
    "def info(*msg, quiet=False):\n",
    "    now = datetime.now()\n",
    "    if not quiet:\n",
    "        print(f'[{now}]', *msg, file=sys.stderr, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyro_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/pyro_util.py\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "\n",
    "def as_torch(x, dtype=None, device=None):\n",
    "    # Cast inputs and set device\n",
    "    return torch.tensor(x, dtype=dtype, device=device)\n",
    "\n",
    "def all_torch(dtype=None, device=None, **kwargs):\n",
    "    # Cast inputs and set device\n",
    "    return {k: as_torch(kwargs[k], dtype=dtype, device=device) for k in kwargs}\n",
    "\n",
    "def shape_info(model, *args, **kwargs):\n",
    "    _trace = pyro.poutine.trace(model).get_trace(*args, **kwargs)\n",
    "    _trace.compute_log_prob()\n",
    "    info(_trace.format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/pandas_util.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def idxwhere(condition):\n",
    "    return list(condition[condition].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model.py\n",
    "\n",
    "from sfacts.pyro_util import as_torch, all_torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "\n",
    "def stickbreaking_betas_to_probs(beta):\n",
    "    beta1m_cumprod = (1 - beta).cumprod(-1)\n",
    "    return torch_pad(beta, (0, 1), value=1) * torch_pad(beta1m_cumprod, (1, 0), value=1)\n",
    "\n",
    "\n",
    "def stickbreaking_betas_to_probs2(beta):\n",
    "    \"\"\"I thought this might be more stable, but it turns out the gradient is NOT more stable.\"\"\"\n",
    "    log_beta1m = torch.log(1 - beta)\n",
    "    log_beta1m_cumprod = log_beta1m.cumsum(-1)\n",
    "    log_beta_pad = torch_pad(torch.log(beta), (0, 1), value=0)\n",
    "    log_beta1m_cumprod_pad = torch_pad(log_beta1m_cumprod, (1, 0), value=0)\n",
    "#     beta1m_cumprod = (1 - beta).cumprod(-1)\n",
    "#     return torch_pad(beta, (0, 1), value=1) * torch_pad(beta1m_cumprod, (1, 0), value=1)\n",
    "    return torch.exp(log_beta_pad + log_beta1m_cumprod_pad)\n",
    "\n",
    "\n",
    "\n",
    "def NegativeBinomialReparam(mu, r, eps):\n",
    "    p = torch.clamp(1. / ((r / mu) + 1.), eps, 1 - eps)\n",
    "    return dist.NegativeBinomial(\n",
    "        total_count=r,\n",
    "        probs=p,\n",
    "    )\n",
    "\n",
    "def model(\n",
    "    n,\n",
    "    g,\n",
    "    s,\n",
    "    gamma_hyper=1.,\n",
    "    delta_hyper_temp=0.1,\n",
    "    delta_hyper_p=0.9,\n",
    "    rho_hyper=1.,\n",
    "    pi_hyper=1.,\n",
    "    alpha_hyper_hyper_mean=100.,\n",
    "    alpha_hyper_hyper_scale=1.,\n",
    "    alpha_hyper_scale=0.5,\n",
    "    epsilon_hyper_alpha=1.5,\n",
    "    epsilon_hyper_beta=1.5 / 0.01,\n",
    "    mu_hyper_mean=1.,\n",
    "    mu_hyper_scale=1.,\n",
    "#     m_hyper_r=1.,\n",
    "    m_eps=1e-5,\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "):\n",
    "    (\n",
    "        gamma_hyper,\n",
    "        delta_hyper_temp,\n",
    "        delta_hyper_p,\n",
    "        rho_hyper,\n",
    "        pi_hyper,\n",
    "        alpha_hyper_hyper_mean,\n",
    "        alpha_hyper_hyper_scale,\n",
    "        alpha_hyper_scale,\n",
    "        epsilon_hyper_alpha,\n",
    "        epsilon_hyper_beta,\n",
    "        mu_hyper_mean,\n",
    "        mu_hyper_scale,\n",
    "#         m_hyper_r,\n",
    "        m_eps,\n",
    "    ) = (\n",
    "        as_torch(x, dtype=dtype, device=device)\n",
    "        for x in [\n",
    "            gamma_hyper,\n",
    "            delta_hyper_temp,\n",
    "            delta_hyper_p,\n",
    "            rho_hyper,\n",
    "            pi_hyper,\n",
    "            alpha_hyper_hyper_mean,\n",
    "            alpha_hyper_hyper_scale,\n",
    "            alpha_hyper_scale,\n",
    "            epsilon_hyper_alpha,\n",
    "            epsilon_hyper_beta,\n",
    "            mu_hyper_mean,\n",
    "            mu_hyper_scale,\n",
    "#             m_hyper_r,\n",
    "            m_eps,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    _zero = as_torch(0, dtype=dtype, device=device).squeeze()\n",
    "    _one = as_torch(1, dtype=dtype, device=device).squeeze()\n",
    "\n",
    "    # Genotypes\n",
    "#     delta_hyper_p = pyro.sample('delta_hyper_p', dist.Beta(1., 1.))\n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.RelaxedBernoulli(temperature=gamma_hyper, logits=_zero)\n",
    "            )\n",
    "            # Position presence/absence\n",
    "            delta = pyro.sample(\n",
    "                'delta', dist.RelaxedBernoulli(temperature=delta_hyper_temp, probs=delta_hyper_p)\n",
    "            )\n",
    "#             delta = pyro.sample(\n",
    "#                 'delta', dist.Beta(delta_hyper_p * delta_hyper_temp, (1 - delta_hyper_p) * delta_hyper_temp)\n",
    "#             )\n",
    "    \n",
    "    # Meta-community composition\n",
    "    rho_betas = pyro.sample('rho_betas', dist.Beta(1, rho_hyper).expand([s - 1]).to_event())\n",
    "    rho = pyro.deterministic('rho', stickbreaking_betas_to_probs(rho_betas))\n",
    "#     rho = pyro.sample('rho', dist.Dirichlet(rho_hyper * torch.ones(s, dtype=dtype, device=device)))\n",
    "\n",
    "    alpha_hyper_mean = pyro.sample('alpha_hyper_mean', dist.LogNormal(loc=torch.log(alpha_hyper_hyper_mean), scale=alpha_hyper_hyper_scale))\n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        # Community composition\n",
    "        pi = pyro.sample('pi', dist.Dirichlet(pi_hyper * rho, validate_args=False))\n",
    "        # Sequencing error\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta)).unsqueeze(-1)\n",
    "        alpha = pyro.sample('alpha', dist.LogNormal(loc=torch.log(alpha_hyper_mean), scale=alpha_hyper_scale)).unsqueeze(-1)\n",
    "        # Sample coverage\n",
    "        mu = pyro.sample('mu', dist.LogNormal(loc=torch.log(mu_hyper_mean), scale=mu_hyper_scale))\n",
    "        \n",
    "    # Depth at each position\n",
    "    nu = pyro.deterministic(\"nu\", pi @ delta)\n",
    "    m_hyper_r = pyro.sample('m_hyper_r', dist.LogNormal(loc=_one, scale=_one))\n",
    "    m = pyro.sample('m', NegativeBinomialReparam(nu * mu.reshape((-1,1)), m_hyper_r, eps=m_eps).to_event())\n",
    "\n",
    "    # Expected fractions of each allele at each position\n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ (gamma * delta) / nu)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    \n",
    "    # Observation\n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m,\n",
    "        ).to_event(),\n",
    "    )\n",
    "    \n",
    "def condition_model(model, data=None, device='cpu', dtype=torch.float32, **model_kwargs):\n",
    "    if data is None:\n",
    "        data = {}\n",
    "        \n",
    "    conditioned_model = partial(\n",
    "        pyro.condition(\n",
    "            model,\n",
    "            data=all_torch(**data, dtype=dtype, device=device),\n",
    "        ),\n",
    "        **model_kwargs,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    return conditioned_model\n",
    "    \n",
    "def simulate(model):\n",
    "    obs = pyro.infer.Predictive(model, num_samples=1)()\n",
    "    obs = {\n",
    "        k: obs[k].detach().cpu().numpy().squeeze()\n",
    "        for k in obs.keys()\n",
    "    }\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/genotype.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import scipy as sp\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prob_to_sign(gamma):\n",
    "    return gamma * 2 - 1\n",
    "\n",
    "def genotype_distance(x, y):\n",
    "    x = prob_to_sign(x)\n",
    "    y = prob_to_sign(y)\n",
    "    dist = ((x - y) / 2) ** 2\n",
    "    weight = (x * y) ** 2\n",
    "    wmean_dist = ((weight * dist).mean()) / ((weight.mean()))\n",
    "    return wmean_dist\n",
    "\n",
    "def sign_genotype_distance(x, y):\n",
    "    dist = ((x - y) / 2) ** 2\n",
    "    weight = (x * y) ** 2\n",
    "    wmean_dist = ((weight * dist).mean()) / ((weight.mean()))\n",
    "    return wmean_dist\n",
    "\n",
    "def genotype_pdist(gamma, progress=False):\n",
    "    metric = sign_genotype_distance\n",
    "    X = np.asarray(gamma * 2 - 1)\n",
    "    m = X.shape[0]\n",
    "    dm = np.empty((m * (m - 1)) // 2)\n",
    "    k = 0\n",
    "    with tqdm(total=len(dm), disable=(not progress)) as pbar:\n",
    "        for i in range(0, m - 1):\n",
    "            for j in range(i + 1, m):\n",
    "                dm[k] = metric(X[i], X[j])\n",
    "                k = k + 1\n",
    "                pbar.update()\n",
    "    return dm\n",
    "    \n",
    "\n",
    "def counts_to_p_estimate(y, m, pseudo=1):\n",
    "    return (y + pseudo) / (m + pseudo * 2)\n",
    "\n",
    "def genotype_linkage(gamma, progress=False, **kwargs):\n",
    "    dmat = genotype_pdist(gamma, progress=progress)\n",
    "    kw = dict(method='complete')\n",
    "    kw.update(kwargs)\n",
    "    return linkage(dmat, **kw), dmat\n",
    "\n",
    "def mask_missing_genotype(gamma, delta):\n",
    "    return sp.special.expit(sp.special.logit(gamma) * delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/plot.py\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sfacts.genotype import genotype_linkage, prob_to_sign\n",
    "from scipy.spatial.distance import squareform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_clustermap_dims(nx, ny, scalex=0.15, scaley=0.02, dwidth=0.2, dheight=1.0):\n",
    "    mwidth = nx * scalex\n",
    "    mheight = ny * scaley\n",
    "    fwidth = mwidth + dwidth\n",
    "    fheight = mheight + dheight\n",
    "    dendrogram_ratio = (dwidth / fwidth, dheight / fheight)\n",
    "    return fwidth, fheight, dendrogram_ratio\n",
    "    \n",
    "\n",
    "def plot_genotype(gamma, linkage_kw=None, scalex=0.15, scaley=0.02, dwidth=0.2, dheight=1.0, **kwargs):\n",
    "    if linkage_kw is None:\n",
    "        linkage_kw = {}\n",
    "    linkage, _ = genotype_linkage(gamma, **linkage_kw)\n",
    "    \n",
    "    gamma_t = gamma.T\n",
    "    ny, nx = gamma_t.shape\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=scalex, scaley=scaley, dwidth=dwidth, dheight=dheight,\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        cmap='coolwarm',\n",
    "        dendrogram_ratio=dendrogram_ratio,\n",
    "        col_linkage=linkage,\n",
    "        figsize=(fwidth, fheight),\n",
    "        xticklabels=1,\n",
    "        yticklabels=0,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    grid = sns.clustermap(prob_to_sign(gamma_t), **kw)\n",
    "    grid.cax.set_visible(False)\n",
    "    return grid\n",
    "    \n",
    "def plot_missing(delta, scalex=0.15, scaley=0.02, dwidth=0.2, dheight=1.0, **kwargs):\n",
    "    delta_t = delta.T\n",
    "    ny, nx = delta_t.shape\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=scalex, scaley=scaley, dwidth=dwidth, dheight=dheight,\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        vmin=0, vmax=1, dendrogram_ratio=dendrogram_ratio, figsize=(fwidth, fheight), xticklabels=1, yticklabels=0,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    grid = sns.clustermap(delta_t, **kw)\n",
    "    grid.cax.set_visible(False)\n",
    "    return grid\n",
    "    \n",
    "def plot_community(pi, scalex=0.2, scaley=0.1, dwidth=0.2, dheight=1.0, **kwargs):\n",
    "    ny, nx = pi.shape\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=scalex, scaley=scaley, dwidth=dwidth, dheight=dheight,\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        metric='cosine', vmin=0, vmax=1, dendrogram_ratio=dendrogram_ratio, figsize=(fwidth, fheight), xticklabels=1,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    grid = sns.clustermap(pi, **kw)\n",
    "    grid.cax.set_visible(False)\n",
    "    return grid\n",
    "    \n",
    "def plot_genotype_similarity(gamma, linkage_kw=None, **kwargs):\n",
    "    if linkage_kw is None:\n",
    "        linkage_kw = {}\n",
    "    linkage, dmat = genotype_linkage(gamma, **linkage_kw)\n",
    "    dmat = squareform(dmat)\n",
    "    \n",
    "    nx = ny = gamma.shape[0]\n",
    "    fwidth, fheight, dendrogram_ratio = calculate_clustermap_dims(\n",
    "        nx, ny, scalex=0.15, scaley=0.15, dwidth=0.5, dheight=0.5\n",
    "    )\n",
    "    \n",
    "    kw = dict(\n",
    "        vmin=0, vmax=1, dendrogram_ratio=dendrogram_ratio, row_linkage=linkage, col_linkage=linkage, figsize=(fwidth, fheight), xticklabels=1, yticklabels=1,\n",
    "    )\n",
    "    kw.update(kwargs)\n",
    "    grid = sns.clustermap(1 - dmat, **kw)\n",
    "    grid.cax.set_visible(False)\n",
    "    return grid\n",
    "    \n",
    "def plot_genotype_comparison(data=None, **kwargs):\n",
    "    stacked = pd.concat([\n",
    "        pd.DataFrame(data[k], index=[f'{k}_{i}' for i in range(data[k].shape[0])])\n",
    "        for k in data\n",
    "    ])\n",
    "    kw = dict(xticklabels=1)\n",
    "    kw.update(kwargs)\n",
    "    return plot_genotype(stacked, **kw)\n",
    "\n",
    "def plot_community_comparison(data=None, **kwargs):\n",
    "    stacked = pd.concat([\n",
    "        pd.DataFrame(data[k], columns=[f'{k}_{i}' for i in range(data[k].shape[1])])\n",
    "        for k in data\n",
    "    ], axis=1)\n",
    "    kw = dict(xticklabels=1)\n",
    "    kw.update(kwargs)\n",
    "    return plot_community(stacked, **kw)\n",
    "\n",
    "def plot_loss_history(trace):\n",
    "    trace = np.array(trace)\n",
    "    plt.plot((trace - trace.min()))\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/estimation.py\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import non_negative_factorization\n",
    "from sfacts.genotype import genotype_pdist, mask_missing_genotype\n",
    "from sfacts.pyro_util import all_torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "from sfacts.model import condition_model\n",
    "\n",
    "def cluster_genotypes(\n",
    "    gamma, thresh, progress=False, precomputed_pdist=None\n",
    "):\n",
    "    \n",
    "    if precomputed_pdist is None:\n",
    "        compressed_dmat = genotype_pdist(gamma, progress=progress)\n",
    "    else:\n",
    "        compressed_dmat = precomputed_pdist\n",
    "\n",
    "    clust = pd.Series(\n",
    "        AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            affinity=\"precomputed\",\n",
    "            linkage=\"complete\",\n",
    "            distance_threshold=thresh,\n",
    "        )\n",
    "        .fit(squareform(compressed_dmat))\n",
    "        .labels_\n",
    "    )\n",
    "\n",
    "    return clust, compressed_dmat\n",
    "\n",
    "def initialize_parameters_by_clustering_samples(\n",
    "    y, m, thresh, additional_strains_factor=0.5, progress=False, precomputed_pdist=None,\n",
    "):\n",
    "    n, g = y.shape\n",
    "\n",
    "    sample_genotype = (y + 1) / (m + 2)\n",
    "    clust, cdmat = cluster_genotypes(sample_genotype, thresh=thresh, progress=progress, precomputed_pdist=precomputed_pdist)\n",
    "\n",
    "    y_total = (\n",
    "        pd.DataFrame(pd.DataFrame(y))\n",
    "        .groupby(clust)\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    m_total = (\n",
    "        pd.DataFrame(pd.DataFrame(m))\n",
    "        .groupby(clust)\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    clust_genotype = (y_total + 1) / (m_total + 2)\n",
    "    additional_haplotypes = int(\n",
    "        additional_strains_factor * clust_genotype.shape[0]\n",
    "    )\n",
    "\n",
    "\n",
    "    gamma_init = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(clust_genotype),\n",
    "            pd.DataFrame(np.ones((additional_haplotypes, g)) * 0.5),\n",
    "        ]\n",
    "    ).values\n",
    "\n",
    "    s_init = gamma_init.shape[0]\n",
    "    pi_init = np.ones((n, s_init))\n",
    "    for i in range(n):\n",
    "        pi_init[i, clust[i]] = s_init - 1\n",
    "    pi_init /= pi_init.sum(1, keepdims=True)\n",
    "\n",
    "    assert (~np.isnan(gamma_init)).all()\n",
    "\n",
    "    return gamma_init, pi_init, cdmat\n",
    "\n",
    "def initialize_parameters_by_nmf(\n",
    "    y, m, s, progress=False, solver='mu', alpha=100., l1_ratio=1.0, tol=1e-2, **kwargs\n",
    "):    \n",
    "    n, g = y.shape\n",
    "\n",
    "    # Fit to counts of both reference and alternative alleles by stacking them.\n",
    "    stacked_metagenotype = np.concatenate([y, m - y], axis=1)\n",
    "    pi_unnorm, gamma_unnorm, _ = non_negative_factorization(\n",
    "        stacked_metagenotype,\n",
    "        n_components=s,\n",
    "        solver=solver,\n",
    "        verbose=int(progress),\n",
    "        alpha=alpha,\n",
    "        l1_ratio=l1_ratio,\n",
    "        tol=tol,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # TODO: Find a more principled way to convert pi_unnorm into pi_init.\n",
    "    pi_init = (pi_unnorm + 1) / (pi_unnorm + 1).sum(1, keepdims=True)\n",
    "    gamma_init = ((gamma_unnorm[:, :g] + 1) / (gamma_unnorm[:, :g] + gamma_unnorm[:, -g:] + 2))\n",
    "\n",
    "    return gamma_init, pi_init, None\n",
    "\n",
    "\n",
    "def estimate_parameters(\n",
    "    model,\n",
    "    data,\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "    initialize_params=None,\n",
    "    maxiter=10000,\n",
    "    lagA=20,\n",
    "    lagB=100,\n",
    "    opt=pyro.optim.Adamax({\"lr\": 1e-2}, {\"clip_norm\": 100}),\n",
    "    progress=True,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    conditioned_model = condition_model(\n",
    "        model,\n",
    "        data=data,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    if initialize_params is None:\n",
    "        initialize_params = {}\n",
    "\n",
    "    _guide = pyro.infer.autoguide.AutoLaplaceApproximation(\n",
    "        conditioned_model,\n",
    "        init_loc_fn=pyro.infer.autoguide.initialization.init_to_value(\n",
    "            values=all_torch(**initialize_params, dtype=dtype, device=device)\n",
    "        ),\n",
    "    )\n",
    "    svi = pyro.infer.SVI(\n",
    "        conditioned_model,\n",
    "        _guide,\n",
    "        opt,\n",
    "        loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    history = []\n",
    "    pbar = tqdm(range(maxiter), disable=(not progress))\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                pbar.close()\n",
    "                raise RuntimeError(\"ELBO NaN?\")\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if (i % 10 == 0):\n",
    "                if i > lagB:\n",
    "                    delta = history[-2] - history[-1]\n",
    "                    delta_lagA = (history[-lagA] - history[-1]) / lagA\n",
    "                    delta_lagB = (history[-lagB] - history[-1]) / lagB\n",
    "                    if (delta_lagA <= 0) and (delta_lagB <= 0):\n",
    "                        if progress:\n",
    "                            pbar.close()\n",
    "                            info(\"Converged\")\n",
    "                        break\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta': delta,\n",
    "                        f'lag{lagA}': delta_lagA,\n",
    "                        f'lag{lagB}': delta_lagB,\n",
    "                    })\n",
    "    except KeyboardInterrupt:\n",
    "        pbar.close()\n",
    "        info(\"Interrupted\")\n",
    "        pass\n",
    "    est = pyro.infer.Predictive(conditioned_model, guide=_guide, num_samples=1)()\n",
    "    est = {\n",
    "        k: est[k].detach().cpu().numpy().mean(0).squeeze()\n",
    "        for k in est.keys()\n",
    "    }\n",
    "    \n",
    "    if device.startswith(\"cuda\"):\n",
    "#         info(\n",
    "#             \"CUDA available mem: {}\".format(\n",
    "#                 torch.cuda.get_device_properties(0).total_memory\n",
    "#             ),\n",
    "#         )\n",
    "#         info(\"CUDA reserved mem: {}\".format(torch.cuda.memory_reserved(0)))\n",
    "#         info(\"CUDA allocated mem: {}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         info(\n",
    "#             \"CUDA free mem: {}\".format(\n",
    "#                 torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)\n",
    "#             )\n",
    "#         )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return est, history\n",
    "\n",
    "\n",
    "def merge_similar_genotypes(\n",
    "    gamma, pi, thresh, delta=None, progress=False,\n",
    "):\n",
    "    if delta is None:\n",
    "        delta = np.ones_like(gamma)\n",
    "        \n",
    "    gamma_adjust = mask_missing_genotype(gamma, delta)\n",
    "\n",
    "    clust, dmat = cluster_genotypes(gamma_adjust, thresh=thresh, progress=progress)\n",
    "    gamma_mean = (\n",
    "        pd.DataFrame(pd.DataFrame(gamma_adjust))\n",
    "        .groupby(clust)\n",
    "        .apply(lambda x: sp.special.expit(sp.special.logit(x)).mean(0))\n",
    "        .values\n",
    "    )\n",
    "    delta_mean = (\n",
    "        pd.DataFrame(pd.DataFrame(delta))\n",
    "        .groupby(clust)\n",
    "        .mean()\n",
    "        .values\n",
    "    )\n",
    "    pi_sum = (\n",
    "        pd.DataFrame(pd.DataFrame(pi))\n",
    "        .groupby(clust, axis='columns')\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    \n",
    "    return gamma_mean, pi_sum, delta_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/evaluation.py\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    q = 1 - p\n",
    "    ent = -(p * np.log2(p) + q * np.log2(q))\n",
    "    return ent\n",
    "\n",
    "def sum_binary_entropy(p, normalize=False, axis=None):\n",
    "    q = 1 - p\n",
    "    ent = np.sum(-(p * np.log2(p) + q * np.log2(q)), axis=axis)\n",
    "    if normalize:\n",
    "        ent = ent / p.shape[axis]\n",
    "    return ent\n",
    "\n",
    "def mean_masked_genotype_entropy(gamma, delta):\n",
    "    return (binary_entropy(gamma) * delta).mean(1)\n",
    "\n",
    "def sample_mean_masked_genotype_entropy(pi, gamma, delta):\n",
    "    return (pi @ mean_masked_genotype_entropy(gamma, delta).reshape((-1, 1))).squeeze()\n",
    "\n",
    "def match_genotypes(gammaA, gammaB):\n",
    "    g = gammaA.shape[1]\n",
    "    dist = pd.DataFrame(cdist(gammaA, gammaB, metric='cityblock'))\n",
    "    return dist.idxmin(axis=1), dist.min(axis=1) / g\n",
    "\n",
    "def _rmse(x, y):\n",
    "    return np.sqrt(np.square(x - y).mean())\n",
    "\n",
    "def _rss(x, y):\n",
    "    return np.sqrt(np.square(x - y).sum())\n",
    "\n",
    "def community_accuracy_test(pi_sim, pi_fit, reps=99):\n",
    "    bc_sim = 1 - pdist(pi_sim, metric='braycurtis')\n",
    "    bc_fit = 1 - pdist(pi_fit, metric='braycurtis')\n",
    "    err = _rmse(bc_sim, bc_fit)\n",
    "    \n",
    "    null = []\n",
    "    n = len(bc_sim)\n",
    "    for i in range(reps):\n",
    "        bc_sim_permute = np.random.permutation(bc_sim)\n",
    "        null.append(_rmse(bc_sim, bc_sim_permute))\n",
    "    null = np.array(null)\n",
    "    \n",
    "    return err, null, err / np.mean(null), (np.sort(null) < err).mean()\n",
    "\n",
    "def metacommunity_composition_rss(pi_sim, pi_fit):\n",
    "    mean_sim = pi_sim.mean(0)\n",
    "    mean_fit = pi_fit.mean(0)\n",
    "    s_sim = mean_sim.shape[0]\n",
    "    s_fit = mean_fit.shape[0]\n",
    "    s = max(s_sim, s_fit)\n",
    "    mean_sim = np.sort(np.pad(mean_sim, pad_width=(0, s - s_sim)))\n",
    "    mean_fit = np.sort(np.pad(mean_fit, pad_width=(0, s - s_fit)))\n",
    "    return _rss(mean_sim, mean_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/data.py\n",
    "\n",
    "from sfacts.logging_util import info\n",
    "from sfacts.pandas_util import idxwhere\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "class Metagenotype():\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        data = xr.Dataset(*args, **kwargs)\n",
    "\n",
    "def load_input_data(allpaths):\n",
    "    data = []\n",
    "    for path in allpaths:\n",
    "        info(path)\n",
    "        d = xr.open_dataarray(path).squeeze()\n",
    "        info(f\"Shape: {d.sizes}.\")\n",
    "        data.append(d)\n",
    "    info(\"Concatenating data from {} files.\".format(len(data)))\n",
    "    data = xr.concat(data, \"library_id\", fill_value=0)\n",
    "    info(f\"Finished concatenating data: {data.sizes}\")\n",
    "    return data\n",
    "\n",
    "def select_informative_positions(data, incid_thresh):\n",
    "    minor_allele_incid = (data > 0).mean(\"library_id\").min(\"allele\")\n",
    "    informative_positions = idxwhere(\n",
    "        minor_allele_incid.to_series() > incid_thresh\n",
    "    )\n",
    "    return informative_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/workflow.py\n",
    "\n",
    "import pyro\n",
    "from sfacts.pandas_util import idxwhere\n",
    "from sfacts.model import model, simulate, condition_model\n",
    "from sfacts.estimation import (\n",
    "    initialize_parameters_by_clustering_samples,\n",
    "    initialize_parameters_by_nmf,\n",
    "    estimate_parameters,\n",
    "    merge_similar_genotypes\n",
    ")\n",
    "from sfacts.genotype import mask_missing_genotype\n",
    "from sfacts.evaluation import (\n",
    "    match_genotypes,\n",
    "    sample_mean_masked_genotype_entropy,\n",
    "    community_accuracy_test,\n",
    "    metacommunity_composition_rss,\n",
    ")\n",
    "from sfacts.data import load_input_data, select_informative_positions\n",
    "import time\n",
    "import numpy as np\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "def fit_to_data(\n",
    "    y,\n",
    "    m,\n",
    "    fit_kwargs,\n",
    "    initialize='nmf',\n",
    "    initialize_kwargs=None,\n",
    "    postclust=True,\n",
    "    postclust_kwargs=None,\n",
    "    seed=1,\n",
    "    quiet=False,\n",
    "    additional_conditioning_data=None\n",
    "):\n",
    "    if additional_conditioning_data is None:\n",
    "        additional_conditioning_data = {}\n",
    "\n",
    "    n, g = y.shape\n",
    "    info(f\"Setting RNG seed to {seed}.\", quiet=quiet)\n",
    "    pyro.util.set_rng_seed(seed)\n",
    "    if initialize == 'nmf':\n",
    "        info(f\"Initializing {n} samples and {g} positions using NMF.\", quiet=quiet)\n",
    "        assert initialize_kwargs is not None\n",
    "        gamma_init, pi_init, _ = initialize_parameters_by_nmf(\n",
    "            y,\n",
    "            m,\n",
    "            random_state=seed,\n",
    "            **initialize_kwargs\n",
    "        )\n",
    "        initialize_params=dict(gamma=gamma_init, pi=pi_init)\n",
    "        s_fit = gamma_init.shape[0]\n",
    "        info(f\"Initialized {s_fit} strains in {n} samples.\", quiet=quiet)\n",
    "    elif initialize == 'clust':\n",
    "        info(f\"Initializing {n} samples and {g} positions using hierarchical clustering.\", quiet=quiet)\n",
    "        assert initialize_kwargs is not None\n",
    "        gamma_init, pi_init, _ = initialize_parameters_by_clustering_samples(\n",
    "            y,\n",
    "            m,\n",
    "            **initialize_kwargs\n",
    "        )\n",
    "        initialize_params=dict(gamma=gamma_init, pi=pi_init)\n",
    "        s_fit = gamma_init.shape[0]\n",
    "        info(f\"Initialized {s_fit} strains in {n} samples.\", quiet=quiet)\n",
    "    elif not initialize:\n",
    "        initialize_params = None\n",
    "        s_fit = fit_kwargs.pop('s')\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Initializing strategy: '{initialize}' not known.\")\n",
    "\n",
    "    info(f\"Optimizing model parameters.\", quiet=quiet)\n",
    "    info(f\"Setting RNG seed to {seed}.\", quiet=quiet)\n",
    "    pyro.util.set_rng_seed(seed)\n",
    "    fit, history = estimate_parameters(\n",
    "        model,\n",
    "        data=dict(y=y, m=m, **additional_conditioning_data),\n",
    "        n=n,\n",
    "        g=g,\n",
    "        s=s_fit,\n",
    "        initialize_params=initialize_params,\n",
    "        **fit_kwargs,\n",
    "    )\n",
    "    \n",
    "    if postclust:\n",
    "        info(f\"Dereplicating highly similar strains.\", quiet=quiet)\n",
    "        merge_gamma, merge_pi, merge_delta = merge_similar_genotypes(\n",
    "            fit['gamma'],\n",
    "            fit['pi'],\n",
    "            delta=fit['delta'],\n",
    "            **postclust_kwargs,\n",
    "        )\n",
    "        mrg = fit.copy()\n",
    "        mrg['gamma'] = merge_gamma\n",
    "        mrg['pi'] = merge_pi\n",
    "        mrg['delta'] = merge_delta\n",
    "        s_mrg = mrg['gamma'].shape[0]\n",
    "        info(f\"Original {s_fit} strains down to {s_mrg} after dereplication.\", quiet=quiet)\n",
    "    else:\n",
    "        mrg = fit\n",
    "    info(f\"Finished fitting to data.\", quiet=quiet)\n",
    "        \n",
    "    return mrg, fit, history\n",
    "\n",
    "def simulate_fit_and_evaluate(\n",
    "    s_sim,\n",
    "    n_sim,\n",
    "    g_sim,\n",
    "    n_fit,\n",
    "    g_fit,\n",
    "    sim_kwargs,\n",
    "    fit_kwargs,\n",
    "    seed_sim=1,\n",
    "    seed_fit=1,\n",
    "    preclust=True,\n",
    "    preclust_kwargs=None,\n",
    "    postclust=True,\n",
    "    postclust_kwargs=None,\n",
    "    quiet=False,\n",
    "):\n",
    "    info(f\"Setting RNG seed to {seed_sim}.\", quiet=quiet)\n",
    "    pyro.util.set_rng_seed(seed_sim)\n",
    "    info(f\"Simulating data from model.\", quiet=quiet)\n",
    "    sim = simulate(\n",
    "        condition_model(\n",
    "            model,\n",
    "            n=n_sim,\n",
    "            g=g_sim,\n",
    "            s=s_sim,\n",
    "            **sim_kwargs,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    info(f\"Starting runtime clock.\", quiet=quiet)\n",
    "    start_time = time.time()\n",
    "    mrg, fit, history = fit_to_data(\n",
    "        sim['y'][:n_fit, :g_fit],\n",
    "        sim['m'][:n_fit, :g_fit],\n",
    "        fit_kwargs=fit_kwargs,\n",
    "        preclust=preclust,\n",
    "        preclust_kwargs=preclust_kwargs,\n",
    "        postclust=postclust,\n",
    "        postclust_kwargs=postclust_kwargs,\n",
    "        seed=seed_fit,\n",
    "        quiet=quiet,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    info(f\"Stopping runtime clock.\", quiet=quiet)\n",
    "    \n",
    "    info(f\"Calculating statistics.\", quiet=quiet)\n",
    "    s_mrg = mrg['gamma'].shape[0]\n",
    "    \n",
    "    sim_gamma_adj = mask_missing_genotype(sim['gamma'][:, :g_fit], sim['delta'][:, :g_fit])\n",
    "    mrg_gamma_adj = mask_missing_genotype(mrg['gamma'], mrg['delta'])\n",
    "    best_hit, best_dist = match_genotypes(sim_gamma_adj, mrg_gamma_adj)\n",
    "    weighted_mean_genotype_error = (best_dist * sim['pi'][:n_fit].mean(0)).sum()\n",
    "    runtime = end_time - start_time\n",
    "    \n",
    "    _, _, beta_diversity_error_ratio, _ = (\n",
    "        community_accuracy_test(sim['pi'][:n_fit], mrg['pi'])\n",
    "    )\n",
    "    \n",
    "    metacommunity_composition_error = metacommunity_composition_rss(sim['pi'], mrg['pi'])\n",
    "    \n",
    "    mean_sample_weighted_genotype_entropy = (\n",
    "        sample_mean_masked_genotype_entropy(mrg['pi'], mrg['gamma'], mrg['delta']).mean()\n",
    "    )\n",
    "    info(f\"Finished calculating statistics.\", quiet=quiet)\n",
    "    \n",
    "    return (\n",
    "        weighted_mean_genotype_error,\n",
    "        beta_diversity_error_ratio,\n",
    "        metacommunity_composition_error,\n",
    "        mean_sample_weighted_genotype_entropy,\n",
    "        runtime,\n",
    "        sim,\n",
    "        mrg\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_data(\n",
    "    data, \n",
    "    incid_thresh=0.1,\n",
    "    cvrg_thresh=0.15,\n",
    "):\n",
    "    info(\"Filtering positions.\")\n",
    "    informative_positions = select_informative_positions(\n",
    "        data, incid_thresh\n",
    "    )\n",
    "    npos_available = len(informative_positions)\n",
    "    info(\n",
    "        f\"Found {npos_available} informative positions with minor \"\n",
    "        f\"allele incidence of >{incid_thresh}\"\n",
    "    )\n",
    "\n",
    "    info(\"Filtering libraries.\")\n",
    "    suff_cvrg_samples = idxwhere(\n",
    "        (\n",
    "            (\n",
    "                data.sel(position=informative_positions).sum([\"allele\"]) > 0\n",
    "            ).mean(\"position\")\n",
    "            > cvrg_thresh\n",
    "        ).to_series()\n",
    "    )\n",
    "    nlibs = len(suff_cvrg_samples)\n",
    "    info(\n",
    "        f\"Found {nlibs} libraries with >{cvrg_thresh:0.1%} \"\n",
    "        f\"of informative positions covered.\"\n",
    "    )\n",
    "    return informative_positions, suff_cvrg_samples\n",
    "\n",
    "def sample_positions(\n",
    "    informative_positions,\n",
    "    npos=1000,\n",
    "    seed=None,\n",
    "):\n",
    "    if seed is not None:\n",
    "        info(f\"Setting RNG seed to {seed}.\")\n",
    "        np.random.seed(seed)\n",
    "    npos_available = len(informative_positions)\n",
    "    _npos = min(npos, npos_available)\n",
    "    info(f\"Randomly sampling {npos} positions.\")\n",
    "    position_ss = np.random.choice(\n",
    "        informative_positions,\n",
    "        size=_npos,\n",
    "        replace=False,\n",
    "    )\n",
    "    info(f\"Finished sampling.\")\n",
    "    return position_ss  \n",
    "\n",
    "def filter_subsample_and_fit(\n",
    "    data,\n",
    "    incid_thresh=0.1,\n",
    "    cvrg_thresh=0.15,\n",
    "    npos=1000,\n",
    "    seed=1,\n",
    "    **fit_to_data_kwargs\n",
    "):\n",
    "    info(f\"Full data shape: {data.sizes}.\")\n",
    "    informative_positions, suff_cvrg_samples = filter_data(\n",
    "        data, incid_thresh=incid_thresh, cvrg_thresh=cvrg_thresh\n",
    "    )\n",
    "    position_ss = sample_positions(informative_positions, npos, seed=seed)\n",
    "    info(\"Constructing input data.\")\n",
    "    data_fit = data.sel(library_id=suff_cvrg_samples, position=position_ss)\n",
    "    m_ss = data_fit.sum(\"allele\")\n",
    "    n, g_ss = m_ss.shape\n",
    "    y_obs_ss = data_fit.sel(allele=\"alt\")  \n",
    "    mrg_ss, fit_ss, history = fit_to_data(\n",
    "        y_obs_ss.values,\n",
    "        m_ss.values,\n",
    "        seed=seed,\n",
    "        **fit_to_data_kwargs,\n",
    "    )\n",
    "    return mrg_ss, data_fit, history\n",
    "    \n",
    "\n",
    "def filter_subsample_fit_and_refit_genotypes(\n",
    "    data,\n",
    "    fit_kwargs,\n",
    "    incid_thresh=0.1,\n",
    "    cvrg_thresh=0.15,\n",
    "    npos=1000,\n",
    "    seed=1,\n",
    "    **kwargs\n",
    "):\n",
    "    info(f\"Full data shape: {data.sizes}.\")\n",
    "    informative_positions, suff_cvrg_samples = filter_data(\n",
    "        data, incid_thresh=incid_thresh, cvrg_thresh=cvrg_thresh\n",
    "    )\n",
    "    position_ss = sample_positions(informative_positions, npos, seed=seed)\n",
    "    info(\"Constructing input data.\")\n",
    "    data_filt = data.sel(library_id=suff_cvrg_samples)\n",
    "    data_ss = data_filt.sel(position=position_ss)\n",
    "    m_ss = data_ss.sum(\"allele\")\n",
    "    n, g_ss = m_ss.shape\n",
    "    y_obs_ss = data_ss.sel(allele=\"alt\")\n",
    "    mrg_ss, fit_ss, history = fit_to_data(\n",
    "        y_obs_ss.values,\n",
    "        m_ss.values,\n",
    "        seed=seed,\n",
    "        fit_kwargs=fit_kwargs,\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    info(f\"Refitting genotypes at all positions\")\n",
    "    s = mrg_ss['gamma'].shape[0]\n",
    "    refit_kwargs = fit_kwargs.copy()\n",
    "    if s in refit_kwargs:\n",
    "        del refit_kwargs['s']\n",
    "    n = len(suff_cvrg_samples)\n",
    "    g_total = len(informative_positions)\n",
    "    fixed = mrg_ss.copy()\n",
    "    for k in ['gamma', 'delta', 'nu', 'm', 'p_noerr', 'p', 'y', 'rho']:\n",
    "        del fixed[k]\n",
    "        \n",
    "    y = data_filt.sel(allele=\"alt\").values\n",
    "    m = data_filt.sum(\"allele\").values\n",
    "    out = fixed.copy()\n",
    "    gamma_out = []\n",
    "    delta_out = []\n",
    "    nu_out = []\n",
    "    m_out = []\n",
    "    p_noerr_out = []\n",
    "    p_out = []\n",
    "    y_out = []\n",
    "    nwindows = g_total // npos + 1\n",
    "    for window_i, j_start in enumerate(range(0, g_total, npos)):\n",
    "        window_ip1 = window_i + 1\n",
    "        info(f\"Fitting genotype window {window_ip1} of {nwindows}.\")\n",
    "        j_stop = min(j_start + g_ss, g_total)\n",
    "        refit, history = estimate_parameters(\n",
    "            model,\n",
    "            data=dict(y=y[:, j_start:j_stop], m=m[:, j_start:j_stop], **fixed),\n",
    "            n=n,\n",
    "            g=j_stop - j_start,\n",
    "            s=s,\n",
    "            **refit_kwargs,\n",
    "        )\n",
    "        gamma_out.append(refit['gamma'])\n",
    "        delta_out.append(refit['delta'])\n",
    "        nu_out.append(refit['nu'])\n",
    "        m_out.append(refit['m'])\n",
    "        p_noerr_out.append(refit['p_noerr'])\n",
    "        p_out.append(refit['p'])\n",
    "        y_out.append(refit['y'])\n",
    "        info(f\"Finished fitting genotype window {window_ip1} of {nwindows}.\")\n",
    "    info(f\"Finished all windows.\")\n",
    "        \n",
    "    out['gamma'] = np.concatenate(gamma_out, axis=1)\n",
    "    out['delta'] = np.concatenate(delta_out, axis=1)\n",
    "    out['nu'] = np.concatenate(nu_out, axis=1)\n",
    "    out['m'] = np.concatenate(m_out, axis=1)\n",
    "    out['p_noerr'] = np.concatenate(p_noerr_out, axis=1)\n",
    "    out['p'] = np.concatenate(p_out, axis=1)\n",
    "    out['y'] = np.concatenate(y_out, axis=1)\n",
    "    info(f\"Finished constructing arrays.\")\n",
    "\n",
    "    return out, data_filt, informative_positions, position_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/app.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import sfacts\n",
    "\n",
    "\n",
    "def parse_args(argv):\n",
    "    p = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    # Input\n",
    "    p.add_argument(\n",
    "        \"pileup\",\n",
    "        nargs=\"+\",\n",
    "        help=\"\"\"\n",
    "Single, fully processed, pileup table in NetCDF format\n",
    "with the following dimensions:\n",
    "    * library_id\n",
    "    * position\n",
    "    * allele\n",
    "                        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Shape of the model\n",
    "    p.add_argument(\"--nstrains\", metavar=\"INT\", type=int, default=1000)\n",
    "    p.add_argument(\n",
    "        \"--npos\",\n",
    "        metavar=\"INT\",\n",
    "        default=2000,\n",
    "        type=int,\n",
    "        help=(\"Number of positions to sample for model fitting.\"),\n",
    "    )\n",
    "\n",
    "    # Data filtering\n",
    "    p.add_argument(\n",
    "        \"--incid-thresh\",\n",
    "        metavar=\"FLOAT\",\n",
    "        type=float,\n",
    "        default=0.02,\n",
    "        help=(\n",
    "            \"Minimum fraction of samples that must have the minor allele \"\n",
    "            \"for the position to be considered 'informative'.\"\n",
    "        ),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--cvrg-thresh\",\n",
    "        metavar=\"FLOAT\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=(\n",
    "            \"Minimum fraction of 'informative' positions with counts \"\n",
    "            \"necessary for sample to be included.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Regularization\n",
    "    p.add_argument(\n",
    "        \"--gamma-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e-2,\n",
    "        type=float,\n",
    "        help=(\"Ambiguity regularization parameter.\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--pi-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e-1,\n",
    "        type=float,\n",
    "        help=(\n",
    "            \"Heterogeneity regularization parameter (will be scaled by 1 / s).\"\n",
    "        ),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--rho-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e0,\n",
    "        type=float,\n",
    "        help=(\"Diversity regularization parameter.\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--epsilon-hyper\", metavar=\"FLOAT\", default=0.01, type=float\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--epsilon\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=None,\n",
    "        type=float,\n",
    "        help=(\"Fixed error rate for all samples.\"),\n",
    "    )\n",
    "    p.add_argument(\"--alpha-hyper\", metavar=\"FLOAT\", default=100.0, type=float)\n",
    "    p.add_argument(\n",
    "        \"--alpha\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=None,\n",
    "        type=float,\n",
    "        help=(\"Fixed concentration for all samples.\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--collapse\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=0.0,\n",
    "        type=float,\n",
    "        help=(\"Merge strains with a cosine distance of less than this value.\"),\n",
    "    )\n",
    "\n",
    "    # Fitting\n",
    "    p.add_argument(\"--random-seed\", default=0, type=int, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--max-iter\", default=10000, type=int, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--lag\", default=50, type=int, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--stop-at\", default=5.0, type=float, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--learning-rate\", default=1e-0, type=float, help=(\"FIXME\"))\n",
    "    p.add_argument(\"--clip-norm\", default=100.0, type=float, help=(\"FIXME\"))\n",
    "\n",
    "    # Hardware\n",
    "    p.add_argument(\"--device\", default=\"cpu\", help=(\"PyTorch device name.\"))\n",
    "\n",
    "    # Output\n",
    "    p.add_argument(\n",
    "        \"--outpath\",\n",
    "        metavar=\"PATH\",\n",
    "        help=(\"Path for genotype fraction output.\"),\n",
    "    )\n",
    "\n",
    "    args = p.parse_args(argv)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\n",
    "        \"error\",\n",
    "        message=\"Encountered NaN: loss\",\n",
    "        category=UserWarning,\n",
    "        # module=\"trace_elbo\",  # FIXME: What is the correct regex for module?\n",
    "        lineno=217,\n",
    "    )\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"CUDA initialization: Found no NVIDIA\",\n",
    "        category=UserWarning,\n",
    "        lineno=130,\n",
    "    )\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"torch.tensor results are registered as constants\",\n",
    "        category=torch.jit.TracerWarning,\n",
    "        # module=\"trace_elbo\",  # FIXME: What is the correct regex for module?\n",
    "        lineno=95,\n",
    "    )\n",
    "\n",
    "    args = parse_args(sys.argv[1:])\n",
    "    info(args)\n",
    "\n",
    "    info(f\"Setting random seed: {args.random_seed}\")\n",
    "    np.random.seed(args.random_seed)\n",
    "\n",
    "    info(\"Loading input data.\")\n",
    "    data = _load_input_data(args.pileup)\n",
    "    info(f\"Full data shape: {data.sizes}.\")\n",
    "\n",
    "    info(\"Filtering positions.\")\n",
    "    informative_positions = select_informative_positions(\n",
    "        data, args.incid_thresh\n",
    "    )\n",
    "    npos_available = len(informative_positions)\n",
    "    info(\n",
    "        f\"Found {npos_available} informative positions with minor \"\n",
    "        f\"allele incidence of >{args.incid_thresh}\"\n",
    "    )\n",
    "    npos = min(args.npos, npos_available)\n",
    "    info(f\"Randomly sampling {npos} positions.\")\n",
    "    position_ss = np.random.choice(\n",
    "        informative_positions,\n",
    "        size=npos,\n",
    "        replace=False,\n",
    "    )\n",
    "\n",
    "    info(\"Filtering libraries.\")\n",
    "    suff_cvrg_samples = idxwhere(\n",
    "        (\n",
    "            (\n",
    "                data.sel(position=informative_positions).sum([\"allele\"]) > 0\n",
    "            ).mean(\"position\")\n",
    "            > args.cvrg_thresh\n",
    "        ).to_series()\n",
    "    )\n",
    "    nlibs = len(suff_cvrg_samples)\n",
    "    info(\n",
    "        f\"Found {nlibs} libraries with >{args.cvrg_thresh:0.1%} \"\n",
    "        f\"of informative positions covered.\"\n",
    "    )\n",
    "\n",
    "    info(\"Constructing input data.\")\n",
    "    data_fit = data.sel(library_id=suff_cvrg_samples, position=position_ss)\n",
    "    m_ss = data_fit.sum(\"allele\")\n",
    "    n, g_ss = m_ss.shape\n",
    "    y_obs_ss = data_fit.sel(allele=\"alt\")\n",
    "\n",
    "    info(\"Optimizing model parameters.\")\n",
    "    mapest1, history1 = find_map(\n",
    "        model_fit,\n",
    "        init=as_torch_all(\n",
    "            gamma=init_genotype,\n",
    "            pi=init_frac,\n",
    "            dtype=torch.float32,\n",
    "            device=args.device,\n",
    "        ),\n",
    "        lag=args.lag,\n",
    "        stop_at=args.stop_at,\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_iter=args.max_iter,\n",
    "        clip_norm=args.clip_norm,\n",
    "    )\n",
    "    if args.device.startswith(\"cuda\"):\n",
    "        info(\n",
    "            \"CUDA available mem: {}\".format(\n",
    "                torch.cuda.get_device_properties(0).total_memory\n",
    "            ),\n",
    "        )\n",
    "        info(\"CUDA reserved mem: {}\".format(torch.cuda.memory_reserved(0)))\n",
    "        info(\"CUDA allocated mem: {}\".format(torch.cuda.memory_allocated(0)))\n",
    "        info(\n",
    "            \"CUDA free mem: {}\".format(\n",
    "                torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)\n",
    "            )\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    info(\"Finished fitting model.\")\n",
    "    result = xr.Dataset(\n",
    "        {\n",
    "            \"gamma\": ([\"strain\", \"position\"], mapest3[\"gamma\"]),\n",
    "            \"rho\": ([\"strain\"], mapest3[\"rho\"]),\n",
    "            \"alpha_hyper\": ([], mapest3[\"alpha_hyper\"]),\n",
    "            \"pi\": ([\"library_id\", \"strain\"], mapest3[\"pi\"]),\n",
    "            \"epsilon\": ([\"library_id\"], mapest3[\"epsilon\"]),\n",
    "            \"rho_hyper\": ([], mapest3[\"rho_hyper\"]),\n",
    "            \"epsilon_hyper\": ([], mapest3[\"epsilon_hyper\"]),\n",
    "            \"pi_hyper\": ([], mapest3[\"pi_hyper\"]),\n",
    "            \"alpha\": ([\"library_id\"], mapest3[\"alpha\"]),\n",
    "            \"p_noerr\": ([\"library_id\", \"position\"], mapest3[\"p_noerr\"]),\n",
    "            \"p\": ([\"library_id\", \"position\"], mapest3[\"p\"]),\n",
    "            \"y\": ([\"library_id\", \"position\"], y_obs_ss),\n",
    "            \"m\": ([\"library_id\", \"position\"], m_ss),\n",
    "            \"elbo_trace\": ([\"iteration\"], history1),\n",
    "        },\n",
    "        coords=dict(\n",
    "            strain=np.arange(s_collapse),\n",
    "            position=data_fit.position,\n",
    "            library_id=data_fit.library_id,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if args.outpath:\n",
    "        info(\"Saving results.\")\n",
    "        result.to_netcdf(\n",
    "            args.outpath,\n",
    "            encoding=dict(\n",
    "                gamma=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                pi=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                p_noerr=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                p=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "                y=dict(dtype=\"uint16\", zlib=True, complevel=6),\n",
    "                m=dict(dtype=\"uint16\", zlib=True, complevel=6),\n",
    "                elbo_trace=dict(dtype=\"float32\", zlib=True, complevel=6),\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "delta_hyper_temp, delta_hyper_p = 0.1, 0.9\n",
    "x = pyro.sample('delta', dist.Beta(delta_hyper_temp * (delta_hyper_p), delta_hyper_temp * (1 - delta_hyper_p)).expand([10000])).cpu().numpy()\n",
    "plt.hist(x, bins=100)\n",
    "print(x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_hyper_alpha, epsilon_hyper_beta = 1.5, 1.5 / 0.01\n",
    "plt.hist(pyro.sample('epsilon_hyper', dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta).expand([10000])).cpu().numpy(), bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pyro.sample('test', sf.model.NegativeBinomialReparam(torch.tensor(10.), r=torch.tensor(1.), eps=torch.tensor(1e-5)).expand([1000])).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.pyro_util.shape_info(sf.model.model, n=100, g=200, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimShape-1: Small study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "pyro.util.set_rng_seed(seed)\n",
    "\n",
    "n_sim = 100\n",
    "g_sim = 5000\n",
    "s_sim = 20\n",
    "\n",
    "sim1 = sf.model.simulate(\n",
    "    sf.model.condition_model(\n",
    "        sf.model.model,\n",
    "        data=dict(\n",
    "            alpha_hyper_mean=100.\n",
    "        ),\n",
    "        n=n_sim,\n",
    "        g=g_sim,\n",
    "        s=s_sim,\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_p=0.7,\n",
    "        pi_hyper=0.5,\n",
    "        rho_hyper=10.,\n",
    "        mu_hyper_mean=2.,\n",
    "        mu_hyper_scale=0.5,\n",
    "        m_hyper_r=10.,\n",
    "        alpha_hyper_scale=0.5,\n",
    "        epsilon_hyper_alpha=1.5,\n",
    "        epsilon_hyper_beta=1.5/0.01,\n",
    "        device='cpu'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plt = 100\n",
    "g_plt = 200\n",
    "s_plt = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_community(sim1['pi'][:s_plt, :n_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(\n",
    "    sf.genotype.counts_to_p_estimate(\n",
    "        sim1['y'][:n_plt, :g_plt],\n",
    "        sim1['m'][:n_plt, :g_plt]),\n",
    "    linkage_kw=dict(progress=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_similarity(sf.genotype.counts_to_p_estimate(sim1['y'][:n_plt, :g_plt], sim1['m'][:n_plt, :g_plt]), linkage_kw=dict(progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(sim1['gamma'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_missing(sim1['delta'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_missing(sim1['nu'][:n_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(sim1['m'][:n_plt, :g_plt], norm=mpl.colors.SymLogNorm(linthresh=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_similarity(sim1['gamma'][:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1['epsilon'], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1['alpha'], bins=20)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fit = 1000  # sim1['y'].shape[1]\n",
    "n_fit = sim1['y'].shape[0]\n",
    "\n",
    "sim1_gamma_init, sim1_pi_init, sim1_cdmat = sf.estimation.initialize_parameters_by_clustering_samples(\n",
    "    sim1['y'][:n_fit, :g_fit],\n",
    "    sim1['m'][:n_fit, :g_fit],\n",
    "    thresh=0.05,\n",
    "    additional_strains_factor=0.,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "print(sim1_pi_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype(sim1_gamma_init[:s_plt, :g_plt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_similarity(sim1_gamma_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_community(sim1_pi_init[:n_plt, :s_plt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fit = sim1_gamma_init.shape[0]\n",
    "initialize_params = dict(gamma=sim1_gamma_init, pi=sim1_pi_init)\n",
    "\n",
    "sim1_fit1, history = sf.estimation.estimate_parameters(\n",
    "    sf.model.model,\n",
    "    data=dict(y=sim1['y'][:, :g_fit], m=sim1['m'][:, :g_fit]),\n",
    "    n=n_fit,\n",
    "    g=g_fit,\n",
    "    s=s_fit,\n",
    "    gamma_hyper=0.1,\n",
    "    pi_hyper=1.0,\n",
    "    rho_hyper=0.5,\n",
    "    mu_hyper_mean=5,\n",
    "    mu_hyper_scale=5.,\n",
    "    m_hyper_r=10.,\n",
    "    delta_hyper_temp=0.1,\n",
    "    delta_hyper_p=0.9,\n",
    "    alpha_hyper_hyper_mean=100.,\n",
    "    alpha_hyper_hyper_scale=10.,\n",
    "    alpha_hyper_scale=0.5,\n",
    "    epsilon_hyper_alpha=1.5,\n",
    "    epsilon_hyper_beta=1.5 / 0.01,\n",
    "    initialize_params=initialize_params,\n",
    "    device='cpu',\n",
    "    lag=100,\n",
    "    lr=1e-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1_fit1_gamma_merge, sim1_fit1_pi_merge, sim1_fit1_delta_merge  = sf.estimation.merge_similar_genotypes(\n",
    "    sim1_fit1['gamma'],\n",
    "    sim1_fit1['pi'],\n",
    "    delta=sim1_fit1['delta'],\n",
    "    thresh=0.1,\n",
    ")\n",
    "\n",
    "# print(sim1_gamma_init.shape[0], sim1_fit1['gamma'].shape[0], sim1_fit1_gamma_merge.shape[0])\n",
    "print(sim1_fit1['gamma'].shape[0], sim1_fit1_gamma_merge.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1_gamma_adjusted = sf.genotype.mask_missing_genotype(sim1['gamma'][:, :g_fit], sim1['delta'][:, :g_fit])\n",
    "sim1_fit1_gamma_adjusted = sf.genotype.mask_missing_genotype(sim1_fit1['gamma'], sim1_fit1['delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_genotype_comparison(\n",
    "    data=dict(\n",
    "        true=sim1_gamma_adjusted[:, :g_plt],\n",
    "#         fit=sim1_fit1['gamma'][:, :g_plt],\n",
    "        adj=sim1_fit1_gamma_adjusted[:, :g_plt],\n",
    "#         init=sim1_gamma_init,\n",
    "#         merg=sim1_fit1_gamma_merge,\n",
    "    ),\n",
    "    linkage_kw=dict(progress=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_community_comparison(\n",
    "    data=dict(\n",
    "        true=sim1['pi'],\n",
    "        fit=sim1_fit1['pi'],\n",
    "#         init=sim1_pi_init,\n",
    "#         merg=sim1_fit1_pi_merge,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['epsilon'], sim1_fit1['epsilon'])\n",
    "plt.plot([0, 0.04], [0, 0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['alpha'], sim1_fit1['alpha'])\n",
    "plt.plot([0, 200], [0, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim1_fit1['delta'], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['mu'], sim1_fit1['mu'])\n",
    "plt.plot([0, 40], [0, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot comparing genotype accuracy to true strain abundance\n",
    "# colored by mean entropy of the estimated genotype masked by delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1_fit1['alpha'], sample_mean_masked_genotype_entropy(sim1_fit1['pi'], sim1_fit1['gamma'], sim1_fit1['delta']))\n",
    "sample_mean_masked_genotype_entropy(sim1_fit1['pi'], sim1_fit1['gamma'], sim1_fit1['delta']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hit, best_dist = match_genotypes(sim1_gamma_adjusted[:, :g_fit], sim1_fit1_gamma_adjusted[:, :g_fit])\n",
    "\n",
    "print('weighted_mean_distance:', (best_dist * sim1['pi'].mean(0)).sum())\n",
    "plt.scatter((sim1['pi'] * sim1['mu'].reshape(-1, 1)).sum(0), best_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_sim = 1 - pdist(sim1['pi'], metric='braycurtis')\n",
    "bc_fit = 1 - pdist(sim1_fit1['pi'], metric='braycurtis')\n",
    "plt.scatter(\n",
    "    bc_sim,\n",
    "    bc_fit,\n",
    "    marker='.',\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "community_accuracy_test(sim1['pi'], sim1_fit1['pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strains that are not representative of true haplotypes\n",
    "# are high entropy (even after masking with delta)\n",
    "# and have low estimated total coverage.\n",
    "\n",
    "best_true_strain, best_true_strain_dist = match_genotypes(sim1_fit1_gamma_adjusted[:, :g_fit], sim1_gamma_adjusted[:, :g_fit])\n",
    "best_true_strain_dist\n",
    "\n",
    "plt.scatter((sim1_fit1['pi'] * sim1_fit1['mu'].reshape((-1, 1))).sum(0), best_true_strain_dist, c=mean_masked_genotype_entropy(sim1_fit1['gamma'], sim1_fit1['delta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1_fit1_gamma_adjusted[:, :g_plt], linkage_kw=dict(progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_community(sim1_fit1['pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_masked_genotype_entropy(sim1_fit1['gamma'][:, :g_plt], sim1_fit1['delta'][:, :g_plt]))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim1_fit1['alpha'], bins=20)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_genotype(sim1_fit1['gamma'][mean_masked_genotype_entropy(sim1_fit1['gamma'], sim1_fit1['delta']) < 0.1, :g_fit])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "82px",
    "width": "168px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}