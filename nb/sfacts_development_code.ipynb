{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sfacts.logging_util import *\n",
    "from sfacts.pyro_util import *\n",
    "from sfacts.model import *\n",
    "from sfacts.genotype import *\n",
    "from sfacts.plot import *\n",
    "from sfacts.estimation import *\n",
    "from sfacts.evaluation import *\n",
    "from sfacts.workflow import *\n",
    "from sfacts.data import *\n",
    "from sfacts.pandas_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sfacts as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from torch.jit import TracerWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\",\n",
    "    category=torch.jit.TracerWarning,\n",
    "#     module=\"trace_elbo\",  # FIXME: What is the correct regex for module?\n",
    "#     lineno=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree sfacts -I __pycache__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/logging_util.py\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "\n",
    "def info(*msg, quiet=False):\n",
    "    now = datetime.now()\n",
    "    if not quiet:\n",
    "        print(f\"[{now}]\", *msg, file=sys.stderr, flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyro_util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/data.py\n",
    "from sfacts.logging_util import info\n",
    "from sfacts.pandas_util import idxwhere\n",
    "from sfacts.math import binary_entropy\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import scipy as sp\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def _on_2_simplex(d):\n",
    "    return (d.min() >= 0) and (d.max() <= 1.0)\n",
    "\n",
    "\n",
    "def _strictly_positive(d):\n",
    "    return d.min() > 0\n",
    "\n",
    "\n",
    "def _positive_counts(d):\n",
    "    return (d.astype(int) == d).all()\n",
    "\n",
    "\n",
    "class WrappedDataArrayMixin:\n",
    "    constraints = {}\n",
    "\n",
    "    # The following are all white-listed and\n",
    "    # transparently passed through to self.data, but with\n",
    "    # different symantics for the return value.\n",
    "    dims = ()\n",
    "    safe_unwrapped = [\n",
    "        \"shape\",\n",
    "        \"sizes\",\n",
    "        \"to_pandas\",\n",
    "        \"to_dataframe\",\n",
    "        \"min\",\n",
    "        \"max\",\n",
    "        \"sum\",\n",
    "        \"mean\",\n",
    "        \"median\",\n",
    "        \"values\",\n",
    "        \"pipe\",\n",
    "        \"to_series\",\n",
    "        \"isel\",\n",
    "        \"sel\",\n",
    "    ]\n",
    "    # safe_lifted = []\n",
    "    variable_name = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_ndarray(cls, x, coords=None):\n",
    "        if coords is None:\n",
    "            coords = {k: None for k in cls.dims}\n",
    "        shapes = {k: x.shape[i] for i, k in enumerate(cls.dims)}\n",
    "        for k in coords:\n",
    "            if coords[k] is None:\n",
    "                coords[k] = range(shapes[k])\n",
    "        data = xr.DataArray(\n",
    "            x,\n",
    "            dims=cls.dims,\n",
    "            coords=coords,\n",
    "        )\n",
    "        return cls(data)\n",
    "\n",
    "    @classmethod\n",
    "    def stack(cls, mapping, dim, prefix=False, validate=True):\n",
    "        if not len(cls.dims) == 2:\n",
    "            raise NotImplementedError(\n",
    "                \"Generic stacking has only been implemented for 2D wrapped DataArrays\"\n",
    "            )\n",
    "        axis = cls.dims.index(dim)\n",
    "        data = []\n",
    "        for k, d in mapping.items():\n",
    "            if prefix:\n",
    "                d = (\n",
    "                    d.to_pandas()\n",
    "                    .rename(lambda s: f\"{k}_{s}\", axis=axis)\n",
    "                    .stack()\n",
    "                    .to_xarray()\n",
    "                )\n",
    "            else:\n",
    "                d = d.data\n",
    "            data.append(d)\n",
    "        out = cls(xr.concat(data, dim=dim))\n",
    "        if validate:\n",
    "            out.validate_constraints()\n",
    "        return out\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.validate_fast()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self.dims:\n",
    "            return getattr(self.data, name)\n",
    "        elif name in self.safe_unwrapped:\n",
    "            return getattr(self.data, name)\n",
    "        # elif name in self.safe_lifted:\n",
    "        #     return lambda *args, **kwargs: self.__class__(\n",
    "        #         getattr(self.data, name)(*args, **kwargs)\n",
    "        #     )\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f\"'{self.__class__.__name__}' object has no attribute '{name}' \"\n",
    "                f\"and this name is not found in '{self.__class__.__name__}.dims', \"\n",
    "                f\"'{self.__class__.__name__}.safe_unwrapped', \"\n",
    "                f\"or '{self.__class__.__name__}.safe_lifted'. \"\n",
    "                f\"Consider working with the '{self.__class__.__name__}.data' \"\n",
    "                f\"xr.DataArray object directly.\"\n",
    "            )\n",
    "\n",
    "    def validate_fast(self):\n",
    "        assert len(self.data.shape) == len(self.dims)\n",
    "        assert self.data.dims == self.dims\n",
    "\n",
    "    def validate_constraints(self):\n",
    "        self.validate_fast()\n",
    "        for name in self.constraints:\n",
    "            assert self.constraints[name](self.data), f\"Failed constraint: {name}\"\n",
    "\n",
    "    def lift(self, func, *args, **kwargs):\n",
    "        return self.__class__(func(self.data, *args, **kwargs))\n",
    "\n",
    "    def mlift(self, name, *args, **kwargs):\n",
    "        func = getattr(self, name)\n",
    "        return self.__class__(func(*args, **kwargs))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.data})\"\n",
    "\n",
    "    @classmethod\n",
    "    def concat(cls, data, dim):\n",
    "        out_data = []\n",
    "        new_coords = []\n",
    "        for name in data:\n",
    "            d = data[name].data\n",
    "            out_data.append(d)\n",
    "            new_coords.extend([f\"{name}_{i}\" for i in d[dim].values])\n",
    "        out_data = xr.concat(out_data, dim)\n",
    "        out_data[dim] = new_coords\n",
    "        return cls(out_data)\n",
    "\n",
    "    def to_world(self):\n",
    "        return World(self.data.to_dataset())\n",
    "    \n",
    "    \n",
    "    def random_sample(self, n, dim, replace=False, keep_order=True):\n",
    "        dim_n = self.data.sizes[dim]\n",
    "        ii = np.random.choice(np.arange(dim_n), size=n, replace=replace)\n",
    "        if keep_order:\n",
    "            ii = sorted(ii)\n",
    "        return self.__class__(data=self.data.isel(**{dim: ii}))\n",
    "\n",
    "\n",
    "class Metagenotypes(WrappedDataArrayMixin):\n",
    "    dims = (\"sample\", \"position\", \"allele\")\n",
    "    constraints = dict(positive_counts=_positive_counts)\n",
    "    variable_name = \"metagenotypes\"\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename_or_obj, validate=True):\n",
    "        data = (\n",
    "            xr.open_dataarray(filename_or_obj)\n",
    "            .rename({\"library_id\": \"sample\"})\n",
    "            .squeeze(drop=True)\n",
    "        )\n",
    "        data.name = \"metagenotypes\"\n",
    "        result = cls(data)\n",
    "        if validate:\n",
    "            result.validate_constraints()\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def from_counts_and_totals(cls, y, m, coords=None):\n",
    "        if coords is None:\n",
    "            coords = {}\n",
    "        if not \"allele\" in coords:\n",
    "            coords[\"allele\"] = [\"alt\", \"ref\"]\n",
    "        x = np.stack([y, m - y], axis=-1)\n",
    "        return cls.from_ndarray(x, coords=coords)\n",
    "\n",
    "    def dump(self, path, validate=True):\n",
    "        if validate:\n",
    "            self.validate_constraints()\n",
    "        self.data.astype(np.uint8).to_dataset(name=\"tally\").to_netcdf(\n",
    "            path, encoding=dict(tally=dict(zlib=True, complevel=6))\n",
    "        )\n",
    "\n",
    "    def select_variable_positions(self, thresh):\n",
    "        # TODO: Consider using .lift() to do this.\n",
    "        variable_positions = (\n",
    "            self.data\n",
    "            .argmin('allele', skipna=False)\n",
    "            .mean('sample')\n",
    "            .pipe(\n",
    "                lambda x: (x > thresh) &\n",
    "                (x < (1 - thresh))\n",
    "            )\n",
    "        )\n",
    "        return self.mlift(\"sel\", position=variable_positions)\n",
    "\n",
    "    def select_samples_with_coverage(self, cvrg_thresh):\n",
    "        # TODO: Consider using .lift() to do this.\n",
    "        x = self.data\n",
    "        covered_samples = (x.sum(\"allele\") > 0).mean(\"position\") > cvrg_thresh\n",
    "        return self.mlift(\"sel\", sample=covered_samples)\n",
    "\n",
    "    def frequencies(self, pseudo=0.0):\n",
    "        \"Convert metagenotype counts to a frequency with optional pseudocount.\"\n",
    "        return (self.data + pseudo) / (\n",
    "            self.data.sum(\"allele\") + pseudo * self.sizes[\"allele\"]\n",
    "        )\n",
    "\n",
    "    def dominant_allele_fraction(self, pseudo=0.0):\n",
    "        \"Convert metagenotype counts to a frequencies with optional pseudocount.\"\n",
    "        return self.frequencies(pseudo=pseudo).max(\"allele\")\n",
    "\n",
    "    def alt_allele_fraction(self, pseudo=0.0):\n",
    "        return self.frequencies(pseudo=pseudo).sel(allele=\"alt\")\n",
    "\n",
    "    def to_estimated_genotypes(self, pseudo=1.0):\n",
    "        return Genotypes(\n",
    "            self.alt_allele_fraction(pseudo=pseudo).rename({\"sample\": \"strain\"})\n",
    "        )\n",
    "\n",
    "    def total_counts(self):\n",
    "        return self.data.sum(\"allele\")\n",
    "\n",
    "    def allele_counts(self, allele=\"alt\"):\n",
    "        return self.sel(allele=allele)\n",
    "\n",
    "    def mean_depth(self, dim=\"sample\"):\n",
    "        if dim == \"sample\":\n",
    "            over = \"position\"\n",
    "        elif dim == \"position\":\n",
    "            over = \"sample\"\n",
    "        return self.total_counts().mean(over)\n",
    "\n",
    "    def to_counts_and_totals(self, binary_allele=\"alt\"):\n",
    "        return dict(\n",
    "            y=self.allele_counts(allele=binary_allele).values,\n",
    "            m=self.total_counts().values,\n",
    "        )\n",
    "\n",
    "    def pdist(self, dim=\"sample\", pseudo=1.0, **kwargs):\n",
    "        if dim == \"sample\":\n",
    "            _dim = \"strain\"\n",
    "        else:\n",
    "            _dim = dim\n",
    "        return (\n",
    "            self.to_estimated_genotypes(pseudo=pseudo)\n",
    "            .pdist(dim=_dim, **kwargs)\n",
    "            .rename_axis(columns=dim, index=dim)\n",
    "        )\n",
    "\n",
    "    def cosine_pdist(self, dim=\"sample\"):\n",
    "        if dim != \"sample\":\n",
    "            raise NotImplementedError(\"Only dim 'sample' has been implemented.\")\n",
    "        d = self.to_dataframe().unstack(dim).T\n",
    "        return pd.DataFrame(\n",
    "            squareform(pdist(d.values, metric=\"cosine\")), index=d.index, columns=d.index\n",
    "        )\n",
    "\n",
    "    def linkage(self, dim=\"sample\", pseudo=1.0, **kwargs):\n",
    "        if dim == \"sample\":\n",
    "            _dim = \"strain\"\n",
    "        else:\n",
    "            _dim = dim\n",
    "        return self.to_estimated_genotypes(pseudo=pseudo).linkage(dim=_dim, **kwargs)\n",
    "\n",
    "    def cosine_linkage(\n",
    "        self,\n",
    "        dim=\"sample\",\n",
    "        method=\"complete\",\n",
    "        optimal_ordering=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        dmat = self.cosine_pdist(dim=dim)\n",
    "        cdmat = squareform(dmat)\n",
    "        return linkage(\n",
    "            cdmat, method=method, optimal_ordering=optimal_ordering, **kwargs\n",
    "        )\n",
    "\n",
    "    def entropy(self, dim=\"sample\"):\n",
    "        if dim == \"sample\":\n",
    "            over = \"position\"\n",
    "        elif dim == \"position\":\n",
    "            over = \"sample\"\n",
    "        p = self.dominant_allele_fraction()\n",
    "        ent = binary_entropy(p)\n",
    "        return ent.sum(over).rename(\"entropy\")\n",
    "\n",
    "\n",
    "class Genotypes(WrappedDataArrayMixin):\n",
    "    dims = (\"strain\", \"position\")\n",
    "    constraints = dict(on_2_simplex=_on_2_simplex)\n",
    "    variable_name = \"genotypes\"\n",
    "\n",
    "    def softmask_missing(self, missingness, eps=1e-10):\n",
    "        clip = partial(np.clip, a_min=eps, a_max=(1 - eps))\n",
    "        return self.lift(\n",
    "            lambda g, m: sp.special.expit(sp.special.logit(clip(g)) * clip(m)),\n",
    "            m=missingness.data,\n",
    "        )\n",
    "\n",
    "    def discretized(self):\n",
    "        return self.lift(np.round)\n",
    "\n",
    "    def fuzzed(self, eps=1e-5):\n",
    "        return self.lift(lambda x: (x + eps) / (1 + 2 * eps))\n",
    "\n",
    "    # TODO: Move distance metrics to a new module?\n",
    "    @staticmethod\n",
    "    def _convert_to_sign_representation(p):\n",
    "        \"Alternative representation of binary genotype on a [-1, 1] interval.\"\n",
    "        return p * 2 - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _genotype_sign_representation_dissimilarity(x, y, pseudo=0.0):\n",
    "        \"Dissimilarity between 1D genotypes, accounting for fuzzyness.\"\n",
    "        dist = ((x - y) / 2) ** 2\n",
    "        weight = (x * y) ** 2\n",
    "        wmean_dist = ((weight * dist).mean()) / ((weight.mean() + pseudo))\n",
    "        return wmean_dist\n",
    "\n",
    "    @staticmethod\n",
    "    def _genotype_dissimilarity(x, y, pseudo=0.0):\n",
    "        return self._genotype_sign_representation_dissimilarity(\n",
    "            self._genotype_p_to_s(x), self._genotype_p_to_s(y)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _genotype_dissimilarity_cdmat(unwrapped_values, pseudo=0.0, quiet=True):\n",
    "        g_sign = Genotypes._convert_to_sign_representation(unwrapped_values)\n",
    "        s, _ = g_sign.shape\n",
    "        cdmat = np.empty((s * (s - 1)) // 2)\n",
    "        k = 0\n",
    "        with tqdm(total=len(cdmat), disable=quiet) as pbar:\n",
    "            for i in range(0, s - 1):\n",
    "                for j in range(i + 1, s):\n",
    "                    cdmat[k] = Genotypes._genotype_sign_representation_dissimilarity(\n",
    "                        g_sign[i], g_sign[j], pseudo=pseudo\n",
    "                    )\n",
    "                    k = k + 1\n",
    "                    pbar.update()\n",
    "        return cdmat\n",
    "\n",
    "    def pdist(self, dim=\"strain\", pseudo=0.0, quiet=True):\n",
    "        index = getattr(self, dim)\n",
    "        if dim == \"strain\":\n",
    "            unwrapped_values = self.values\n",
    "            cdmat = self._genotype_dissimilarity_cdmat(\n",
    "                unwrapped_values, quiet=quiet, pseudo=pseudo\n",
    "            )\n",
    "        elif dim == \"position\":\n",
    "            unwrapped_values = self.values.T\n",
    "            cdmat = pdist(\n",
    "                self._convert_to_sign_representation(self.values.T), metric=\"cosine\"\n",
    "            )\n",
    "        # Reboxing\n",
    "        dmat = pd.DataFrame(squareform(cdmat), index=index, columns=index)\n",
    "        return dmat\n",
    "\n",
    "    def linkage(\n",
    "        self,\n",
    "        dim=\"strain\",\n",
    "        pseudo=0.0,\n",
    "        quiet=True,\n",
    "        method=\"complete\",\n",
    "        optimal_ordering=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        dmat = self.pdist(dim=dim, pseudo=pseudo, quiet=quiet)\n",
    "        cdmat = squareform(dmat)\n",
    "        return linkage(\n",
    "            cdmat, method=method, optimal_ordering=optimal_ordering, **kwargs\n",
    "        )\n",
    "\n",
    "    def cosine_pdist(self, dim=\"strain\"):\n",
    "        if dim == \"strain\":\n",
    "            d = self.values\n",
    "            index = self.strain\n",
    "        elif dim == \"position\":\n",
    "            d = self.values.T\n",
    "            index = self.position\n",
    "        d = self._convert_to_sign_representation(d)\n",
    "        cdmat = pdist(d, metric=\"cosine\")\n",
    "        return pd.DataFrame(squareform(cdmat), index=index, columns=index)\n",
    "\n",
    "    def cosine_linkage(\n",
    "        self, dim=\"strain\", method=\"complete\", optimal_ordering=True, **kwargs\n",
    "    ):\n",
    "        cdmat = squareform(self.cosine_pdist(dim=dim))\n",
    "        return linkage(\n",
    "            cdmat, method=method, optimal_ordering=optimal_ordering, **kwargs\n",
    "        )\n",
    "\n",
    "    def entropy(self, dim=\"strain\"):\n",
    "        if dim == \"strain\":\n",
    "            sum_over = \"position\"\n",
    "        elif dim == \"position\":\n",
    "            sum_over = \"strain\"\n",
    "        p = self.data\n",
    "        ent = binary_entropy(p)\n",
    "        return ent.sum(sum_over).rename(\"entropy\")\n",
    "\n",
    "\n",
    "class Missingness(WrappedDataArrayMixin):\n",
    "    dims = (\"strain\", \"position\")\n",
    "    constraints = dict(on_2_simplex=_on_2_simplex)\n",
    "    variable_name = \"missingness\"\n",
    "\n",
    "\n",
    "class Communities(WrappedDataArrayMixin):\n",
    "    dims = (\"sample\", \"strain\")\n",
    "    constraints = dict(strains_sum_to_1=lambda d: (d.sum(\"strain\") == 1.0).all())\n",
    "    variable_name = \"communities\"\n",
    "\n",
    "    def fuzzed(self, eps=1e-5):\n",
    "        new_data = self.data + eps\n",
    "        new_data = new_data / new_data.sum(\"strain\")\n",
    "        return self.__class__(new_data)\n",
    "\n",
    "    def pdist(self, dim=\"strain\", quiet=True):\n",
    "        index = getattr(self, dim)\n",
    "        if dim == \"strain\":\n",
    "            unwrapped_values = self.values.T\n",
    "            cdmat = pdist(unwrapped_values, metric=\"cosine\")\n",
    "        elif dim == \"sample\":\n",
    "            unwrapped_values = self.values\n",
    "            cdmat = pdist(unwrapped_values, metric=\"braycurtis\")\n",
    "        # Reboxing\n",
    "        dmat = pd.DataFrame(squareform(cdmat), index=index, columns=index)\n",
    "        return dmat\n",
    "\n",
    "    def linkage(\n",
    "        self,\n",
    "        dim=\"strain\",\n",
    "        quiet=True,\n",
    "        method=\"average\",\n",
    "        optimal_ordering=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        dmat = self.pdist(dim=dim, quiet=quiet)\n",
    "        cdmat = squareform(dmat)\n",
    "        return linkage(\n",
    "            cdmat, method=method, optimal_ordering=optimal_ordering, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "class Overdispersion(WrappedDataArrayMixin):\n",
    "    dims = (\"sample\",)\n",
    "    constraints = dict(strains_sum_to_1=_strictly_positive)\n",
    "    variable_name = \"overdispersion\"\n",
    "\n",
    "\n",
    "class ErrorRate(WrappedDataArrayMixin):\n",
    "    dims = (\"sample\",)\n",
    "    constraints = dict(on_2_simplex=_on_2_simplex)\n",
    "    variable_name = \"error_rate\"\n",
    "\n",
    "\n",
    "class World:\n",
    "    safe_lifted = [\"isel\", \"sel\"]\n",
    "    safe_unwrapped = [\"sizes\"]\n",
    "    dims = (\"sample\", \"position\", \"strain\", \"allele\")\n",
    "    variables = [Genotypes, Missingness, Communities, Metagenotypes]\n",
    "    _variable_wrapper_map = {wrapper.variable_name: wrapper for wrapper in variables}\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # self._align_dims(data)\n",
    "        self.validate_fast()\n",
    "\n",
    "    #     @classmethod\n",
    "    #     def _align_dims(cls, data):\n",
    "    #         missing_dims = [k for k in cls.dims if k not in data.dims]\n",
    "    #         return data.expand_dims(missing_dims).transpose(*cls.dims)\n",
    "\n",
    "    def validate_fast(self):\n",
    "        assert not (\n",
    "            set(self.data.dims) - set(self.dims)\n",
    "        ), f\"Found data dims that shouldn't exist: {self.data.dims}\"\n",
    "\n",
    "    def validate_constraints(self):\n",
    "        self.validate_fast()\n",
    "        for variable_name in _variable_wrapper_map:\n",
    "            if variable_name in self.data:\n",
    "                wrapped_variable = getattr(self, name)\n",
    "                wrapped_variable.validate_constraints()\n",
    "\n",
    "    def random_sample(self, n, dim, replace=False, keep_order=True):\n",
    "        dim_n = self.data.sizes[dim]\n",
    "        ii = np.random.choice(np.arange(dim_n), size=n, replace=replace)\n",
    "        if keep_order:\n",
    "            ii = sorted(ii)\n",
    "        return self.__class__(data=self.data.isel(**{dim: ii}))\n",
    "\n",
    "    @property\n",
    "    def masked_genotypes(self):\n",
    "        return self.genotypes.softmask_missing(self.missingness)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self.dims:\n",
    "            # Return dims for those registered in self.dims.\n",
    "            return getattr(self.data, name)\n",
    "        if name in self._variable_wrapper_map:\n",
    "            # Return wrapped variables for those registered in self.variables.\n",
    "            return self._variable_wrapper_map[name](self.data[name])\n",
    "        elif name in self.safe_unwrapped:\n",
    "            # Return a naked version of the variables registered in self.safe_unwrapped\n",
    "            return getattr(self.data, name)\n",
    "        elif name in self.safe_lifted:\n",
    "            # Return a lifted version of the the attributes registered in safe_lifted\n",
    "            return lambda *args, **kwargs: self.__class__(\n",
    "                getattr(self.data, name)(*args, **kwargs)\n",
    "            )\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f\"'{self.__class__.__name__}' object has no attribute '{name}' \"\n",
    "                f\"and this name is not found in '{self.__class__.__name__}.dims', \"\n",
    "                f\"'{self.__class__.__name__}.safe_unwrapped', \"\n",
    "                f\"or '{self.__class__.__name__}.safe_lifted'. \"\n",
    "                f\"Consider working with the '{self.__class__.__name__}.data' object directly.\"\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def concat(cls, data, dim, rename_coords=False):\n",
    "        new_coords = []\n",
    "        # Add source metadata and rename concatenation coordinates\n",
    "        renamed_data = []\n",
    "        shared_variables = set([str(v) for v in list(data.values())[0].data.variables])\n",
    "        for name in data:\n",
    "            d = data[name].data.copy()\n",
    "            d[\"_concat_from\"] = xr.DataArray(name, dims=(dim,), coords={dim: d[dim]})\n",
    "            if rename_coords:\n",
    "                new_coords.extend([f\"{name}_{i}\" for i in d[dim].values])\n",
    "            else:\n",
    "                new_coords.extend(d[dim].values)\n",
    "            shared_variables &= set([str(v) for v in d.variables])\n",
    "            renamed_data.append(d)\n",
    "        # Drop unshared variables\n",
    "        ready_data = []\n",
    "        for d in renamed_data:\n",
    "            ready_data.append(d[list(shared_variables - set(cls.dims))])\n",
    "        # Concatenate\n",
    "        out_data = xr.concat(\n",
    "            ready_data, dim, data_vars=\"minimal\", coords=\"minimal\", compat=\"override\"\n",
    "        )\n",
    "        out_data[dim] = new_coords\n",
    "        return cls(out_data)\n",
    "\n",
    "\n",
    "def latent_metagenotypes_pdist(world, dim='sample'):\n",
    "    if dim == 'sample':\n",
    "        dim = 'strain'\n",
    "    return Genotypes(world.data.p.rename({\"sample\": \"strain\"})).pdist(dim=dim)\n",
    "\n",
    "\n",
    "def latent_metagenotypes_linkage(world, dim='sample', method=\"average\", optimal_ordering=True):\n",
    "    return linkage(\n",
    "        squareform(latent_metagenotypes_pdist(world, dim=dim)),\n",
    "        method=method,\n",
    "        optimal_ordering=optimal_ordering,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/plot.py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.spatial.distance import squareform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sfacts as sf\n",
    "from functools import partial\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "\n",
    "def _calculate_clustermap_sizes(\n",
    "    nx,\n",
    "    ny,\n",
    "    scalex=0.15,\n",
    "    scaley=0.02,\n",
    "    cwidth=0,\n",
    "    cheight=0,\n",
    "    dwidth=0.2,\n",
    "    dheight=1.0,\n",
    "):\n",
    "    # TODO: Incorporate colors.\n",
    "    mwidth = nx * scalex\n",
    "    mheight = ny * scaley\n",
    "    fwidth = mwidth + cwidth + dwidth\n",
    "    fheight = mheight + cheight + dheight\n",
    "    dendrogram_ratio = (dwidth / fwidth, dheight / fheight)\n",
    "    colors_ratio = (cwidth / fwidth, cheight / fheight)\n",
    "    return (fwidth, fheight), dendrogram_ratio, colors_ratio\n",
    "\n",
    "\n",
    "def _min_max_normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "def _scale_to_max_of_one(x):\n",
    "    return x / x.max()\n",
    "\n",
    "\n",
    "def dictionary_union(*args):\n",
    "    out = args[0].copy()\n",
    "    for a in args:\n",
    "        out.update(a)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_generic_clustermap_factory(\n",
    "    matrix_func,\n",
    "    col_colors_func=None,\n",
    "    row_colors_func=None,\n",
    "    col_linkage_func=None,\n",
    "    row_linkage_func=None,\n",
    "    row_col_annotation_cmap=mpl.cm.viridis,\n",
    "    scalex=0.05,\n",
    "    scaley=0.05,\n",
    "    cwidth=0.1,\n",
    "    cheight=0.1,\n",
    "    dwidth=1.0,\n",
    "    dheight=1.0,\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    center=None,\n",
    "    cmap=None,\n",
    "    norm=mpl.colors.PowerNorm(1.0),\n",
    "    xticklabels=0,\n",
    "    yticklabels=0,\n",
    "    metric=\"correlation\",\n",
    "    cbar_pos=None,\n",
    "    transpose=False,\n",
    "    isel=None\n",
    "):\n",
    "    def _plot_func(\n",
    "        world,\n",
    "        matrix_func=matrix_func,\n",
    "        col_linkage_func=col_linkage_func,\n",
    "        row_linkage_func=row_linkage_func,\n",
    "        col_colors_func=col_colors_func,\n",
    "        row_colors_func=row_colors_func,\n",
    "        row_col_annotation_cmap=row_col_annotation_cmap,\n",
    "        scalex=scalex,\n",
    "        scaley=scaley,\n",
    "        cwidth=cwidth,\n",
    "        cheight=cheight,\n",
    "        dwidth=dwidth,\n",
    "        dheight=dheight,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        center=center,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        xticklabels=xticklabels,\n",
    "        yticklabels=yticklabels,\n",
    "        metric=metric,\n",
    "        cbar_pos=cbar_pos,\n",
    "        transpose=transpose,\n",
    "        isel=isel,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        matrix_data = matrix_func(world)\n",
    "        \n",
    "        if isel is None:\n",
    "            isel={}\n",
    "            \n",
    "        matrix_data = matrix_data.isel(**isel).to_pandas()\n",
    "\n",
    "        if transpose:\n",
    "            matrix_data = matrix_data.T\n",
    "            col_linkage_func, row_linkage_func = (\n",
    "                row_linkage_func,\n",
    "                col_linkage_func,\n",
    "            )\n",
    "            col_colors_func, row_colors_func = row_colors_func, col_colors_func\n",
    "            scalex, scaley = scaley, scalex\n",
    "            cwidth, cheight = cheight, cwidth\n",
    "            dwidth, dheight = dheight, dwidth\n",
    "            xticklabels, yticklabels = yticklabels, xticklabels\n",
    "\n",
    "        if col_linkage_func is None:\n",
    "            col_linkage = None\n",
    "        else:\n",
    "            col_linkage = col_linkage_func(world)\n",
    "\n",
    "        if row_linkage_func is None:\n",
    "            row_linkage = None\n",
    "        else:\n",
    "            row_linkage = row_linkage_func(world)\n",
    "\n",
    "        if col_colors_func is None:\n",
    "            col_colors = None\n",
    "        else:\n",
    "            col_colors = (\n",
    "                col_colors_func(world)\n",
    "                .pipe(_scale_to_max_of_one)\n",
    "                .to_dataframe()\n",
    "                .applymap(row_col_annotation_cmap)\n",
    "            )\n",
    "            cwidth *= col_colors.shape[1]\n",
    "\n",
    "        if row_colors_func is None:\n",
    "            row_colors = None\n",
    "        else:\n",
    "            row_colors = (\n",
    "                row_colors_func(world)\n",
    "                .pipe(_scale_to_max_of_one)\n",
    "                .to_dataframe()\n",
    "                .applymap(row_col_annotation_cmap)\n",
    "            )\n",
    "            cheight *= row_colors.shape[1]\n",
    "\n",
    "        ny, nx = matrix_data.shape\n",
    "        figsize, dendrogram_ratio, colors_ratio = _calculate_clustermap_sizes(\n",
    "            nx,\n",
    "            ny,\n",
    "            scalex=scalex,\n",
    "            scaley=scaley,\n",
    "            cwidth=cwidth,\n",
    "            cheight=cheight,\n",
    "            dwidth=dwidth,\n",
    "            dheight=dheight,\n",
    "        )\n",
    "        #     sf.logging_util.info(matrix_data.shape, applied_scale_kwargs, figsize, dendrogram_ratio, colors_ratio)\n",
    "\n",
    "        clustermap_kwargs = dict(\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            center=center,\n",
    "            norm=norm,\n",
    "            cmap=cmap,\n",
    "            xticklabels=xticklabels,\n",
    "            yticklabels=yticklabels,\n",
    "            col_linkage=col_linkage,\n",
    "            row_linkage=row_linkage,\n",
    "            row_colors=row_colors,\n",
    "            col_colors=col_colors,\n",
    "            figsize=figsize,\n",
    "            dendrogram_ratio=dendrogram_ratio,\n",
    "            colors_ratio=colors_ratio,\n",
    "            metric=metric,\n",
    "            cbar_pos=cbar_pos,\n",
    "        )\n",
    "        clustermap_kwargs.update(kwargs)\n",
    "\n",
    "        grid = sns.clustermap(matrix_data, **clustermap_kwargs)\n",
    "        return grid\n",
    "\n",
    "    return _plot_func\n",
    "\n",
    "\n",
    "plot_metagenotype = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.metagenotypes.alt_allele_fraction(pseudo=1.0).T,\n",
    "    row_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"position\"),\n",
    "    col_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"sample\"),\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    center=0.5,\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w.metagenotypes.sum(\"allele\")\n",
    "            .mean(\"position\")\n",
    "            .pipe(np.sqrt)\n",
    "            .rename(\"mean_depth\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "plot_expected_fractions = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.data[\"p\"].T,\n",
    "    row_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"position\"),\n",
    "    col_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"sample\"),\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    center=0.5,\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w.metagenotypes.sum(\"allele\")\n",
    "            .mean(\"position\")\n",
    "            .pipe(np.sqrt)\n",
    "            .rename(\"mean_depth\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "plot_prediction_error = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: (\n",
    "        w.data[\"p\"] - w.metagenotypes.frequencies().sel(allele=\"alt\")\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .T,\n",
    "    row_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"position\"),\n",
    "    col_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"sample\"),\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    ")\n",
    "\n",
    "plot_dominance = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.metagenotypes.dominant_allele_fraction(pseudo=1.0)\n",
    "    .T,\n",
    "    col_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"sample\"),\n",
    "    metric=\"cosine\",\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w.metagenotypes.sum(\"allele\")\n",
    "            .mean(\"position\")\n",
    "            .pipe(np.sqrt)\n",
    "            .rename(\"mean_depth\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "plot_depth = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.metagenotypes.sum(\"allele\").T,\n",
    "    row_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"position\"),\n",
    "    col_linkage_func=lambda w: w.metagenotypes.linkage(dim=\"sample\"),\n",
    "    scalex=0.15,\n",
    "    scaley=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    center=0.5,\n",
    "    cmap=\"gray\",\n",
    "    xticklabels=1,\n",
    "    yticklabels=0,\n",
    "    col_colors_func=(\n",
    "        lambda w: (\n",
    "            w.metagenotypes.sum(\"allele\")\n",
    "            .mean(\"position\")\n",
    "            .pipe(np.sqrt)\n",
    "            .rename(\"mean_depth\")\n",
    "        )\n",
    "    ),\n",
    "    norm=mpl.colors.SymLogNorm(linthresh=1.0),\n",
    ")\n",
    "\n",
    "plot_genotype = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.genotypes,\n",
    "    row_linkage_func=lambda w: w.genotypes.linkage(dim=\"strain\"),\n",
    "    col_linkage_func=lambda w: w.genotypes.linkage(dim=\"position\"),\n",
    "    scaley=0.20,\n",
    "    scalex=0.01,\n",
    "    vmin=0,\n",
    "    center=0.5,\n",
    "    vmax=1,\n",
    "    cmap=\"coolwarm\",\n",
    "    yticklabels=1,\n",
    "    xticklabels=0,\n",
    "    row_colors_func=(lambda w: (w.genotypes.entropy())),\n",
    ")\n",
    "\n",
    "plot_masked_genotype = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.masked_genotypes,\n",
    "    row_linkage_func=lambda w: w.masked_genotypes.linkage(dim=\"strain\"),\n",
    "    col_linkage_func=lambda w: w.genotypes.linkage(dim=\"position\"),\n",
    "    scaley=0.20,\n",
    "    scalex=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=\"coolwarm\",\n",
    "    yticklabels=1,\n",
    "    xticklabels=0,\n",
    "    row_colors_func=(lambda w: (w.genotypes.entropy())),\n",
    ")\n",
    "\n",
    "plot_missing = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.missingness,\n",
    "    row_linkage_func=lambda w: w.genotypes.linkage(dim=\"strain\"),\n",
    "    col_linkage_func=lambda w: w.genotypes.linkage(dim=\"position\"),\n",
    "    metric=\"cosine\",\n",
    "    scaley=0.20,\n",
    "    scalex=0.01,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=None,\n",
    "    yticklabels=1,\n",
    "    xticklabels=0,\n",
    "    row_colors_func=(lambda w: (1 - w.missingness.mean(\"position\"))),\n",
    ")\n",
    "\n",
    "plot_community = plot_generic_clustermap_factory(\n",
    "    matrix_func=lambda w: w.communities.data.T,\n",
    "    row_linkage_func=lambda w: w.genotypes.linkage(dim=\"strain\"),\n",
    "    col_linkage_func=lambda w: w.communities.linkage(dim=\"sample\"),\n",
    "    row_colors_func=(lambda w: (w.communities.sum(\"sample\").pipe(np.sqrt))),\n",
    "    metric=\"cosine\",\n",
    "    scaley=0.20,\n",
    "    scalex=0.14,\n",
    "    dwidth=1.0,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=None,\n",
    "    norm=mpl.colors.PowerNorm(1 / 2),\n",
    "    xticklabels=1,\n",
    "    yticklabels=1,\n",
    ")\n",
    "\n",
    "\n",
    "def plot_loss_history(trace):\n",
    "    trace = np.array(trace)\n",
    "    plt.plot((trace - trace.min()))\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "\n",
    "def nmds_ordination(dmat):\n",
    "    init = MDS(\n",
    "        n_components=2,\n",
    "        max_iter=3000,\n",
    "        eps=1e-9,\n",
    "        random_state=1,\n",
    "        dissimilarity=\"precomputed\",\n",
    "        n_jobs=1,\n",
    "    ).fit_transform(dmat)\n",
    "    nmds = MDS(\n",
    "        n_components=2,\n",
    "        metric=False,\n",
    "        max_iter=3000,\n",
    "        eps=1e-12,\n",
    "        dissimilarity=\"precomputed\",\n",
    "        random_state=1,\n",
    "        n_jobs=1,\n",
    "        n_init=1,\n",
    "    )\n",
    "    ordin = nmds.fit_transform(dmat, init=init)\n",
    "\n",
    "    ordin = pd.DataFrame(\n",
    "        ordin,\n",
    "        index=dmat.index,\n",
    "        columns=[f\"PC{i}\" for i in np.arange(ordin.shape[1]) + 1],\n",
    "    )\n",
    "    return ordin\n",
    "\n",
    "\n",
    "def ordination_plot(\n",
    "    world,\n",
    "    dmat_func,\n",
    "    ordin_func=nmds_ordination,\n",
    "    colors_func=None,\n",
    "    sizes_func=None,\n",
    "    xy=(\"PC1\", \"PC2\"),\n",
    "    ax=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Plot nMDS ordination with markers colored/shaped by metadata features.\"\"\"\n",
    "    x, y = xy\n",
    "    dmat = dmat_func(world)\n",
    "\n",
    "    if colors_func is None:\n",
    "        colors = None\n",
    "    else:\n",
    "        colors = colors_func(world)\n",
    "\n",
    "    if sizes_func is None:\n",
    "        sizes = None\n",
    "    else:\n",
    "        sizes = sizes_func(world)\n",
    "\n",
    "    ordin = ordin_func(dmat)\n",
    "\n",
    "    scatter_kwargs = dict(\n",
    "        c=colors,\n",
    "        cmap=\"viridis\",\n",
    "        s=sizes,\n",
    "        edgecolor=\"k\",\n",
    "        lw=0.2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    scatter_kwargs.update(kwargs)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.scatter(x=x, y=y, data=ordin, **scatter_kwargs)\n",
    "    ax.set_xlabel(f\"{x}\")\n",
    "    ax.set_ylabel(f\"{y}\")\n",
    "    return ax, ordin\n",
    "\n",
    "\n",
    "def plot_metagenotype_frequency_spectrum(\n",
    "    world,\n",
    "    sample_list=None,\n",
    "    show_dominant=False,\n",
    "    axwidth=2,\n",
    "    axheight=1.5,\n",
    "    bins=None,\n",
    "    axs=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if sample_list is None:\n",
    "        sample_list = world.sample.values\n",
    "\n",
    "    hist_kwargs = dict(color=\"black\")\n",
    "    hist_kwargs.update(kwargs)\n",
    "\n",
    "    n = len(sample_list)\n",
    "    if not axs:\n",
    "        fig, axs = plt.subplots(\n",
    "            n, n, figsize=(axwidth * n, axheight * n), sharex=True, sharey=True\n",
    "        )\n",
    "    axs = np.asarray(axs).reshape((n, n))\n",
    "\n",
    "    if bins is None:\n",
    "        bins = np.linspace(0.5, 1.0, num=21)\n",
    "\n",
    "    frequencies = world.metagenotypes.mlift(\"sel\", sample=sample_list).frequencies()\n",
    "    for sample_i, row in zip(sample_list, axs):\n",
    "        for sample_j, ax in zip(sample_list, row):\n",
    "            domfreq_ij = (\n",
    "                frequencies.sel(sample=[sample_i, sample_j])\n",
    "                .mean(\"sample\")\n",
    "                .max(\"allele\")\n",
    "            )\n",
    "            ax.hist(domfreq_ij, bins=bins, **hist_kwargs)\n",
    "\n",
    "    if show_dominant:\n",
    "        max_frac = world.communities.sel(sample=sample_list).max(\"strain\")\n",
    "        max_frac_complement = 1 - max_frac\n",
    "        for i, sample in enumerate(sample_list):\n",
    "            ax = axs[i, i]\n",
    "            ax.axvline(\n",
    "                max_frac.sel(sample=sample),\n",
    "                linestyle=\"--\",\n",
    "                lw=1,\n",
    "                color=\"darkblue\",\n",
    "            )\n",
    "            ax.axvline(\n",
    "                max_frac_complement.sel(sample=sample),\n",
    "                linestyle=\"--\",\n",
    "                lw=1,\n",
    "                color=\"darkred\",\n",
    "            )\n",
    "\n",
    "    for i, sample in enumerate(sample_list):\n",
    "        ax_left = axs[i, 0]\n",
    "        ax_top = axs[0, i]\n",
    "        ax_left.set_ylabel(sample)\n",
    "        ax_top.set_title(sample)\n",
    "\n",
    "    ax.set_xlim(0.5, 1)\n",
    "\n",
    "\n",
    "def plot_metagenotype_frequency_spectrum_comparison(\n",
    "    worlds, sample, alpha=0.5, bins=None, ax=None\n",
    "):\n",
    "    if bins is None:\n",
    "        bins = np.linspace(0.5, 1.0, num=21)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    plot_hist = lambda w, label: ax.hist(\n",
    "        w.metagenotypes.mlift(\"sel\", sample=[sample])\n",
    "        .dominant_allele_fraction()\n",
    "        .values.squeeze(),\n",
    "        bins=bins,\n",
    "        alpha=alpha,\n",
    "        label=label,\n",
    "    )\n",
    "    for k, w in worlds.items():\n",
    "        plot_hist(w, k)\n",
    "    ax.set_title(sample)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_beta_diversity_comparison(\n",
    "    worldA,\n",
    "    worldB,\n",
    "    **kwargs,\n",
    "):\n",
    "    cdmatA = squareform(worldA.communities.pdist(dim=\"sample\"))\n",
    "    cdmatB = squareform(worldB.communities.pdist(dim=\"sample\"))\n",
    "\n",
    "    return sns.jointplot(\"a\", \"b\", data=pd.DataFrame(dict(a=cdmatA, b=cdmatB)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model.py\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "import xarray as xr\n",
    "import sfacts as sf\n",
    "from sfacts.pyro_util import all_torch\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self, generative, dims, description, default_hyperparameters=None):\n",
    "        \"\"\"\n",
    "\n",
    "        *generative* :: Pyro generative model function(shape_dim_0, shape_dim_1, shape_dim_2, ..., **hyper_parameters)\n",
    "        *dims* :: Sequence of names for dim_0, dim_1, dim_2, ...\n",
    "        *description* :: Mapping from model variable to its dims.\n",
    "        *default_hyperparameters* :: Values to use for hyperparameters when not explicitly set.\n",
    "        \"\"\"\n",
    "        if default_hyperparameters is None:\n",
    "            default_hyperparameters = {}\n",
    "\n",
    "        self.generative = generative\n",
    "        self.dims = dims\n",
    "        self.description = description\n",
    "        self.default_hyperparameters = default_hyperparameters\n",
    "\n",
    "    #         _ = self(self._dummy_shape, **all_torch(**self.default_hyperparameters))\n",
    "\n",
    "    #         info(f\"New Structure({self.generative}, {self.default_hyperparameters})\")\n",
    "\n",
    "    def __call__(self, shape, data, hyperparameters, unit):\n",
    "        assert len(shape) == len(self.dims)\n",
    "        conditioned_generative = pyro.condition(self.generative, data)\n",
    "        return conditioned_generative(*shape, **hyperparameters, _unit=unit)\n",
    "\n",
    "    #     def condition(self, **data):\n",
    "    #         new_data = self.data.copy()\n",
    "    #         new_data.update(data)\n",
    "    #         return self.__class__(\n",
    "    #             generative=self.generative,\n",
    "    #             dims=self.dims,\n",
    "    #             description=self.description,\n",
    "    #             default_hyperparameters=self.default_hyperparameters,\n",
    "    #             data=new_data,\n",
    "    #         )\n",
    "\n",
    "    @property\n",
    "    def _dummy_shape(self):\n",
    "        shape = range(1, len(self.dims) + 1)\n",
    "        return shape\n",
    "\n",
    "    def explain_shapes(self, shape=None):\n",
    "        if shape is None:\n",
    "            shape = self._dummy_shape\n",
    "        info(dict(zip(self.dims, shape)))\n",
    "        sf.pyro_util.shape_info(self(shape, **self.default_hyperparameters))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{self.generative.__name__}(\"\n",
    "            # f\"{self.generative}, \"\n",
    "            f\"dims={self.dims}, \"\n",
    "            f\"description={self.description}, \"\n",
    "            f\"default_hyperparameters={self.default_hyperparameters} \"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "\n",
    "# For decorator use.\n",
    "def structure(dims, description, default_hyperparameters=None):\n",
    "    return partial(\n",
    "        Structure,\n",
    "        dims=dims,\n",
    "        description=description,\n",
    "        default_hyperparameters=default_hyperparameters,\n",
    "    )\n",
    "\n",
    "\n",
    "class ParameterizedModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        structure,\n",
    "        coords,\n",
    "        dtype=torch.float32,\n",
    "        device=\"cpu\",\n",
    "        data=None,\n",
    "        hyperparameters=None,\n",
    "    ):\n",
    "        if hyperparameters is None:\n",
    "            hyperparameters = {}\n",
    "\n",
    "        if data is None:\n",
    "            data = {}\n",
    "\n",
    "        self.structure = structure\n",
    "        self.coords = {k: self._coords_or_range(coords[k]) for k in self.structure.dims}\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.hyperparameters = self.structure.default_hyperparameters.copy()\n",
    "        self.hyperparameters.update(hyperparameters)\n",
    "        self.data = data\n",
    "\n",
    "    @property\n",
    "    def sizes(self):\n",
    "        return {k: len(self.coords[k]) for k in self.structure.dims}\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return tuple(self.sizes.values())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}\"\n",
    "            f\"({self.structure}, \"\n",
    "            f\"coords={self.coords}, \"\n",
    "            f\"dtype={self.dtype}, \"\n",
    "            f\"device={self.device}, \"\n",
    "            f\"hyperparameters={self.hyperparameters}, \"\n",
    "            f\"data={self.data})\"\n",
    "        )\n",
    "\n",
    "    def __call__(self):\n",
    "        # Here's where all the action happens.\n",
    "        # All parameters are cast based on dtype and device.\n",
    "        # The model is conditioned on the\n",
    "        # data, and then called with the shape tuple\n",
    "        # and cast hyperparameters.\n",
    "        return self.structure(\n",
    "            self.shape,\n",
    "            data=all_torch(**self.data, dtype=self.dtype, device=self.device),\n",
    "            hyperparameters=all_torch(\n",
    "                **self.hyperparameters, dtype=self.dtype, device=self.device\n",
    "            ),\n",
    "            unit=torch.tensor(1.0, dtype=self.dtype, device=self.device),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _coords_or_range(coords):\n",
    "        if type(coords) == int:\n",
    "            return range(coords)\n",
    "        else:\n",
    "            return coords\n",
    "\n",
    "    def with_hyperparameters(self, **hyperparameters):\n",
    "        new_hyperparameters = self.hyperparameters.copy()\n",
    "        new_hyperparameters.update(hyperparameters)\n",
    "        return self.__class__(\n",
    "            structure=self.structure,\n",
    "            coords=self.coords,\n",
    "            dtype=self.dtype,\n",
    "            device=self.device,\n",
    "            hyperparameters=new_hyperparameters,\n",
    "            data=self.data,\n",
    "        )\n",
    "\n",
    "    def with_amended_coords(self, **coords):\n",
    "        new_coords = self.coords.copy()\n",
    "        new_coords.update(coords)\n",
    "        return self.__class__(\n",
    "            structure=self.structure,\n",
    "            coords=new_coords,\n",
    "            dtype=self.dtype,\n",
    "            device=self.device,\n",
    "            hyperparameters=self.hyperparameters,\n",
    "            data=self.data,\n",
    "        )\n",
    "\n",
    "    def condition(self, **data):\n",
    "        new_data = self.data.copy()\n",
    "        new_data.update(data)\n",
    "        return self.__class__(\n",
    "            structure=self.structure,\n",
    "            coords=self.coords,\n",
    "            dtype=self.dtype,\n",
    "            device=self.device,\n",
    "            hyperparameters=self.hyperparameters,\n",
    "            data=new_data,\n",
    "        )\n",
    "\n",
    "    def format_world(self, data):\n",
    "        out = {}\n",
    "        for k in self.structure.description:\n",
    "            out[k] = xr.DataArray(\n",
    "                data[k],\n",
    "                dims=self.structure.description[k],\n",
    "                coords={dim: self.coords[dim] for dim in self.structure.description[k]},\n",
    "            )\n",
    "        return sf.data.World(xr.Dataset(out))\n",
    "\n",
    "    def simulate(self, n=1, seed=None):\n",
    "        sf.pyro_util.set_random_seed(seed)\n",
    "        obs = pyro.infer.Predictive(self, num_samples=n)()\n",
    "        obs = {k: obs[k].detach().cpu().numpy().squeeze() for k in obs.keys()}\n",
    "        return obs\n",
    "\n",
    "    def simulate_world(self, seed=None):\n",
    "        return self.format_world(self.simulate(n=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model_zoo/__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model_zoo/__init__.py\n",
    "from sfacts.model_zoo.full_metagenotype import full_metagenotype_model_structure\n",
    "from sfacts.model_zoo.full_metagenotype_dirichlet_rho import full_metagenotype_dirichlet_rho_model_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_zoo/components.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model_zoo/components.py\n",
    "import sfacts as sf\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "\n",
    "\n",
    "SHARED_DIMS = (\"sample\", \"position\", \"strain\", \"allele\")\n",
    "SHARED_DESCRIPTIONS = dict(\n",
    "    gamma=(\"strain\", \"position\"),\n",
    "    delta=(\"strain\", \"position\"),\n",
    "    rho=(\"strain\",),\n",
    "    pi=(\"sample\", \"strain\"),\n",
    "    epsilon=(\"sample\",),\n",
    "    m_hyper_r=(\"sample\",),\n",
    "    m_hyper_r_mean=(),\n",
    "    m_hyper_r_scale=(),\n",
    "    mu=(\"sample\",),\n",
    "    nu=(\"sample\", \"position\"),\n",
    "    p_noerr=(\"sample\", \"position\"),\n",
    "    p=(\"sample\", \"position\"),\n",
    "    alpha_hyper_mean=(),\n",
    "    alpha=(\"sample\",),\n",
    "    m=(\"sample\", \"position\"),\n",
    "    y=(\"sample\", \"position\"),\n",
    "    genotypes=(\"strain\", \"position\"),\n",
    "    missingness=(\"strain\", \"position\"),\n",
    "    communities=(\"sample\", \"strain\"),\n",
    "    metagenotypes=(\"sample\", \"position\", \"allele\"),\n",
    ")\n",
    "\n",
    "\n",
    "def _mapping_subset(mapping, keys):\n",
    "    return {k: mapping[k] for k in keys}\n",
    "\n",
    "\n",
    "def stickbreaking_betas_to_probs(beta):\n",
    "    beta1m_cumprod = (1 - beta).cumprod(-1)\n",
    "    return torch_pad(beta, (0, 1), value=1) * torch_pad(beta1m_cumprod, (1, 0), value=1)\n",
    "\n",
    "\n",
    "def NegativeBinomialReparam(mu, r):\n",
    "    p = 1.0 / ((r / mu) + 1.0)\n",
    "    logits = torch.logit(p)\n",
    "    #     p = torch.clamp(p, eps, 1 - eps)\n",
    "    return dist.NegativeBinomial(\n",
    "        total_count=r,\n",
    "        logits=logits,\n",
    "    )\n",
    "\n",
    "\n",
    "def unit_interval_power_transformation(p, alpha, beta):\n",
    "    log_p = torch.log(p)\n",
    "    log_q = torch.log1p(-p)\n",
    "    log_p_raised = log_p * alpha\n",
    "    log_q_raised = log_q * beta\n",
    "    return torch.exp(\n",
    "        log_p_raised - torch.logsumexp(torch.stack([log_p_raised, log_q_raised]), dim=0)\n",
    "    )\n",
    "\n",
    "\n",
    "# def _pp_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "#     # Genotypes\n",
    "#     #     delta_hyper_p = pyro.sample('delta_hyper_p', dist.Beta(1., 1.))\n",
    "#     with pyro.plate(\"position\", g, dim=-1):\n",
    "#         with pyro.plate(\"strain\", s, dim=-2):\n",
    "#             _gamma = pyro.sample(\"_gamma\", dist.Beta(1.0, 1.0))\n",
    "#             gamma = pyro.deterministic(\n",
    "#                 \"gamma\",\n",
    "#                 unit_interval_power_transformation(\n",
    "#                     _gamma, 1 / gamma_hyper, 1 / gamma_hyper\n",
    "#                 ),\n",
    "#             )\n",
    "#             #                 Position presence/absence\n",
    "#             _delta = pyro.sample(\"_delta\", dist.Beta(1.0, 1.0))\n",
    "#             delta = pyro.deterministic(\n",
    "#                 \"delta\",\n",
    "#                 unit_interval_power_transformation(\n",
    "#                     _delta,\n",
    "#                     2 * (1 - delta_hyper_r) / delta_hyper_temp,\n",
    "#                     2 * delta_hyper_r / delta_hyper_temp,\n",
    "#                 ),\n",
    "#             )\n",
    "#\n",
    "#     #                 delta = pyro.sample(\n",
    "#     #                     'delta',\n",
    "#     #                     dist.RelaxedBernoulli(\n",
    "#     #                         temperature=delta_hyper_temp, probs=delta_hyper_p\n",
    "#     #                     ),\n",
    "#     #                 )\n",
    "#\n",
    "#     # These deterministics are accessed by PointMixin class properties.\n",
    "#     pyro.deterministic(\"genotypes\", gamma)\n",
    "#     pyro.deterministic(\"missingness\", delta)\n",
    "#     return gamma, delta\n",
    "#\n",
    "#\n",
    "# def _gsm_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "#     with pyro.plate(\"position\", g, dim=-1):\n",
    "#         with pyro.plate(\"strain\", s, dim=-2):\n",
    "#             gamma = pyro.sample(\n",
    "#                 \"gamma\",\n",
    "#                 dist.RelaxedBernoulli(\n",
    "#                     temperature=gamma_hyper,\n",
    "#                     probs=0.5,\n",
    "#                 ),\n",
    "#             )\n",
    "#             delta = pyro.sample(\n",
    "#                 \"delta\",\n",
    "#                 dist.RelaxedBernoulli(\n",
    "#                     temperature=delta_hyper_temp, probs=delta_hyper_r\n",
    "#                 ),\n",
    "#             )\n",
    "#     pyro.deterministic(\"genotypes\", gamma)\n",
    "#     pyro.deterministic(\"missingness\", delta)\n",
    "#     return gamma, delta\n",
    "#\n",
    "#\n",
    "# def _beta_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "#     with pyro.plate(\"position\", g, dim=-1):\n",
    "#         with pyro.plate(\"strain\", s, dim=-2):\n",
    "#             gamma = pyro.sample(\n",
    "#                 \"gamma\",\n",
    "#                 dist.Beta(\n",
    "#                     gamma_hyper,\n",
    "#                     gamma_hyper,\n",
    "#                 ),\n",
    "#             )\n",
    "#             delta = pyro.sample(\n",
    "#                 \"delta\",\n",
    "#                 dist.RelaxedBernoulli(\n",
    "#                     delta_hyper_r * delta_hyper_temp,\n",
    "#                     (1 - delta_hyper_r) * delta_hyper_temp,\n",
    "#                 ),\n",
    "#             )\n",
    "#     pyro.deterministic(\"genotypes\", gamma)\n",
    "#     pyro.deterministic(\"missingness\", delta)\n",
    "#     return gamma, delta\n",
    "#\n",
    "#\n",
    "# def _hybrid_gamma_delta_module(s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp):\n",
    "#     with pyro.plate(\"position\", g, dim=-1):\n",
    "#         with pyro.plate(\"strain\", s, dim=-2):\n",
    "#             _gamma = pyro.sample(\"_gamma\", dist.Beta(1.0, 1.0))\n",
    "#             gamma = pyro.deterministic(\n",
    "#                 \"gamma\",\n",
    "#                 unit_interval_power_transformation(\n",
    "#                     _gamma, 1 / gamma_hyper, 1 / gamma_hyper\n",
    "#                 ),\n",
    "#             )\n",
    "#             #                 Position presence/absence\n",
    "#             delta = pyro.sample(\n",
    "#                 \"delta\",\n",
    "#                 dist.RelaxedBernoulli(\n",
    "#                     temperature=delta_hyper_temp, probs=delta_hyper_r\n",
    "#                 ),\n",
    "#             )\n",
    "#     pyro.deterministic(\"genotypes\", gamma)\n",
    "#     pyro.deterministic(\"missingness\", delta)\n",
    "#     return gamma, delta\n",
    "#\n",
    "#\n",
    "# def _dp_rho_module(s, rho_hyper):\n",
    "#     #         # TODO: Will torch.ones(s) fail when I try to run this on the GPU because it's, by default on the CPU?\n",
    "#     #         rho = pyro.sample(\n",
    "#     #             \"rho\", dist.Dirichlet(rho_hyper * torch.ones(s))\n",
    "#     #         )\n",
    "#     # Meta-community composition\n",
    "#     rho_betas = pyro.sample(\n",
    "#         \"rho_betas\", dist.Beta(1.0, rho_hyper).expand([s - 1]).to_event()\n",
    "#     )\n",
    "#     rho = pyro.deterministic(\"rho\", stickbreaking_betas_to_probs(rho_betas))\n",
    "#     pyro.deterministic(\"metacommunity\", rho)\n",
    "#     return rho\n",
    "#\n",
    "#\n",
    "# def _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "#     n,\n",
    "#     pi_hyper,\n",
    "#     rho,\n",
    "#     epsilon_hyper_alpha,\n",
    "#     epsilon_hyper_beta,\n",
    "#     alpha_hyper_mean,\n",
    "#     alpha_hyper_scale,\n",
    "#     mu_hyper_mean,\n",
    "#     mu_hyper_scale,\n",
    "# ):\n",
    "#     with pyro.plate(\"sample\", n, dim=-1):\n",
    "#         # Community composition\n",
    "#         pi = pyro.sample(\"pi\", dist.Dirichlet(pi_hyper * rho, validate_args=False))\n",
    "#         # Sequencing error\n",
    "#         epsilon = pyro.sample(\n",
    "#             \"epsilon\", dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta)\n",
    "#         ).unsqueeze(-1)\n",
    "#         alpha = pyro.sample(\n",
    "#             \"alpha\",\n",
    "#             dist.LogNormal(loc=torch.log(alpha_hyper_mean), scale=alpha_hyper_scale),\n",
    "#         ).unsqueeze(-1)\n",
    "#         # Sample coverage\n",
    "#         mu = pyro.sample(\n",
    "#             \"mu\", dist.LogNormal(loc=torch.log(mu_hyper_mean), scale=mu_hyper_scale)\n",
    "#         )\n",
    "#     pyro.deterministic(\"communities\", pi)\n",
    "#     return pi, epsilon, alpha, mu\n",
    "#\n",
    "#\n",
    "# def _dirichlet_pi_epsilon_alpha_r_mu_module(\n",
    "#     n,\n",
    "#     pi_hyper,\n",
    "#     rho,\n",
    "#     epsilon_hyper_alpha,\n",
    "#     epsilon_hyper_beta,\n",
    "#     alpha_hyper_mean,\n",
    "#     alpha_hyper_scale,\n",
    "#     mu_hyper_mean,\n",
    "#     mu_hyper_scale,\n",
    "# ):\n",
    "#     with pyro.plate(\"sample\", n, dim=-1):\n",
    "#         # Community composition\n",
    "#         pi = pyro.sample(\"pi\", dist.Dirichlet(pi_hyper * rho, validate_args=False))\n",
    "#         # Sequencing error\n",
    "#         epsilon = pyro.sample(\n",
    "#             \"epsilon\", dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta)\n",
    "#         ).unsqueeze(-1)\n",
    "#         alpha = pyro.sample(\n",
    "#             \"alpha\",\n",
    "#             dist.LogNormal(loc=torch.log(alpha_hyper_mean), scale=alpha_hyper_scale),\n",
    "#         ).unsqueeze(-1)\n",
    "#         # Sample coverage\n",
    "#         mu = pyro.sample(\n",
    "#             \"mu\", dist.LogNormal(loc=torch.log(mu_hyper_mean), scale=mu_hyper_scale)\n",
    "#         )\n",
    "#     pyro.deterministic(\"communities\", pi)\n",
    "#     return pi, epsilon, alpha, mu\n",
    "#\n",
    "#\n",
    "# def _lognormal_alpha_hyper_mean_module(alpha_hyper_hyper_mean, alpha_hyper_hyper_scale):\n",
    "#     alpha_hyper_mean = pyro.sample(\n",
    "#         \"alpha_hyper_mean\",\n",
    "#         dist.LogNormal(\n",
    "#             loc=torch.log(alpha_hyper_hyper_mean), scale=alpha_hyper_hyper_scale\n",
    "#         ),\n",
    "#     )\n",
    "#     return alpha_hyper_mean\n",
    "#\n",
    "#\n",
    "# def _m_hyper_r_module(n, m_hyper_r_scale):\n",
    "#     m_hyper_r_mean = pyro.sample(\"m_hyper_r_mean\", dist.LogNormal(loc=0.0, scale=10.0))\n",
    "#     m_hyper_r = pyro.sample(\n",
    "#         \"m_hyper_r\",\n",
    "#         dist.LogNormal(loc=torch.log(m_hyper_r_mean), scale=m_hyper_r_scale)\n",
    "#         .expand([n, 1])\n",
    "#         .to_event(),\n",
    "#     )\n",
    "#     return m_hyper_r\n",
    "#\n",
    "#\n",
    "# def _betabinomial_observation_module(pi, gamma, delta, m_hyper_r, mu, epsilon, alpha):\n",
    "#     # Depth at each position\n",
    "#     nu = pyro.deterministic(\"nu\", pi @ delta)\n",
    "#     # TODO: Consider using pyro.distributions.GammaPoisson parameterization?\n",
    "#     m = pyro.sample(\n",
    "#         \"m\",\n",
    "#         NegativeBinomialReparam(nu * mu.reshape((-1, 1)), m_hyper_r).to_event(),\n",
    "#     )\n",
    "#\n",
    "#     # Expected fractions of each allele at each position\n",
    "#     p_noerr = pyro.deterministic(\"p_noerr\", pi @ (gamma * delta) / nu)\n",
    "#     p = pyro.deterministic(\n",
    "#         \"p\", (1 - epsilon / 2) * (p_noerr) + (epsilon / 2) * (1 - p_noerr)\n",
    "#     )\n",
    "#\n",
    "#     # Observation\n",
    "#     y = pyro.sample(\n",
    "#         \"y\",\n",
    "#         dist.BetaBinomial(\n",
    "#             concentration1=alpha * p,\n",
    "#             concentration0=alpha * (1 - p),\n",
    "#             total_count=m,\n",
    "#             #             validate_args=False,\n",
    "#         ).to_event(),\n",
    "#     )\n",
    "#     # TODO: Check that dim=0 works?\n",
    "#     metagenotypes = pyro.deterministic(\"metagenotypes\", torch.stack([y, m - y], dim=-1))\n",
    "#\n",
    "#\n",
    "# @sf.model.structure(\n",
    "#     dims=SHARED_DIMS,\n",
    "#     description=_mapping_subset(\n",
    "#         SHARED_DESCRIPTIONS,\n",
    "#         [\n",
    "#             \"rho\",\n",
    "#             \"epsilon\",\n",
    "#             \"m_hyper_r\",\n",
    "#             \"mu\",\n",
    "#             \"nu\",\n",
    "#             \"p_noerr\",\n",
    "#             \"p\",\n",
    "#             \"m\",\n",
    "#             \"y\",\n",
    "#             \"alpha_hyper_mean\",\n",
    "#             \"alpha\",\n",
    "#             \"genotypes\",\n",
    "#             \"missingness\",\n",
    "#             \"communities\",\n",
    "#             \"metagenotypes\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     default_hyperparameters=dict(\n",
    "#         gamma_hyper=0.01,\n",
    "#         delta_hyper_temp=0.01,\n",
    "#         delta_hyper_r=0.9,\n",
    "#         rho_hyper=5.0,\n",
    "#         pi_hyper=0.2,\n",
    "#         epsilon_hyper_alpha=1.5,\n",
    "#         epsilon_hyper_beta=1.5 / 0.01,\n",
    "#         mu_hyper_mean=1.0,\n",
    "#         mu_hyper_scale=10.0,\n",
    "#         #         m_hyper_r_mu=1.,\n",
    "#         m_hyper_r_scale=1.0,\n",
    "#         alpha_hyper_hyper_mean=100.0,\n",
    "#         alpha_hyper_hyper_scale=1.0,\n",
    "#         alpha_hyper_scale=0.5,\n",
    "#     ),\n",
    "# )\n",
    "# def pp_fuzzy_missing_dp_betabinomial_metagenotype(\n",
    "#     n,\n",
    "#     g,\n",
    "#     s,\n",
    "#     a,\n",
    "#     gamma_hyper,\n",
    "#     delta_hyper_r,\n",
    "#     delta_hyper_temp,\n",
    "#     rho_hyper,  # =1.0,\n",
    "#     pi_hyper,  # =1.0,\n",
    "#     alpha_hyper_hyper_mean,  # =100.0,\n",
    "#     alpha_hyper_hyper_scale,  # =1.0,\n",
    "#     alpha_hyper_scale,  # =0.5,\n",
    "#     epsilon_hyper_alpha,  # =1.5,\n",
    "#     epsilon_hyper_beta,  # =1.5 / 0.01,\n",
    "#     mu_hyper_mean,  # =1.0,\n",
    "#     mu_hyper_scale,  # =1.0,\n",
    "#     #         m_hyper_r_mu,\n",
    "#     m_hyper_r_scale,\n",
    "# ):\n",
    "#     gamma, delta = _pp_gamma_delta_module(\n",
    "#         s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp\n",
    "#     )\n",
    "#     rho = _dp_rho_module(s, rho_hyper)\n",
    "#     alpha_hyper_mean = _lognormal_alpha_hyper_mean_module(\n",
    "#         alpha_hyper_hyper_mean, alpha_hyper_hyper_scale\n",
    "#     )\n",
    "#     pi, epsilon, alpha, mu = _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "#         n,\n",
    "#         pi_hyper,\n",
    "#         rho,\n",
    "#         epsilon_hyper_alpha,\n",
    "#         epsilon_hyper_beta,\n",
    "#         alpha_hyper_mean,\n",
    "#         alpha_hyper_scale,\n",
    "#         mu_hyper_mean,\n",
    "#         mu_hyper_scale,\n",
    "#     )\n",
    "#     m_hyper_r = _m_hyper_r_module(n, m_hyper_r_scale)\n",
    "#     _betabinomial_observation_module(pi, gamma, delta, m_hyper_r, mu, epsilon, alpha)\n",
    "#\n",
    "#\n",
    "# @sf.model.structure(\n",
    "#     dims=SHARED_DIMS,\n",
    "#     description=_mapping_subset(\n",
    "#         SHARED_DESCRIPTIONS,\n",
    "#         [\n",
    "#             \"rho\",\n",
    "#             \"epsilon\",\n",
    "#             \"m_hyper_r\",\n",
    "#             \"mu\",\n",
    "#             \"nu\",\n",
    "#             \"p_noerr\",\n",
    "#             \"p\",\n",
    "#             \"m\",\n",
    "#             \"y\",\n",
    "#             \"alpha_hyper_mean\",\n",
    "#             \"alpha\",\n",
    "#             \"genotypes\",\n",
    "#             \"missingness\",\n",
    "#             \"communities\",\n",
    "#             \"metagenotypes\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     default_hyperparameters=dict(\n",
    "#         gamma_hyper=0.01,\n",
    "#         delta_hyper_temp=0.01,\n",
    "#         delta_hyper_r=0.9,\n",
    "#         rho_hyper=5.0,\n",
    "#         pi_hyper=0.2,\n",
    "#         epsilon_hyper_alpha=1.5,\n",
    "#         epsilon_hyper_beta=1.5 / 0.01,\n",
    "#         mu_hyper_mean=1.0,\n",
    "#         mu_hyper_scale=10.0,\n",
    "#         #         m_hyper_r_mu=1.,\n",
    "#         m_hyper_r_scale=1.0,\n",
    "#         alpha_hyper_hyper_mean=100.0,\n",
    "#         alpha_hyper_hyper_scale=1.0,\n",
    "#         alpha_hyper_scale=0.5,\n",
    "#     ),\n",
    "# )\n",
    "# def gsm_fuzzy_missing_dp_betabinomial_metagenotype(\n",
    "#     n,\n",
    "#     g,\n",
    "#     s,\n",
    "#     a,\n",
    "#     gamma_hyper,\n",
    "#     delta_hyper_r,\n",
    "#     delta_hyper_temp,\n",
    "#     rho_hyper,  # =1.0,\n",
    "#     pi_hyper,  # =1.0,\n",
    "#     alpha_hyper_hyper_mean,  # =100.0,\n",
    "#     alpha_hyper_hyper_scale,  # =1.0,\n",
    "#     alpha_hyper_scale,  # =0.5,\n",
    "#     epsilon_hyper_alpha,  # =1.5,\n",
    "#     epsilon_hyper_beta,  # =1.5 / 0.01,\n",
    "#     mu_hyper_mean,  # =1.0,\n",
    "#     mu_hyper_scale,  # =1.0,\n",
    "#     #         m_hyper_r_mu,\n",
    "#     m_hyper_r_scale,\n",
    "# ):\n",
    "#     gamma, delta = _gsm_gamma_delta_module(\n",
    "#         s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp\n",
    "#     )\n",
    "#     rho = _dp_rho_module(s, rho_hyper)\n",
    "#     alpha_hyper_mean = _lognormal_alpha_hyper_mean_module(\n",
    "#         alpha_hyper_hyper_mean, alpha_hyper_hyper_scale\n",
    "#     )\n",
    "#     pi, epsilon, alpha, mu = _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "#         n,\n",
    "#         pi_hyper,\n",
    "#         rho,\n",
    "#         epsilon_hyper_alpha,\n",
    "#         epsilon_hyper_beta,\n",
    "#         alpha_hyper_mean,\n",
    "#         alpha_hyper_scale,\n",
    "#         mu_hyper_mean,\n",
    "#         mu_hyper_scale,\n",
    "#     )\n",
    "#     m_hyper_r = _m_hyper_r_module(n, m_hyper_r_scale)\n",
    "#     _betabinomial_observation_module(pi, gamma, delta, m_hyper_r, mu, epsilon, alpha)\n",
    "#\n",
    "#\n",
    "# @sf.model.structure(\n",
    "#     dims=SHARED_DIMS,\n",
    "#     description=_mapping_subset(\n",
    "#         SHARED_DESCRIPTIONS,\n",
    "#         [\n",
    "#             \"rho\",\n",
    "#             \"epsilon\",\n",
    "#             \"m_hyper_r\",\n",
    "#             \"mu\",\n",
    "#             \"nu\",\n",
    "#             \"p_noerr\",\n",
    "#             \"p\",\n",
    "#             \"m\",\n",
    "#             \"y\",\n",
    "#             \"alpha_hyper_mean\",\n",
    "#             \"alpha\",\n",
    "#             \"genotypes\",\n",
    "#             \"missingness\",\n",
    "#             \"communities\",\n",
    "#             \"metagenotypes\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     default_hyperparameters=dict(\n",
    "#         gamma_hyper=0.01,\n",
    "#         delta_hyper_temp=0.01,\n",
    "#         delta_hyper_r=0.9,\n",
    "#         rho_hyper=5.0,\n",
    "#         pi_hyper=0.2,\n",
    "#         epsilon_hyper_alpha=1.5,\n",
    "#         epsilon_hyper_beta=1.5 / 0.01,\n",
    "#         mu_hyper_mean=1.0,\n",
    "#         mu_hyper_scale=10.0,\n",
    "#         #         m_hyper_r_mu=1.,\n",
    "#         m_hyper_r_scale=1.0,\n",
    "#         alpha_hyper_hyper_mean=100.0,\n",
    "#         alpha_hyper_hyper_scale=1.0,\n",
    "#         alpha_hyper_scale=0.5,\n",
    "#     ),\n",
    "# )\n",
    "# def hybrid_fuzzy_missing_dp_betabinomial_metagenotype(\n",
    "#     n,\n",
    "#     g,\n",
    "#     s,\n",
    "#     a,\n",
    "#     gamma_hyper,\n",
    "#     delta_hyper_r,\n",
    "#     delta_hyper_temp,\n",
    "#     rho_hyper,  # =1.0,\n",
    "#     pi_hyper,  # =1.0,\n",
    "#     alpha_hyper_hyper_mean,  # =100.0,\n",
    "#     alpha_hyper_hyper_scale,  # =1.0,\n",
    "#     alpha_hyper_scale,  # =0.5,\n",
    "#     epsilon_hyper_alpha,  # =1.5,\n",
    "#     epsilon_hyper_beta,  # =1.5 / 0.01,\n",
    "#     mu_hyper_mean,  # =1.0,\n",
    "#     mu_hyper_scale,  # =1.0,\n",
    "#     #         m_hyper_r_mu,\n",
    "#     m_hyper_r_scale,\n",
    "# ):\n",
    "#     gamma, delta = _hybrid_gamma_delta_module(\n",
    "#         s, g, gamma_hyper, delta_hyper_r, delta_hyper_temp\n",
    "#     )\n",
    "#     rho = _dp_rho_module(s, rho_hyper)\n",
    "#     alpha_hyper_mean = _lognormal_alpha_hyper_mean_module(\n",
    "#         alpha_hyper_hyper_mean, alpha_hyper_hyper_scale\n",
    "#     )\n",
    "#     pi, epsilon, alpha, mu = _dirichlet_pi_epsilon_alpha_mu_module(\n",
    "#         n,\n",
    "#         pi_hyper,\n",
    "#         rho,\n",
    "#         epsilon_hyper_alpha,\n",
    "#         epsilon_hyper_beta,\n",
    "#         alpha_hyper_mean,\n",
    "#         alpha_hyper_scale,\n",
    "#         mu_hyper_mean,\n",
    "#         mu_hyper_scale,\n",
    "#     )\n",
    "#     m_hyper_r = _m_hyper_r_module(n, m_hyper_r_scale)\n",
    "#     _betabinomial_observation_module(pi, gamma, delta, m_hyper_r, mu, epsilon, alpha)\n",
    "#\n",
    "#\n",
    "# @sf.model.structure(\n",
    "#     dims=SHARED_DIMS,\n",
    "#     description=_mapping_subset(\n",
    "#         SHARED_DESCRIPTIONS,\n",
    "#         [\"m\", \"y\", \"genotypes\", \"rho\", \"communities\", \"metagenotypes\"],\n",
    "#     ),\n",
    "#     default_hyperparameters=dict(\n",
    "#         gamma_hyper=0.01,\n",
    "#         rho_hyper=0.01,\n",
    "#         pi_hyper=0.2,\n",
    "#     ),\n",
    "# )\n",
    "# def simple_metagenotype(\n",
    "#     n,\n",
    "#     g,\n",
    "#     s,\n",
    "#     a,\n",
    "#     gamma_hyper,\n",
    "#     rho_hyper,\n",
    "#     pi_hyper,\n",
    "# ):\n",
    "#     with pyro.plate(\"position\", g, dim=-1):\n",
    "#         with pyro.plate(\"strain\", s, dim=-2):\n",
    "#             gamma = pyro.sample(\"gamma\", dist.Beta(gamma_hyper, gamma_hyper))\n",
    "#     pyro.deterministic(\"genotypes\", gamma)\n",
    "#\n",
    "#     # Meta-community composition\n",
    "#     rho_betas = pyro.sample(\n",
    "#         \"rho_betas\", dist.Beta(1.0, rho_hyper).expand([s - 1]).to_event()\n",
    "#     )\n",
    "#     rho = pyro.deterministic(\"rho\", stickbreaking_betas_to_probs(rho_betas))\n",
    "#\n",
    "#     with pyro.plate(\"sample\", n, dim=-1):\n",
    "#         # Community composition\n",
    "#         pi = pyro.sample(\n",
    "#             \"pi\",\n",
    "#             dist.Dirichlet(\n",
    "#                 pi_hyper * rho,\n",
    "#                 validate_args=False,\n",
    "#             ),\n",
    "#         )\n",
    "#     pyro.deterministic(\"communities\", pi)\n",
    "#\n",
    "#     # Depth at each position\n",
    "#     m = pyro.sample(\n",
    "#         \"m\",\n",
    "#         NegativeBinomialReparam(\n",
    "#             torch.tensor(100),\n",
    "#             torch.tensor(0.1),\n",
    "#         )\n",
    "#         .expand([n, g])\n",
    "#         .to_event(),\n",
    "#     )\n",
    "#\n",
    "#     # Expected fractions of each allele at each position\n",
    "#     p = pyro.deterministic(\"p\", pi @ gamma)\n",
    "#\n",
    "#     # Observation\n",
    "#     y = pyro.sample(\n",
    "#         \"y\",\n",
    "#         dist.Binomial(\n",
    "#             probs=p,\n",
    "#             total_count=m,\n",
    "#             validate_args=False,\n",
    "#         ).to_event(),\n",
    "#     )\n",
    "#     metagenotypes = pyro.deterministic(\"metagenotypes\", torch.stack([y, m - y], dim=-1))\n",
    "#\n",
    "#\n",
    "# @sf.model.structure(\n",
    "#     dims=SHARED_DIMS,\n",
    "#     description=_mapping_subset(\n",
    "#         SHARED_DESCRIPTIONS,\n",
    "#         [\"m\", \"y\", \"genotypes\", \"rho\", \"communities\", \"metagenotypes\"],\n",
    "#     ),\n",
    "#     default_hyperparameters=dict(\n",
    "#         gamma_hyper=0.01,\n",
    "#         rho_hyper=0.01,\n",
    "#         pi_hyper=0.2,\n",
    "#     ),\n",
    "# )\n",
    "# def simple_metagenotype2(\n",
    "#     n,\n",
    "#     g,\n",
    "#     s,\n",
    "#     a,\n",
    "#     gamma_hyper,\n",
    "#     rho_hyper,\n",
    "#     pi_hyper,\n",
    "# ):\n",
    "#     with pyro.plate(\"position\", g, dim=-1):\n",
    "#         with pyro.plate(\"strain\", s, dim=-2):\n",
    "#             _gamma = pyro.sample(\"_gamma\", dist.Beta(1.0, 1.0))\n",
    "#             gamma = pyro.deterministic(\n",
    "#                 \"gamma\",\n",
    "#                 unit_interval_power_transformation(\n",
    "#                     _gamma, 1 / gamma_hyper, 1 / gamma_hyper\n",
    "#                 ),\n",
    "#             )\n",
    "#     pyro.deterministic(\"genotypes\", gamma)\n",
    "#\n",
    "#     # Meta-community composition\n",
    "#     rho_betas = pyro.sample(\n",
    "#         \"rho_betas\", dist.Beta(1.0, rho_hyper).expand([s - 1]).to_event()\n",
    "#     )\n",
    "#     rho = pyro.deterministic(\"rho\", stickbreaking_betas_to_probs(rho_betas))\n",
    "#\n",
    "#     with pyro.plate(\"sample\", n, dim=-1):\n",
    "#         # Community composition\n",
    "#         pi = pyro.sample(\n",
    "#             \"pi\",\n",
    "#             dist.Dirichlet(\n",
    "#                 pi_hyper * rho,\n",
    "#                 validate_args=False,\n",
    "#             ),\n",
    "#         )\n",
    "#     pyro.deterministic(\"communities\", pi)\n",
    "#\n",
    "#     # Depth at each position\n",
    "#     m = pyro.sample(\n",
    "#         \"m\",\n",
    "#         NegativeBinomialReparam(\n",
    "#             torch.tensor(100),\n",
    "#             torch.tensor(0.1),\n",
    "#         )\n",
    "#         .expand([n, g])\n",
    "#         .to_event(),\n",
    "#     )\n",
    "#\n",
    "#     # Expected fractions of each allele at each position\n",
    "#     p = pyro.deterministic(\"p\", pi @ gamma)\n",
    "#\n",
    "#     # Observation\n",
    "#     y = pyro.sample(\n",
    "#         \"y\",\n",
    "#         dist.Binomial(\n",
    "#             probs=p,\n",
    "#             total_count=m,\n",
    "#         ).to_event(),\n",
    "#     )\n",
    "#     metagenotypes = pyro.deterministic(\"metagenotypes\", torch.stack([y, m - y], dim=-1))\n",
    "#\n",
    "#\n",
    "# @sf.model.structure(\n",
    "#     dims=SHARED_DIMS,\n",
    "#     description=_mapping_subset(\n",
    "#         SHARED_DESCRIPTIONS,\n",
    "#         [\"m\", \"y\", \"epsilon\", \"genotypes\", \"rho\", \"communities\", \"metagenotypes\"],\n",
    "#     ),\n",
    "#     default_hyperparameters=dict(\n",
    "#         gamma_hyper=0.01,\n",
    "#         rho_hyper=0.01,\n",
    "#         pi_hyper=0.2,\n",
    "#         epsilon_hyper_alpha=1.5,\n",
    "#         epsilon_hyper_beta=1.5 / 0.01,\n",
    "#     ),\n",
    "# )\n",
    "# def simple_metagenotype_plus_error(\n",
    "#     n,\n",
    "#     g,\n",
    "#     s,\n",
    "#     a,\n",
    "#     gamma_hyper,\n",
    "#     rho_hyper,\n",
    "#     pi_hyper,\n",
    "#     epsilon_hyper_alpha,\n",
    "#     epsilon_hyper_beta,\n",
    "# ):\n",
    "#     with pyro.plate(\"position\", g, dim=-1):\n",
    "#         with pyro.plate(\"strain\", s, dim=-2):\n",
    "#             gamma = pyro.sample(\"gamma\", dist.Beta(gamma_hyper, gamma_hyper))\n",
    "#     pyro.deterministic(\"genotypes\", gamma)\n",
    "#\n",
    "#     # Meta-community composition\n",
    "#     rho_betas = pyro.sample(\n",
    "#         \"rho_betas\", dist.Beta(1.0, rho_hyper).expand([s - 1]).to_event()\n",
    "#     )\n",
    "#     rho = pyro.deterministic(\"rho\", stickbreaking_betas_to_probs(rho_betas))\n",
    "#\n",
    "#     with pyro.plate(\"sample\", n, dim=-1):\n",
    "#         # Community composition\n",
    "#         pi = pyro.sample(\n",
    "#             \"pi\",\n",
    "#             dist.Dirichlet(\n",
    "#                 pi_hyper * rho,\n",
    "#                 validate_args=False,\n",
    "#             ),\n",
    "#         )\n",
    "#         epsilon = pyro.sample(\n",
    "#             \"epsilon\", dist.Beta(epsilon_hyper_alpha, epsilon_hyper_beta)\n",
    "#         ).unsqueeze(-1)\n",
    "#     pyro.deterministic(\"communities\", pi)\n",
    "#\n",
    "#     # Depth at each position\n",
    "#     m = pyro.sample(\n",
    "#         \"m\",\n",
    "#         NegativeBinomialReparam(\n",
    "#             torch.tensor(100),\n",
    "#             torch.tensor(0.1),\n",
    "#         )\n",
    "#         .expand([n, g])\n",
    "#         .to_event(),\n",
    "#     )\n",
    "#\n",
    "#     # Expected fractions of each allele at each position\n",
    "#     p = pyro.deterministic(\"p\", pi @ gamma)\n",
    "#\n",
    "#     # Observation\n",
    "#     y = pyro.sample(\n",
    "#         \"y\",\n",
    "#         dist.Binomial(\n",
    "#             probs=p,\n",
    "#             total_count=m,\n",
    "#             validate_args=False,\n",
    "#         ).to_event(),\n",
    "#     )\n",
    "#     metagenotypes = pyro.deterministic(\"metagenotypes\", torch.stack([y, m - y], dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_zoo/full_metagenotype.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model_zoo/full_metagenotype.py\n",
    "import sfacts.model\n",
    "from sfacts.model_zoo.components import (\n",
    "    _mapping_subset,\n",
    "    unit_interval_power_transformation,\n",
    "    stickbreaking_betas_to_probs,\n",
    "    NegativeBinomialReparam,\n",
    "    SHARED_DESCRIPTIONS,\n",
    "    SHARED_DIMS,\n",
    ")\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "\n",
    "@sfacts.model.structure(\n",
    "    dims=SHARED_DIMS,\n",
    "    description=_mapping_subset(\n",
    "        SHARED_DESCRIPTIONS,\n",
    "        [\n",
    "            \"rho\",\n",
    "            \"epsilon\",\n",
    "            \"m_hyper_r_mean\",\n",
    "            \"m_hyper_r_scale\",\n",
    "            \"m_hyper_r\",\n",
    "            \"mu\",\n",
    "            \"nu\",\n",
    "            \"p_noerr\",\n",
    "            \"p\",\n",
    "            \"m\",\n",
    "            \"y\",\n",
    "            \"alpha_hyper_mean\",\n",
    "            \"alpha\",\n",
    "            \"genotypes\",\n",
    "            \"missingness\",\n",
    "            \"communities\",\n",
    "            \"metagenotypes\",\n",
    "        ],\n",
    "    ),\n",
    "    default_hyperparameters=dict(\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_r=0.9,\n",
    "        rho_hyper=5.0,\n",
    "        pi_hyper=0.2,\n",
    "        mu_hyper_mean=1.0,\n",
    "        mu_hyper_scale=10.0,\n",
    "        epsilon_hyper_mode=0.01,\n",
    "        epsilon_hyper_spread=1.5,\n",
    "        alpha_hyper_hyper_mean=100.0,\n",
    "        alpha_hyper_hyper_scale=1.0,\n",
    "        alpha_hyper_scale=0.5,\n",
    "    ),\n",
    ")\n",
    "def full_metagenotype_model_structure(\n",
    "    n,\n",
    "    g,\n",
    "    s,\n",
    "    a,\n",
    "    gamma_hyper,\n",
    "    delta_hyper_r,\n",
    "    delta_hyper_temp,\n",
    "    rho_hyper,\n",
    "    pi_hyper,\n",
    "    alpha_hyper_hyper_mean,\n",
    "    alpha_hyper_hyper_scale,\n",
    "    alpha_hyper_scale,\n",
    "    mu_hyper_mean,\n",
    "    mu_hyper_scale,\n",
    "    epsilon_hyper_mode,\n",
    "    epsilon_hyper_spread,\n",
    "    _unit,\n",
    "):\n",
    "    with pyro.plate(\"position\", g, dim=-1):\n",
    "        with pyro.plate(\"strain\", s, dim=-2):\n",
    "            _gamma = pyro.sample(\"_gamma\", dist.Beta(_unit, _unit))\n",
    "            gamma = pyro.deterministic(\n",
    "                \"gamma\",\n",
    "                unit_interval_power_transformation(\n",
    "                    _gamma, 1 / gamma_hyper, 1 / gamma_hyper\n",
    "                ),\n",
    "            )\n",
    "            # Position presence/absence\n",
    "            delta = pyro.sample(\n",
    "                \"delta\",\n",
    "                dist.RelaxedBernoulli(\n",
    "                    temperature=delta_hyper_temp, probs=delta_hyper_r\n",
    "                ),\n",
    "            )\n",
    "    pyro.deterministic(\"genotypes\", gamma)\n",
    "    pyro.deterministic(\"missingness\", delta)\n",
    "\n",
    "    # Meta-community composition\n",
    "    rho_betas = pyro.sample(\n",
    "        \"rho_betas\", dist.Beta(1.0, rho_hyper).expand([s - 1]).to_event()\n",
    "    )\n",
    "    rho = pyro.deterministic(\"rho\", stickbreaking_betas_to_probs(rho_betas))\n",
    "    pyro.deterministic(\"metacommunity\", rho)\n",
    "\n",
    "    alpha_hyper_mean = pyro.sample(\n",
    "        \"alpha_hyper_mean\",\n",
    "        dist.LogNormal(\n",
    "            loc=torch.log(alpha_hyper_hyper_mean),\n",
    "            scale=alpha_hyper_hyper_scale,\n",
    "        ),\n",
    "    )\n",
    "    m_hyper_r_mean = pyro.sample(\"m_hyper_r_mean\", dist.LogNormal(loc=_unit * 0.0, scale=_unit * 10.))\n",
    "    m_hyper_r_scale = pyro.sample(\n",
    "        \"m_hyper_r_scale\", dist.LogNormal(loc=_unit * 0.0, scale=_unit * 10.)\n",
    "    )\n",
    "\n",
    "    with pyro.plate(\"sample\", n, dim=-1):\n",
    "        # Community composition\n",
    "        pi = pyro.sample(\"pi\", dist.Dirichlet(pi_hyper * rho, validate_args=False))\n",
    "        # Sequencing error\n",
    "        epsilon = pyro.sample(\n",
    "            \"epsilon\",\n",
    "            dist.Beta(epsilon_hyper_spread, epsilon_hyper_spread / epsilon_hyper_mode),\n",
    "        ).unsqueeze(-1)\n",
    "        alpha = pyro.sample(\n",
    "            \"alpha\",\n",
    "            dist.LogNormal(loc=torch.log(alpha_hyper_mean), scale=alpha_hyper_scale),\n",
    "        ).unsqueeze(-1)\n",
    "        m_hyper_r = pyro.sample(\n",
    "            \"m_hyper_r\",\n",
    "            dist.LogNormal(loc=torch.log(m_hyper_r_mean), scale=m_hyper_r_scale),\n",
    "        ).unsqueeze(-1)\n",
    "        # Sample coverage\n",
    "        mu = pyro.sample(\n",
    "            \"mu\",\n",
    "            dist.LogNormal(loc=torch.log(mu_hyper_mean), scale=mu_hyper_scale),\n",
    "        )\n",
    "    pyro.deterministic(\"communities\", pi)\n",
    "\n",
    "    # Depth at each position\n",
    "    nu = pyro.deterministic(\"nu\", pi @ delta)\n",
    "    # TODO: Consider using pyro.distributions.GammaPoisson parameterization?\n",
    "    m = pyro.sample(\n",
    "        \"m\",\n",
    "        NegativeBinomialReparam(nu * mu.reshape((-1, 1)), m_hyper_r).to_event(),\n",
    "    )\n",
    "\n",
    "    # Expected fractions of each allele at each position\n",
    "    p_noerr = pyro.deterministic(\"p_noerr\", pi @ (gamma * delta) / nu)\n",
    "    p = pyro.deterministic(\n",
    "        \"p\", (1 - epsilon / 2) * (p_noerr) + (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "\n",
    "    # Observation\n",
    "    y = pyro.sample(\n",
    "        \"y\",\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m,\n",
    "        ).to_event(),\n",
    "    )\n",
    "    metagenotypes = pyro.deterministic(\"metagenotypes\", torch.stack([y, m - y], dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_zoo/full_metagenotype_dirichlet_rho.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/model_zoo/full_metagenotype_dirichlet_rho.py\n",
    "import sfacts.model\n",
    "from sfacts.model_zoo.components import (\n",
    "    _mapping_subset,\n",
    "    unit_interval_power_transformation,\n",
    "    stickbreaking_betas_to_probs,\n",
    "    NegativeBinomialReparam,\n",
    "    SHARED_DESCRIPTIONS,\n",
    "    SHARED_DIMS,\n",
    ")\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "\n",
    "@sfacts.model.structure(\n",
    "    dims=SHARED_DIMS,\n",
    "    description=_mapping_subset(\n",
    "        SHARED_DESCRIPTIONS,\n",
    "        [\n",
    "            \"rho\",\n",
    "            \"epsilon\",\n",
    "            \"m_hyper_r_mean\",\n",
    "            \"m_hyper_r_scale\",\n",
    "            \"m_hyper_r\",\n",
    "            \"mu\",\n",
    "            \"nu\",\n",
    "            \"p_noerr\",\n",
    "            \"p\",\n",
    "            \"m\",\n",
    "            \"y\",\n",
    "            \"alpha_hyper_mean\",\n",
    "            \"alpha\",\n",
    "            \"genotypes\",\n",
    "            \"missingness\",\n",
    "            \"communities\",\n",
    "            \"metagenotypes\",\n",
    "        ],\n",
    "    ),\n",
    "    default_hyperparameters=dict(\n",
    "        gamma_hyper=0.01,\n",
    "        delta_hyper_temp=0.01,\n",
    "        delta_hyper_r=0.9,\n",
    "        rho_hyper=5.0,\n",
    "        pi_hyper=0.2,\n",
    "        mu_hyper_mean=1.0,\n",
    "        mu_hyper_scale=10.0,\n",
    "        epsilon_hyper_mode=0.01,\n",
    "        epsilon_hyper_spread=1.5,\n",
    "        alpha_hyper_hyper_mean=100.0,\n",
    "        alpha_hyper_hyper_scale=1.0,\n",
    "        alpha_hyper_scale=0.5,\n",
    "    ),\n",
    ")\n",
    "def full_metagenotype_dirichlet_rho_model_structure(\n",
    "    n,\n",
    "    g,\n",
    "    s,\n",
    "    a,\n",
    "    gamma_hyper,\n",
    "    delta_hyper_r,\n",
    "    delta_hyper_temp,\n",
    "    rho_hyper,\n",
    "    pi_hyper,\n",
    "    alpha_hyper_hyper_mean,\n",
    "    alpha_hyper_hyper_scale,\n",
    "    alpha_hyper_scale,\n",
    "    mu_hyper_mean,\n",
    "    mu_hyper_scale,\n",
    "    epsilon_hyper_mode,\n",
    "    epsilon_hyper_spread,\n",
    "    _unit,\n",
    "):\n",
    "    with pyro.plate(\"position\", g, dim=-1):\n",
    "        with pyro.plate(\"strain\", s, dim=-2):\n",
    "            _gamma = pyro.sample(\"_gamma\", dist.Beta(_unit, _unit))\n",
    "            gamma = pyro.deterministic(\n",
    "                \"gamma\",\n",
    "                unit_interval_power_transformation(\n",
    "                    _gamma, 1 / gamma_hyper, 1 / gamma_hyper\n",
    "                ),\n",
    "            )\n",
    "            # Position presence/absence\n",
    "            delta = pyro.sample(\n",
    "                \"delta\",\n",
    "                dist.RelaxedBernoulli(\n",
    "                    temperature=delta_hyper_temp, probs=delta_hyper_r\n",
    "                ),\n",
    "            )\n",
    "    pyro.deterministic(\"genotypes\", gamma)\n",
    "    pyro.deterministic(\"missingness\", delta)\n",
    "\n",
    "    # Meta-community composition\n",
    "    rho = pyro.sample(\"rho\", dist.Dirichlet(_unit.repeat(s) * rho_hyper))\n",
    "    pyro.deterministic(\"metacommunity\", rho)\n",
    "\n",
    "    alpha_hyper_mean = pyro.sample(\n",
    "        \"alpha_hyper_mean\",\n",
    "        dist.LogNormal(\n",
    "            loc=torch.log(alpha_hyper_hyper_mean),\n",
    "            scale=alpha_hyper_hyper_scale,\n",
    "        ),\n",
    "    )\n",
    "    m_hyper_r_mean = pyro.sample(\"m_hyper_r_mean\", dist.LogNormal(loc=_unit * 0.0, scale=_unit * 10.))\n",
    "    m_hyper_r_scale = pyro.sample(\n",
    "        \"m_hyper_r_scale\", dist.LogNormal(loc=_unit * 0.0, scale=_unit * 10.)\n",
    "    )\n",
    "\n",
    "    with pyro.plate(\"sample\", n, dim=-1):\n",
    "        # Community composition\n",
    "        pi = pyro.sample(\"pi\", dist.Dirichlet(pi_hyper * rho, validate_args=False))\n",
    "        # Sequencing error\n",
    "        epsilon = pyro.sample(\n",
    "            \"epsilon\",\n",
    "            dist.Beta(epsilon_hyper_spread, epsilon_hyper_spread / epsilon_hyper_mode),\n",
    "        ).unsqueeze(-1)\n",
    "        alpha = pyro.sample(\n",
    "            \"alpha\",\n",
    "            dist.LogNormal(loc=torch.log(alpha_hyper_mean), scale=alpha_hyper_scale),\n",
    "        ).unsqueeze(-1)\n",
    "        m_hyper_r = pyro.sample(\n",
    "            \"m_hyper_r\",\n",
    "            dist.LogNormal(loc=torch.log(m_hyper_r_mean), scale=m_hyper_r_scale),\n",
    "        ).unsqueeze(-1)\n",
    "        # Sample coverage\n",
    "        mu = pyro.sample(\n",
    "            \"mu\",\n",
    "            dist.LogNormal(loc=torch.log(mu_hyper_mean), scale=mu_hyper_scale),\n",
    "        )\n",
    "    pyro.deterministic(\"communities\", pi)\n",
    "\n",
    "    # Depth at each position\n",
    "    nu = pyro.deterministic(\"nu\", pi @ delta)\n",
    "    # TODO: Consider using pyro.distributions.GammaPoisson parameterization?\n",
    "    m = pyro.sample(\n",
    "        \"m\",\n",
    "        NegativeBinomialReparam(nu * mu.reshape((-1, 1)), m_hyper_r).to_event(),\n",
    "    )\n",
    "\n",
    "    # Expected fractions of each allele at each position\n",
    "    p_noerr = pyro.deterministic(\"p_noerr\", pi @ (gamma * delta) / nu)\n",
    "    p = pyro.deterministic(\n",
    "        \"p\", (1 - epsilon / 2) * (p_noerr) + (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "\n",
    "    # Observation\n",
    "    y = pyro.sample(\n",
    "        \"y\",\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m,\n",
    "        ).to_event(),\n",
    "    )\n",
    "    metagenotypes = pyro.deterministic(\"metagenotypes\", torch.stack([y, m - y], dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/estimation.py\n",
    "import sfacts as sf\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import non_negative_factorization\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "# from sklearn.decomposition import non_negative_factorization\n",
    "# from sfacts.genotype import genotype_pdist, adjust_genotype_by_missing\n",
    "from sfacts.pyro_util import all_torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import scipy as sp\n",
    "# from scipy.spatial.distance import squareform\n",
    "import pyro\n",
    "\n",
    "# import pyro.distributions as dist\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sfacts.logging_util import info\n",
    "\n",
    "\n",
    "def nmf_approximation(\n",
    "    world,\n",
    "    s,\n",
    "    regularization=\"both\",\n",
    "    alpha=1.0,\n",
    "    l1_ratio=1.0,\n",
    "    tol=1e-4,\n",
    "    max_iter=int(1e4),\n",
    "    random_state=None,\n",
    "    init=\"random\",\n",
    "    **kwargs,\n",
    "):\n",
    "    d = world.metagenotypes.to_series().unstack(\"sample\")\n",
    "    columns = d.columns\n",
    "    index = d.index\n",
    "\n",
    "    gamma0, pi0, _ = non_negative_factorization(\n",
    "        d.values,\n",
    "        n_components=s,\n",
    "        alpha=alpha,\n",
    "        l1_ratio=l1_ratio,\n",
    "        tol=tol,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "        init=init,\n",
    "        **kwargs,\n",
    "    )\n",
    "    pi1 = (\n",
    "        pd.DataFrame(pi0, columns=columns)\n",
    "        .rename_axis(index=\"strain\")\n",
    "        .stack()\n",
    "        .to_xarray()\n",
    "    )\n",
    "    gamma1 = (\n",
    "        pd.DataFrame(gamma0, index=index)\n",
    "        .rename_axis(columns=\"strain\")\n",
    "        .stack()\n",
    "        .to_xarray()\n",
    "    )\n",
    "\n",
    "    # Rebalance estimates: mean strain genotype of 1\n",
    "    gamma1_strain_factor = gamma1.sum(\"allele\").mean(\"position\")\n",
    "    gamma2 = gamma1 / gamma1_strain_factor\n",
    "    pi2 = pi1 * gamma1_strain_factor\n",
    "\n",
    "    # Transform estimates: sum-to-1\n",
    "    gamma3 = (gamma2 / gamma2.sum(\"allele\")).fillna(0.5)\n",
    "    pi3 = pi2 / pi2.sum(\"strain\")\n",
    "\n",
    "    approx = sf.data.World(\n",
    "        xr.Dataset(\n",
    "            dict(\n",
    "                communities=pi3.transpose(\"sample\", \"strain\"),\n",
    "                genotypes=gamma3.sel(allele=\"alt\").transpose(\"strain\", \"position\"),\n",
    "                metagenotypes=world.metagenotypes.data,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return approx\n",
    "\n",
    "\n",
    "def estimate_parameters(\n",
    "    model,\n",
    "    dtype=torch.float32,\n",
    "    device=\"cpu\",\n",
    "    initialize_params=None,\n",
    "    jit=True,\n",
    "    maxiter=10000,\n",
    "    lagA=20,\n",
    "    lagB=100,\n",
    "    opt=pyro.optim.Adamax({\"lr\": 1e-2}, {\"clip_norm\": 100}),\n",
    "    quiet=False,\n",
    "    seed=None,\n",
    "):\n",
    "    if initialize_params is None:\n",
    "        initialize_params = {}\n",
    "        \n",
    "    if jit:\n",
    "        loss = pyro.infer.JitTrace_ELBO()\n",
    "    else:\n",
    "        loss = pyro.infer.Trace_ELBO()\n",
    "\n",
    "    sf.pyro_util.set_random_seed(seed, warn=(not quiet))\n",
    "\n",
    "    _guide = pyro.infer.autoguide.AutoLaplaceApproximation(\n",
    "        model,\n",
    "        init_loc_fn=pyro.infer.autoguide.initialization.init_to_value(\n",
    "            values=all_torch(**initialize_params, dtype=dtype, device=device)\n",
    "        ),\n",
    "    )\n",
    "    svi = pyro.infer.SVI(model, _guide, opt, loss=loss)\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    history = []\n",
    "    pbar = tqdm(range(maxiter), disable=quiet)\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                pbar.close()\n",
    "                raise RuntimeError(\"ELBO NaN?\")\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if i % 10 == 0:\n",
    "                if i > lagB:\n",
    "                    delta = history[-2] - history[-1]\n",
    "                    delta_lagA = (history[-lagA] - history[-1]) / lagA\n",
    "                    delta_lagB = (history[-lagB] - history[-1]) / lagB\n",
    "                    pbar.set_postfix(\n",
    "                        {\n",
    "                            \"ELBO\": history[-1],\n",
    "                            \"delta\": delta,\n",
    "                            f\"lag{lagA}\": delta_lagA,\n",
    "                            f\"lag{lagB}\": delta_lagB,\n",
    "                        }\n",
    "                    )\n",
    "                    if (delta_lagA <= 0) and (delta_lagB <= 0):\n",
    "                        pbar.close()\n",
    "#                         info(\"Converged\", quiet=quiet)\n",
    "                        break\n",
    "    except KeyboardInterrupt:\n",
    "        pbar.close()\n",
    "        info(\"Interrupted\", quiet=quiet)\n",
    "        pass\n",
    "    est = pyro.infer.Predictive(model, guide=_guide, num_samples=1)()\n",
    "    est = {k: est[k].detach().cpu().numpy().mean(0).squeeze() for k in est.keys()}\n",
    "\n",
    "    if device.startswith(\"cuda\"):\n",
    "        #         info(\n",
    "        #             \"CUDA available mem: {}\".format(\n",
    "        #                 torch.cuda.get_device_properties(0).total_memory\n",
    "        #             ),\n",
    "        #         )\n",
    "        #         info(\"CUDA reserved mem: {}\".format(torch.cuda.memory_reserved(0)))\n",
    "        #         info(\"CUDA allocated mem: {}\".format(torch.cuda.memory_allocated(0)))\n",
    "        #         info(\n",
    "        #             \"CUDA free mem: {}\".format(\n",
    "        #                 torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)\n",
    "        #             )\n",
    "        #         )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model.format_world(est), history\n",
    "\n",
    "\n",
    "def strain_cluster(world, thresh, linkage=\"complete\", pdist_func=None):\n",
    "    if pdist_func is None:\n",
    "        pdist_func = lambda w: w.genotypes.pdist()\n",
    "    clust = pd.Series(\n",
    "        AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=thresh,\n",
    "            linkage=\"complete\",\n",
    "            affinity=\"precomputed\",\n",
    "        )\n",
    "        .fit(pdist_func(world))\n",
    "        .labels_,\n",
    "        index=world.strain,\n",
    "    )\n",
    "    return clust\n",
    "\n",
    "\n",
    "def communities_aggregated_by_strain_cluster(world, thresh, **kwargs):\n",
    "    clust = strain_cluster(world, thresh, **kwargs)\n",
    "    return sf.data.Communities(\n",
    "        world.communities.to_pandas()\n",
    "        .groupby(clust, axis=\"columns\")\n",
    "        .sum()\n",
    "        .rename_axis(columns=\"strain\")\n",
    "        .stack()\n",
    "        .to_xarray()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sfacts/workflow.py\n",
    "import sfacts as sf\n",
    "import pyro\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def _chunk_start_end_iterator(total, per):\n",
    "    for i in range(total // per):\n",
    "        yield (per * i), (per * (i + 1))\n",
    "    if (i + 1) * per < total:\n",
    "        yield (i + 1) * per, total\n",
    "\n",
    "\n",
    "def fit_metagenotypes_simple(\n",
    "    structure,\n",
    "    metagenotypes,\n",
    "    nstrain,\n",
    "    hyperparameters=None,\n",
    "    condition_on=None,\n",
    "    device='cpu',\n",
    "    dtype=torch.float32,\n",
    "    quiet=False,\n",
    "    estimation_kwargs=None,\n",
    "):\n",
    "    sf.logging_util.info(f\"START: Fitting data with shape {metagenotypes.sizes}.\", quiet=quiet)\n",
    "    model = sf.model.ParameterizedModel(\n",
    "        structure,\n",
    "        coords=dict(\n",
    "            sample=metagenotypes.sample.values,\n",
    "            position=metagenotypes.position.values,\n",
    "            allele=metagenotypes.allele.values,\n",
    "            strain=range(nstrain),\n",
    "        ),\n",
    "        hyperparameters=hyperparameters,\n",
    "        data=condition_on,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "    ).condition(\n",
    "        **metagenotypes.to_counts_and_totals()\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    est, history = sf.estimation.estimate_parameters(\n",
    "        model,\n",
    "        quiet=quiet,\n",
    "        **estimation_kwargs,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    delta_time = end_time - start_time\n",
    "    sf.logging_util.info(f\"END: Fit in {delta_time} seconds.\", quiet=quiet)\n",
    "    return est, history\n",
    "\n",
    "def fit_metagenotypes_then_refit_genotypes(\n",
    "    structure,\n",
    "    metagenotypes,\n",
    "    nstrain,\n",
    "    hyperparameters=None,\n",
    "    stage2_hyperparameters=None,\n",
    "    condition_on=None,\n",
    "    device='cpu',\n",
    "    dtype=torch.float32,\n",
    "    quiet=False,\n",
    "    estimation_kwargs=None\n",
    "):\n",
    "    if stage2_hyperparameters is None:\n",
    "        stage2_hyperparameters = {}\n",
    "        \n",
    "    _estimate_parameters = lambda model: sf.estimation.estimate_parameters(\n",
    "        model, quiet=quiet, **estimation_kwargs\n",
    "    )\n",
    "\n",
    "    sf.logging_util.info(f\"START: Fitting data with shape {metagenotypes.sizes}.\", quiet=quiet)\n",
    "    model = sf.model.ParameterizedModel(\n",
    "        structure,\n",
    "        coords=dict(\n",
    "            sample=metagenotypes.sample.values,\n",
    "            position=metagenotypes.position.values,\n",
    "            allele=metagenotypes.allele.values,\n",
    "            strain=range(nstrain),\n",
    "        ),\n",
    "        hyperparameters=hyperparameters,\n",
    "        data=condition_on,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "    ).condition(**metagenotypes.to_counts_and_totals())\n",
    "\n",
    "    start_time = time.time()\n",
    "    est0, history0 = _estimate_parameters(model)\n",
    "    sf.logging_util.info(\"Finished initial fitting.\")\n",
    "    sf.logging_util.info(f\"Refitting missingness.\", quiet=quiet)\n",
    "    est1, history1 = _estimate_parameters(\n",
    "        model\n",
    "        .condition(\n",
    "            pi=est0.data.communities.values,\n",
    "            mu=est0.data.mu.values,\n",
    "            alpha=est0.data.alpha.values,\n",
    "            epsilon=est0.data.epsilon.values,\n",
    "            m_hyper_r=est0.data.m_hyper_r.values,\n",
    "        )\n",
    "    )\n",
    "    sf.logging_util.info(f\"Refitting genotypes.\", quiet=quiet)\n",
    "    est2, history2 = _estimate_parameters(\n",
    "        model\n",
    "        .condition(\n",
    "            delta=est1.data.missingness.values,\n",
    "            pi=est1.data.communities.values,\n",
    "            mu=est1.data.mu.values,\n",
    "            alpha=est1.data.alpha.values,\n",
    "            epsilon=est1.data.epsilon.values,\n",
    "            m_hyper_r=est1.data.m_hyper_r.values,\n",
    "        )\n",
    "        .with_hyperparameters(**stage2_hyperparameters)\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    delta_time = end_time - start_time\n",
    "    sf.logging_util.info(f\"END: Fit in {delta_time} seconds.\", quiet=quiet)\n",
    "    return (est0, est1), (history0, history1)\n",
    "\n",
    "\n",
    "def fit_metagenotype_subsample_collapse_then_iteratively_refit_full_genotypes(\n",
    "    structure,\n",
    "    metagenotypes,\n",
    "    nstrain,\n",
    "    nposition,\n",
    "    thresh,\n",
    "    hyperparameters=None,\n",
    "    stage2_hyperparameters=None,\n",
    "    condition_on=None,\n",
    "    device='cpu',\n",
    "    dtype=torch.float32,\n",
    "    quiet=False,\n",
    "    estimation_kwargs=None\n",
    "):\n",
    "    if stage2_hyperparameters is None:\n",
    "        stage2_hyperparameters = {}\n",
    "\n",
    "    _estimate_parameters = lambda model: sf.estimation.estimate_parameters(\n",
    "        model, quiet=quiet, **estimation_kwargs,\n",
    "    )\n",
    "    _info = lambda *args, **kwargs: sf.logging_util.info(*args, quiet=quiet, **kwargs)\n",
    "\n",
    "    _info(f\"START: Fitting data with shape {metagenotypes.sizes}.\")\n",
    "    _info(f\"Fitting strain compositions using {nposition} randomly sampled positions.\")\n",
    "    metagenotypes_ss = metagenotypes.random_sample(nposition, 'position')\n",
    "    model = sf.model.ParameterizedModel(\n",
    "        structure,\n",
    "        coords=dict(\n",
    "            sample=metagenotypes.sample.values,\n",
    "            position=metagenotypes_ss.position.values,\n",
    "            allele=metagenotypes.allele.values,\n",
    "            strain=range(nstrain),\n",
    "        ),\n",
    "        hyperparameters=hyperparameters,\n",
    "        data=condition_on,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    est_curr, _ = _estimate_parameters(\n",
    "        model\n",
    "        .condition(**metagenotypes_ss.to_counts_and_totals())\n",
    "    )\n",
    "    _info(f\"Finished initial fitting.\")\n",
    "    _info(f\"Refitting genotypes with {stage2_hyperparameters}.\")\n",
    "    est_curr, _ = _estimate_parameters(\n",
    "        model\n",
    "        .with_hyperparameters(**stage2_hyperparameters)\n",
    "        .condition(\n",
    "            delta=est_curr.data.missingness.values,\n",
    "            pi=est_curr.data.communities.values,\n",
    "            mu=est_curr.data.mu.values,\n",
    "            alpha=est_curr.data.alpha.values,\n",
    "            epsilon=est_curr.data.epsilon.values,\n",
    "            m_hyper_r=est_curr.data.m_hyper_r.values,\n",
    "        )\n",
    "        .condition(**metagenotypes_ss.to_counts_and_totals()),\n",
    "    )\n",
    "    _info(f\"Collapsing {nstrain} initial strains.\")\n",
    "    agg_communities = sf.estimation.communities_aggregated_by_strain_cluster(\n",
    "        est_curr, thresh=thresh, pdist_func=lambda w: w.genotypes.pdist(quiet=quiet),\n",
    "    )\n",
    "    _info(f\"{agg_communities.sizes['strain']} strains after collapsing.\")\n",
    "    \n",
    "    _info(f\"Iteratively refitting missingness/genotypes.\")\n",
    "    chunks = {}\n",
    "    for position_start, position_end in _chunk_start_end_iterator(\n",
    "        metagenotypes.sizes['position'],\n",
    "        nposition,\n",
    "    ):\n",
    "        _info(f\"Fitting bin ({position_start}, {position_end}).\")\n",
    "        metagenotypes_chunk = metagenotypes.mlift('isel', position=slice(position_start, position_end))\n",
    "        est_curr, _ = _estimate_parameters(\n",
    "            model\n",
    "            .with_amended_coords(\n",
    "                position=metagenotypes_chunk.position.values,\n",
    "                strain=agg_communities.strain.values,\n",
    "            )\n",
    "            .condition(\n",
    "                pi=agg_communities.values,\n",
    "                mu=est_curr.data.mu.values,\n",
    "                alpha=est_curr.data.alpha.values,\n",
    "                epsilon=est_curr.data.epsilon.values,\n",
    "                m_hyper_r=est_curr.data.m_hyper_r.values,\n",
    "            )\n",
    "            .condition(**metagenotypes_chunk.to_counts_and_totals()),\n",
    "        )\n",
    "        est_curr, _ = _estimate_parameters(\n",
    "            model\n",
    "            .with_amended_coords(\n",
    "                position=metagenotypes_chunk.position.values,\n",
    "                strain=agg_communities.strain.values,\n",
    "            )\n",
    "            .with_hyperparameters(**stage2_hyperparameters)\n",
    "            .condition(\n",
    "                delta=est_curr.data.missingness.values,\n",
    "                pi=est_curr.data.communities.values,\n",
    "                mu=est_curr.data.mu.values,\n",
    "                alpha=est_curr.data.alpha.values,\n",
    "                epsilon=est_curr.data.epsilon.values,\n",
    "                m_hyper_r=est_curr.data.m_hyper_r.values,\n",
    "            )\n",
    "            .condition(**metagenotypes_chunk.to_counts_and_totals()),\n",
    "        )\n",
    "        chunks[position_start] = est_curr\n",
    "    est_curr = sf.data.World.concat(chunks, dim='position', rename_coords=False)\n",
    "    end_time = time.time()\n",
    "    delta_time = end_time - start_time\n",
    "    _info(f\"END: Fit in {delta_time} seconds.\")\n",
    "    return est_curr\n",
    "\n",
    "\n",
    "# def fit_then_relax_genotypes_and_collapse(\n",
    "#     model,\n",
    "#     world,\n",
    "#     thresh,\n",
    "#     initialize_params=None,\n",
    "#     stage2_hyperparameters=None,\n",
    "#     quiet=False,\n",
    "#     **kwargs,\n",
    "# ):\n",
    "#     if stage2_hyperparameters is None:\n",
    "#         stage2_hyperparameters = {}\n",
    "\n",
    "#     est0, history0 = fit_simple(\n",
    "#         model,\n",
    "#         world,\n",
    "#         initialize_params=initialize_params,\n",
    "#         quiet=quiet,\n",
    "#         **kwargs,\n",
    "#     )\n",
    "#     est1, history1 = fit_simple(\n",
    "#         model.with_hyperparameters(**stage2_hyperparameters).condition(\n",
    "#             pi=est0.data.communities.values,\n",
    "#             mu=est0.data.mu.values,\n",
    "#             alpha=est0.data.alpha.values,\n",
    "#             epsilon=est0.data.epsilon.values,\n",
    "#             m_hyper_r=est0.data.m_hyper_r.values,\n",
    "#         ),\n",
    "#         world,\n",
    "#         quiet=quiet,\n",
    "#         **kwargs,\n",
    "#     )\n",
    "#     agg_communities = sf.estimation.communities_aggregated_by_strain_cluster(\n",
    "#         est1, thresh=thresh\n",
    "#     )\n",
    "#     est2, history2 = fit_simple(\n",
    "#         model.with_amended_coords(strain=agg_communities.strain).condition(\n",
    "#             pi=agg_communities.values,\n",
    "#             mu=est1.data.mu.values,\n",
    "#             alpha=est1.data.alpha.values,\n",
    "#             epsilon=est1.data.epsilon.values,\n",
    "#             m_hyper_r=est1.data.m_hyper_r.values,\n",
    "#         ),\n",
    "#         world,\n",
    "#         quiet=quiet,\n",
    "#         **kwargs,\n",
    "#     )\n",
    "#     est3, history3 = fit_simple(\n",
    "#         model.with_amended_coords(strain=agg_communities.strain)\n",
    "#         .with_hyperparameters(**stage2_hyperparameters)\n",
    "#         .condition(\n",
    "#             delta=est2.missingness.values,\n",
    "#             pi=est2.communities.values,\n",
    "#             mu=est2.data.mu.values,\n",
    "#             alpha=est2.data.alpha.values,\n",
    "#             epsilon=est2.data.epsilon.values,\n",
    "#             m_hyper_r=est2.data.m_hyper_r.values,\n",
    "#         ),\n",
    "#         world,\n",
    "#         quiet,\n",
    "#         **kwargs,\n",
    "#     )\n",
    "#     return (est0, est1, est2, est3), (history0, history1, history2, history3)\n",
    "\n",
    "\n",
    "# def fit_subsample_then_refit_relaxed_genotypes(\n",
    "#     model,\n",
    "#     world,\n",
    "#     npositions,\n",
    "#     stage2_hyperparameters,\n",
    "#     initialize_params=None,\n",
    "#     quiet=False,\n",
    "#     **kwargs,\n",
    "# ):\n",
    "#     est0, history0 = fit_simple(model, world, initialize_params=initialize_params, quiet=quiet, **kwargs)\n",
    "#     est1, history1 = fit_simple(\n",
    "#         model.with_hyperparameters(**stage2_hyperparameters).condition(\n",
    "#             # FIXME: Drop conditining on delta for consistency with 3-stage workflow?\n",
    "#             delta=est0.data.missingness.values,\n",
    "#             pi=est0.data.communities.values,\n",
    "#             mu=est0.data.mu.values,\n",
    "#             alpha=est0.data.alpha.values,\n",
    "#             epsilon=est0.data.epsilon.values,\n",
    "#             m_hyper_r=est0.data.m_hyper_r.values,\n",
    "#         ),\n",
    "#         world,\n",
    "#         quiet=quiet,\n",
    "#         **kwargs,\n",
    "#     )\n",
    "#     return (est0, est1), (history0, history1)\n",
    "\n",
    "\n",
    "# def simulation_benchmark(\n",
    "#     nsample,\n",
    "#     nposition,\n",
    "#     sim_nstrain,\n",
    "#     fit_nstrain,\n",
    "#     sim_model,\n",
    "#     fit_model=None,\n",
    "#     sim_data=None,\n",
    "#     sim_hyperparameters=None,\n",
    "#     sim_seed=None,\n",
    "#     fit_data=None,\n",
    "#     fit_hyperparameters=None,\n",
    "#     fit_seed=None,\n",
    "#     opt=pyro.optim.Adamax({\"lr\": 1e-0}, {\"clip_norm\": 100}),\n",
    "#     **fit_kwargs,\n",
    "# ):\n",
    "#     if fit_model is None:\n",
    "#         fit_model = sim_model\n",
    "\n",
    "#     sim = sf.model.ParameterizedModel(\n",
    "#         sim_model,\n",
    "#         coords=dict(\n",
    "#             sample=nsample,\n",
    "#             position=nposition,\n",
    "#             allele=[\"alt\", \"ref\"],\n",
    "#             strain=sim_nstrain,\n",
    "#         ),\n",
    "#         data=sim_data,\n",
    "#         hyperparameters=sim_hyperparameters,\n",
    "#     ).simulate_world(seed=sim_seed)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     est, history, *_ = sf.estimation.estimate_parameters(\n",
    "#         sf.model.ParameterizedModel(\n",
    "#             fit_model,\n",
    "#             coords=dict(\n",
    "#                 sample=nsample,\n",
    "#                 position=nposition,\n",
    "#                 allele=[\"alt\", \"ref\"],\n",
    "#                 strain=fit_nstrain,\n",
    "#             ),\n",
    "#             data=fit_data,\n",
    "#             hyperparameters=fit_hyperparameters,\n",
    "#         ).condition(\n",
    "#             **sim.metagenotypes.to_counts_and_totals(),\n",
    "#         ),\n",
    "#         opt=opt,\n",
    "#         seed=fit_seed,\n",
    "#         **fit_kwargs,\n",
    "#     )\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     return (\n",
    "#         sf.evaluation.weighted_genotype_error(sim, est),\n",
    "#         sf.evaluation.community_error(sim, est),\n",
    "#         end_time - start_time,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "82px",
    "width": "168px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}