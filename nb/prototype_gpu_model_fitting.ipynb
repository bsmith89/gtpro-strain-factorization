{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%shell\n",
    "\n",
    "# pip install pyro-ppl arviz\n",
    "\n",
    "# git init .\n",
    "# git remote add origin https://github.com/bsmith89/gtpro-strain-factorization\n",
    "# git fetch origin\n",
    "# git checkout main\n",
    "\n",
    "# curl -L -o gtpro.nc https://www.dropbox.com/s/3pv7oszorvhbtee/gtpro.nc?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lib.util import info, idxwhere\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "import arviz as az\n",
    "from pyro.ops.contract import einsum\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "def rss(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def binary_entropy(p):\n",
    "    q = (1 - p)\n",
    "    return -p * np.log2(p) - q * np.log2(q)\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    min_loss = loss_history.min()\n",
    "    plt.plot(loss_history - min_loss)\n",
    "    plt.title(f'+{min_loss}')\n",
    "    plt.yscale('log')\n",
    "    return plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_map(\n",
    "    model,\n",
    "    max_iter=int(1e5),\n",
    "    learning_rate = 1e-0,\n",
    "):\n",
    "    guide = pyro.infer.autoguide.AutoLaplaceApproximation(model)\n",
    "    svi = pyro.infer.SVI(\n",
    "        model,\n",
    "        guide,\n",
    "        pyro.optim.Adamax(\n",
    "            optim_args={\"lr\": learning_rate},\n",
    "            clip_args={\"clip_norm\": 100.}\n",
    "        ),\n",
    "        loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "    \n",
    "    pyro.clear_param_store()\n",
    "    pbar = tqdm(range(max_iter))\n",
    "    history = []\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                break\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if (i % 1 == 0):\n",
    "                if i > 1:\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta': history[-2] - history[-1]\n",
    "                    })\n",
    "    except KeyboardInterrupt:\n",
    "        info('Optimization interrupted')\n",
    "    pbar.refresh()\n",
    "    \n",
    "    # Gather MAP from parameter-store\n",
    "    mapest = {\n",
    "        k: v.detach().cpu().numpy().squeeze()\n",
    "        for k, v\n",
    "        in pyro.infer.Predictive(\n",
    "            model, guide=guide, num_samples=1\n",
    "        )().items()\n",
    "    }\n",
    "    return mapest, np.array(history)\n",
    "\n",
    "\n",
    "def mean_residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    out = np.abs(((frac_obs - expect_frac)))\n",
    "    out[np.isnan(out)] = 0\n",
    "    return (out * m).sum() / m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cpu(\n",
    "    s,\n",
    "    m,\n",
    "    y=None,\n",
    "    gamma_hyper=torch.tensor(1.),\n",
    "    pi0=torch.tensor(1.),\n",
    "    rho0=torch.tensor(1.),\n",
    "    epsilon0=torch.tensor(0.01),\n",
    "    alpha0=torch.tensor(1000.),\n",
    "):\n",
    "    \n",
    "    n, g = m.shape\n",
    "    \n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.Beta(gamma_hyper, gamma_hyper)\n",
    "            )\n",
    "    # gamma.shape == (s, g)\n",
    "    \n",
    "    rho_hyper = pyro.sample('rho_hyper', dist.Gamma(rho0, 1.))\n",
    "    rho = pyro.sample('rho', dist.Dirichlet(torch.ones(s) * rho_hyper))\n",
    "#     rho_hyper = pyro.sample('rho_hyper', dist.Beta(rho0, 1 - rho0))\n",
    "#     rho = pyro.sample('rho', dist.Beta(rho_hyper, 1 - rho_hyper).expand([1, s]).to_event(1))\n",
    "    # rho.shape == 's'\n",
    "    \n",
    "    epsilon_hyper = pyro.sample('epsilon_hyper', dist.Beta(1., 1 / epsilon0))\n",
    "    alpha_hyper = pyro.sample('alpha_hyper', dist.Gamma(alpha0, 1.))\n",
    "    pi_hyper = pyro.sample('pi_hyper', dist.Gamma(pi0, 1.))\n",
    "    \n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        pi = pyro.sample('pi', dist.Dirichlet(rho * s * pi_hyper))\n",
    "        alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1) \n",
    "    # pi.shape == (n, s)\n",
    "    # alpha.shape == epsilon.shape == (n,)\n",
    "\n",
    "    p_noerr = pyro.deterministic('p_noerr', (pi @ gamma))\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    # p.shape == (n, g)\n",
    "\n",
    "        \n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ),\n",
    "        obs=y\n",
    "    )\n",
    "    # y.shape == (n, g)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_gpu(\n",
    "#     s,\n",
    "#     m,\n",
    "#     y=None,\n",
    "#     gamma_hyper=torch.tensor(1.).cuda(),\n",
    "#     pi0=torch.tensor(1.).cuda(),\n",
    "#     rho0=torch.tensor(1.).cuda(),\n",
    "#     epsilon0=torch.tensor(0.01).cuda(),\n",
    "#     alpha0=torch.tensor(1000.).cuda(),\n",
    "# ):\n",
    "    \n",
    "#     n, g = m.shape\n",
    "    \n",
    "#     with pyro.plate('position', g, dim=-1):\n",
    "#         with pyro.plate('strain', s, dim=-2):\n",
    "#             gamma = pyro.sample(\n",
    "#                 'gamma', dist.Beta(gamma_hyper, gamma_hyper)\n",
    "#             )\n",
    "#     # gamma.shape == (s, g)\n",
    "    \n",
    "#     rho_hyper = pyro.sample('rho_hyper', dist.Gamma(rho0, 1.))\n",
    "#     rho = pyro.sample('rho', dist.Dirichlet(torch.ones(s).cuda() * rho_hyper))\n",
    "# #     rho_hyper = pyro.sample('rho_hyper', dist.Beta(rho0, 1 - rho0))\n",
    "# #     rho = pyro.sample('rho', dist.Beta(rho_hyper, 1 - rho_hyper).expand([1, s]).to_event(1))\n",
    "#     # rho.shape == 's'\n",
    "    \n",
    "#     epsilon_hyper = pyro.sample('epsilon_hyper', dist.Beta(1., 1 / epsilon0))\n",
    "#     alpha_hyper = pyro.sample('alpha_hyper', dist.Gamma(alpha0, 1.))\n",
    "#     pi_hyper = pyro.sample('pi_hyper', dist.Gamma(pi0, 1.))\n",
    "    \n",
    "#     with pyro.plate('sample', n, dim=-1):\n",
    "#         pi = pyro.sample('pi', dist.Dirichlet(rho * s * pi_hyper))\n",
    "#         alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "#         epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1) \n",
    "#     # pi.shape == (n, s)\n",
    "#     # alpha.shape == epsilon.shape == (n,)\n",
    "\n",
    "#     p_noerr = pyro.deterministic('p_noerr', (pi @ gamma))\n",
    "#     p = pyro.deterministic('p',\n",
    "#         (1 - epsilon / 2) * (p_noerr) +\n",
    "#         (epsilon / 2) * (1 - p_noerr)\n",
    "#     )\n",
    "#     # p.shape == (n, g)\n",
    "\n",
    "        \n",
    "#     y = pyro.sample(\n",
    "#         'y',\n",
    "#         dist.BetaBinomial(\n",
    "#             concentration1=alpha * p,\n",
    "#             concentration0=alpha * (1 - p),\n",
    "#             total_count=m\n",
    "#         ),\n",
    "#         obs=y\n",
    "#     )\n",
    "#     # y.shape == (n, g)\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataarray('gtpro.nc').squeeze()\n",
    "data.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_allele_rcvrg = (data.sum('read').max('allele') / data.sum(['read', 'allele'])).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.concatenate([[0], np.linspace(0.5, 1, num=21)])\n",
    "allele_frac_hist = major_allele_rcvrg.to_pandas().T.apply(lambda x: np.histogram(x, bins=bins)[0]).set_index(bins[:-1]).rename_axis(index='bin_low')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(\n",
    "    allele_frac_hist**(1/5),\n",
    "    metric='cosine',\n",
    "    vmin=0, vmax=7,\n",
    "    row_cluster=False,\n",
    "    figsize=(allele_frac_hist.shape[1]*0.004, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Samples with >5% of positions covered\n",
    "suff_cvrg_samples = (data.sum(['allele', 'read']) > 0).mean('position') > 0.05\n",
    "npos = 4000\n",
    "npos_out = 4000\n",
    "position_ss_ = np.random.choice(\n",
    "    np.arange(data.shape[1]),\n",
    "    size=npos + npos_out,\n",
    "    replace=False\n",
    "    )\n",
    "position_ss, position_ss_out = position_ss_[:npos], position_ss_[npos:]\n",
    "\n",
    "# Build m, y matrices from data, summing over both reads.\n",
    "_data = data[suff_cvrg_samples, position_ss].astype('float32')\n",
    "m = _data.sum(['read', 'allele']).values\n",
    "n, g = m.shape\n",
    "y_obs = _data.sum('read').sel(allele='alt').values\n",
    "\n",
    "\n",
    "# Build fully-conditioned model.\n",
    "s = 3000\n",
    "model_fit = partial(\n",
    "    pyro.condition(\n",
    "        model_gpu,\n",
    "        data={\n",
    "          # 'alpha_hyper': torch.tensor(100.).cuda(),\n",
    "          'alpha': torch.ones(n).cuda() * 100.,\n",
    "          'epsilon_hyper': torch.tensor(0.01).cuda(),\n",
    "          'pi_hyper': torch.tensor(1e-1 / s).cuda(),\n",
    "          'rho_hyper': torch.tensor(1e0).cuda(),\n",
    "#           'epsilon': torch.ones(n).cuda() * 0.001,\n",
    "#           'rho': torch.ones(s).cuda() / s,\n",
    "           'y': torch.tensor(y_obs).cuda(),\n",
    "        }\n",
    "    ),\n",
    "    s=s,\n",
    "    m=torch.tensor(m).cuda(),\n",
    "    gamma_hyper=torch.tensor(1e-2).cuda(),\n",
    "#     pi0=torch.tensor(1e-1).cuda(),\n",
    "#    rho0=torch.tensor(1e-1).cuda(),\n",
    "#    alpha0=torch.tensor(100.).cuda(),  # These two params have no effect IF we condition\n",
    "#    epsilon0=torch.tensor(0.01).cuda(),  #  on epsilon_hyper and alpha_hyper\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mapest, history = find_map(model_fit, learning_rate=1e-0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('test.pickle', 'wb') as f:\n",
    "    pickle.dump(mapest, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('test.pickle', 'rb') as f:\n",
    "    mapest = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_fit = pd.DataFrame(mapest['pi'], index=_data.library_id)\n",
    "gamma_fit = pd.DataFrame(mapest['gamma'], columns=_data.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.max(1).sort_values(ascending=False).values)\n",
    "plt.axhline(1.0, c='k', lw=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.max(0).sort_values(ascending=False).values)\n",
    "plt.axhline(1.0, c='k', lw=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.sum(0).sort_values(ascending=False).values)\n",
    "plt.plot((pi_fit > 0.15).sum(0).sort_values(ascending=False).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mapest['alpha'], bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mapest['epsilon'], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(pi_fit, metric='cosine', vmin=0, vmax=1, xticklabels=1, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(\n",
    "    (gamma_fit.T * 2) - 1,\n",
    "    metric='cosine',\n",
    "#     row_cluster=False, \n",
    "    cmap='coolwarm',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((pi_fit.T * m.mean(1)).sum(1), binary_entropy(gamma_fit).mean(1), s=1)\n",
    "plt.ylabel('strain-entropy')\n",
    "plt.xlabel('estimated-total-coverage')\n",
    "plt.xlim(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_diversity_samples = idxwhere(pi_fit.max(1).sort_values() > 0.99)\n",
    "high_diversity_samples = idxwhere(pi_fit.max(1).sort_values() < 0.75)\n",
    "\n",
    "len(low_diversity_samples), len(high_diversity_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library_id in low_diversity_samples[:3]:\n",
    "    d = data.sel(library_id=library_id).sum(['read'])\n",
    "    d = (d / d.sum('allele')).dropna('position').max('allele')\n",
    "    plt.hist(d, bins=np.linspace(0.5, 0.9999, num=21), density=True, alpha=0.2)\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_depth_variable_positions = ((major_allele_rcvrg > 0.0) & (major_allele_rcvrg < 1.0) & (depth > 2))\n",
    "plt.hist(high_depth_variable_positions.sum('position'))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_depth_variable_positions.sum('position').sel(library_id=low_diversity_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_depth_variable_positions.sum('position').sel(library_id=low_diversity_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(\n",
    "    allele_frac_hist[low_diversity_samples]**(1/5),\n",
    "    metric='cosine',\n",
    "    vmin=0, vmax=7,\n",
    "    row_cluster=False,\n",
    "    figsize=(len(low_diversity_samples)*0.004, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(\n",
    "    allele_frac_hist[high_diversity_samples]**(1/5),\n",
    "    metric='cosine',\n",
    "    vmin=0, vmax=7,\n",
    "    row_cluster=False,\n",
    "    figsize=(len(high_diversity_samples)*0.004, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_obs = y_obs.cpu().numpy() / m_\n",
    "frac_obs_ = frac_obs.copy()\n",
    "frac_obs_[np.isnan(frac_obs_)] = 0.5\n",
    "\n",
    "frac_expect = (mapest['p_noerr'].squeeze()) #* m.numpy()\n",
    "\n",
    "print(np.abs(((frac_obs_ - frac_expect) * m_)).sum().sum() / m_.sum())\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "#sns.heatmap(frac_obs[:,:], cmap='coolwarm', cbar=False, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_taxa = (pi_fit.max(0) < 0.01)\n",
    "drop_taxa.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gamma_fit.loc[drop_taxa].T, vmin=0, vmax=1, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build m, y matrices from data, summing over both reads.\n",
    "_data = data[suff_cvrg_samples, position_ss_out].astype('float32')\n",
    "m = _data.sum(['read', 'allele']).values\n",
    "n, g = m.shape\n",
    "y_obs = _data.sum('read').sel(allele='alt').values\n",
    "\n",
    "\n",
    "# Build fully-conditioned model.\n",
    "s = 3000\n",
    "model_out = partial(\n",
    "    pyro.condition(\n",
    "        model_gpu,\n",
    "        data={\n",
    "          # 'alpha_hyper': torch.tensor(100.).cuda(),\n",
    "          'alpha': torch.ones(n).cuda() * 100.,\n",
    "          'epsilon_hyper': torch.tensor(0.01).cuda(),\n",
    "          # 'pi_hyper': torch.tensor(1e-1 / s).cuda(),\n",
    "          # 'rho_hyper': torch.tensor(1e0).cuda(),\n",
    "#           'epsilon': torch.ones(n).cuda() * 0.001,\n",
    "#           'rho': torch.ones(s).cuda() / s,\n",
    "           'y': torch.tensor(y_obs).cuda(),\n",
    "\n",
    "           'pi': torch.tensor(mapest['pi']).cuda(),\n",
    "        }\n",
    "    ),\n",
    "    s=s,\n",
    "    m=torch.tensor(m).cuda(),\n",
    "    gamma_hyper=torch.tensor(1e-2).cuda(),\n",
    "#     pi0=torch.tensor(1e-1).cuda(),\n",
    "#    rho0=torch.tensor(1e-1).cuda(),\n",
    "#    alpha0=torch.tensor(100.).cuda(),  # These two params have no effect IF we condition\n",
    "#    epsilon0=torch.tensor(0.01).cuda(),  #  on epsilon_hyper and alpha_hyper\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapest_out, history = find_map(model_out, learning_rate=1e-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = data[suff_cvrg_samples, position_ss].astype('float32')\n",
    "m = _data.sum(['read', 'allele']).values\n",
    "n, g = m.shape\n",
    "y_obs = _data.sum('read').sel(allele='alt').values\n",
    "frac_expect = (mapest['p_noerr'].squeeze()) #* m.numpy()\n",
    "\n",
    "def residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    frac_obs[np.isnan(frac_obs)] = 0.5\n",
    "    return np.abs(((frac_obs_ - expect_frac) * m))\n",
    "\n",
    "residual_count(frac_expect, y_obs, m).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mapest_geno, history_geno = find_map(model_geno)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_loss_history(history_geno)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gamma_geno = pd.DataFrame(mapest_geno['gamma'], columns=_data.position) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(gamma_geno.loc[~drop_taxa].T, vmin=0, vmax=1, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_h = binary_entropy(pi_fit).sum(1)\n",
    "strain_h = binary_entropy(gamma_geno).mean(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter((pi_fit.T * m.mean(1)).sum(1), strain_h, s=1)\n",
    "plt.ylabel('strain-entropy')\n",
    "plt.xlabel('estimated-total-coverage')\n",
    "#plt.xlim(-1, 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(m.mean(1), sample_h, s=1)\n",
    "plt.ylabel('sample-entropy')\n",
    "plt.xlabel('sample-mean-coverage')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.hist(sample_h, bins=np.linspace(0, 10, num=50))\n",
    "None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.hist(strain_h, bins=np.linspace(0, 1, num=50))\n",
    "None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Build m, y matrices from data, summing over both reads.\n",
    "_data = data[:, :].astype('float32')\n",
    "m = torch.tensor(_data.sum(['read', 'allele']).values)\n",
    "n, g = m.shape\n",
    "y_obs = torch.tensor(_data.sum('read').sel(allele='alt').values)\n",
    "\n",
    "\n",
    "# Build fully-conditioned model.\n",
    "s = 1500\n",
    "model_frac = partial(\n",
    "    pyro.condition(\n",
    "        model,\n",
    "        data={\n",
    "#           'alpha_hyper': torch.tensor(300.),\n",
    "          'alpha': torch.ones(n) * 10.,\n",
    "          'epsilon_hyper': torch.tensor(0.01),\n",
    "          'pi_hyper': torch.tensor(1e-1 / s),\n",
    "          'rho_hyper': torch.tensor(1e0),\n",
    "#           'epsilon': torch.ones(n) * 0.001,\n",
    "#           'rho': torch.ones(s) / s,\n",
    "           'gamma': torch.tensor(mapest_geno['gamma']),\n",
    "           'y': y_obs,\n",
    "        }\n",
    "    ),\n",
    "    s=s,\n",
    "    m=m,\n",
    "#     gamma_hyper=torch.tensor(1e-0),\n",
    "#     pi0=torch.tensor(1e-1),\n",
    "#    rho0=torch.tensor(1e-1),\n",
    "#    alpha0=torch.tensor(100.),  # These two params have no effect IF we condition\n",
    "#    epsilon0=torch.tensor(0.01),  #  on epsilon_hyper and alpha_hyper\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mapest_frac, history_frac = find_map(model_frac)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}