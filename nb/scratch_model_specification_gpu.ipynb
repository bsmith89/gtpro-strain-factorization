{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lib.util import info, idxwhere\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "#import arviz as az\n",
    "from pyro.ops.contract import einsum\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def qqplot(x, y, **kwargs):\n",
    "    x = np.sort(x)\n",
    "    y = np.sort(y)\n",
    "    _min = min(x[0], y[0])\n",
    "    _max = max(x[-1], y[-1])\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    _kwargs = dict(marker='.', alpha=0.5)\n",
    "    _kwargs.update(kwargs)\n",
    "    ax.scatter(x, y, **_kwargs)\n",
    "    ax.plot([_min, _max], [_min, _max], lw=1, linestyle='--', color='k')\n",
    "    return ax\n",
    "\n",
    "def binary_entropy(p, normalize=False, axis=None):\n",
    "    q = 1 - p\n",
    "    ent = np.sum(-(p * np.log2(p) + q * np.log2(q)), axis=axis)\n",
    "    if normalize:\n",
    "        ent = ent / p.shape[axis]\n",
    "    return ent\n",
    "\n",
    "def binary_entropy_counts(y, m, normalize=False, axis=None):\n",
    "    p = ((y + 1) / (m + 2))\n",
    "    return binary_entropy(p, normalize=False, axis=axis)\n",
    "\n",
    "def entropy(p, axis=None):\n",
    "    ent = -(p * np.log(p))\n",
    "    return np.sum(ent, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    binary_entropy(np.array([[0.1, 0.9, 0.9], [0.1, 0.1, 0.1]]), axis=1, normalize=False),\n",
    "    binary_entropy(np.array([[0.1, 0.9, 0.9], [0.1, 0.1, 0.1]]), axis=0, normalize=False),\n",
    "    binary_entropy(np.array([[0.1, 0.9, 0.9], [0.1, 0.1, 0.1]]), axis=1, normalize=True),\n",
    "    binary_entropy(np.array([[0.1, 0.9, 0.9], [0.1, 0.1, 0.1]]), axis=0, normalize=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegativeBinomialReparam(mu, r):\n",
    "    p = 1 / ((r / mu) + 1)\n",
    "    return dist.NegativeBinomial(\n",
    "        total_count=r,\n",
    "        probs=p\n",
    "    )\n",
    "\n",
    "def as_torch(x, dtype=None, device=None):\n",
    "    # Cast inputs and set device\n",
    "    return torch.tensor(x, dtype=dtype, device=device)\n",
    "\n",
    "def as_torch_all(dtype=None, device=None, **kwargs):\n",
    "    # Cast inputs and set device\n",
    "    return {k: as_torch(kwargs[k], dtype=dtype, device=device) for k in kwargs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_torch(1.0, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model0: Dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model0(\n",
    "    n,\n",
    "    g,\n",
    "    s,\n",
    "    gamma_hyper=as_torch(1.),\n",
    "    rho_hyper=as_torch(1.),\n",
    "    pi_hyper=as_torch(1.),\n",
    "    m_hyper_mu=as_torch(10.),\n",
    "    m_hyper_r=as_torch(1.),\n",
    "    epsilon_hyper=as_torch(0.01),\n",
    "    alpha_hyper=as_torch(100.),\n",
    "    dtype=None,\n",
    "    device=None,\n",
    "):\n",
    "    \n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.Beta(gamma_hyper, gamma_hyper)\n",
    "            )\n",
    "    \n",
    "#     rho_ = pyro.sample('rho_', dist.LogNormal(0, 1 / rho_hyper).expand([s]).to_event())\n",
    "#     rho = pyro.deterministic('rho', rho_ / rho_.sum())\n",
    "    rho = pyro.sample('rho', dist.Dirichlet(torch.ones(s, dtype=dtype, device=device) * rho_hyper))\n",
    "    \n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        pi = pyro.sample('pi', dist.Dirichlet(rho * pi_hyper))\n",
    "        alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1)\n",
    "        \n",
    "    m = pyro.sample('m', NegativeBinomialReparam(m_hyper_mu, m_hyper_r).expand([n, g]).to_event())\n",
    "\n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ gamma)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "        \n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ).to_event(),\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, g, s = 1000, 500, 400\n",
    "\n",
    "model0_sim = partial(\n",
    "    pyro.condition(\n",
    "        model0,\n",
    "        data=as_torch_all(\n",
    "            # NOTHING HERE\n",
    "            dtype=torch.float32,\n",
    "            device=\"cuda\",\n",
    "        ),\n",
    "    ),\n",
    "    s=s,\n",
    "    g=g,\n",
    "    n=n,\n",
    "    **as_torch_all(\n",
    "        gamma_hyper=0.01,\n",
    "        pi_hyper=0.0005,\n",
    "        rho_hyper=1.,\n",
    "        m_hyper_mu=10.,\n",
    "        m_hyper_r=1.,\n",
    "        epsilon_hyper=0.01,\n",
    "        alpha_hyper=100.,\n",
    "        dtype=torch.float32,\n",
    "        device=\"cuda\",\n",
    "    ),\n",
    "    dtype=torch.float32,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "trace = pyro.poutine.trace(model0_sim).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim0 = pyro.infer.Predictive(model0_sim, num_samples=1)()\n",
    "sim0 = {k: sim0[k].detach().cpu().numpy().squeeze() for k in sim0.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim0['pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim0['pi'].max(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim0['pi'].max(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim0['rho']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim0['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0_fit = partial(\n",
    "    pyro.condition(\n",
    "        model0,\n",
    "        data=as_torch_all(\n",
    "            m=sim0['m'], y=sim0['y'],\n",
    "            dtype=torch.float32,\n",
    "            device=\"cuda\",\n",
    "        ),\n",
    "    ),\n",
    "    s=s,\n",
    "    g=g,\n",
    "    n=n,\n",
    "    **as_torch_all(\n",
    "        gamma_hyper=0.1,\n",
    "        pi_hyper=1.,\n",
    "        rho_hyper=1.,\n",
    "        m_hyper_mu=10.,  # Conditioned out\n",
    "        m_hyper_r=1.,  # Conditioned out\n",
    "        epsilon_hyper=0.01,\n",
    "        alpha_hyper=100.,\n",
    "        dtype=torch.float32,\n",
    "        device=\"cuda\",\n",
    "    ),\n",
    "    dtype=torch.float32,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "_guide = pyro.infer.autoguide.AutoLaplaceApproximation(model0_fit)\n",
    "# _guide = pyro.infer.autoguide.AutoNormal(model0_fit)\n",
    "\n",
    "opt = pyro.optim.Adamax({\"lr\": 1e-1}, {\"clip_norm\": 100.})\n",
    "svi = pyro.infer.SVI(\n",
    "    model0_fit,\n",
    "    _guide,\n",
    "    opt,\n",
    "    loss=pyro.infer.JitTrace_ELBO()\n",
    ")\n",
    "pyro.clear_param_store()\n",
    "\n",
    "n_iter = int(5e2)\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(n_iter))\n",
    "for i in pbar:\n",
    "    elbo = svi.step()\n",
    "    \n",
    "    if np.isnan(elbo):\n",
    "        break\n",
    "\n",
    "    # Fit tracking\n",
    "    history.append(elbo)\n",
    "    \n",
    "    # Reporting/Breaking\n",
    "    if (i % 1 == 0):\n",
    "        if i > 1:\n",
    "            pbar.set_postfix({\n",
    "                'ELBO': history[-1],\n",
    "                'delta': history[-2] - history[-1],\n",
    "#                 'pi_hyper': pi_hyper_fit,\n",
    "#                 'rho_hyper': rho_hyper_fit,\n",
    "#                 'gamma_hyper': gamma_hyper_fit,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est0 = pyro.infer.Predictive(model0_fit, guide=_guide, num_samples=100)()\n",
    "est0 = {k: est0[k].detach().cpu().numpy().mean(0).squeeze() for k in est0.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(est0['pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(est0['gamma'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim0['rho']), label='true_metacommunity')\n",
    "plt.plot(np.sort(sim0['pi'].mean(0)), label='true_mean')\n",
    "plt.plot(np.sort(est0['rho']), label='fit_metacommunity')\n",
    "plt.plot(np.sort(est0['pi'].mean(0)), label='fit_mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model1: Gumbel-Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(\n",
    "    n,\n",
    "    g,\n",
    "    s,\n",
    "    gamma_hyper=as_torch(1.),\n",
    "    rho_hyper=as_torch(1.),\n",
    "    pi_hyper=as_torch(1.),\n",
    "    m_hyper_mu=as_torch(10.),\n",
    "    m_hyper_r=as_torch(1.),\n",
    "    epsilon_hyper=as_torch(0.01),\n",
    "    alpha_hyper=as_torch(100.),\n",
    "    dtype=None,\n",
    "    device=None,\n",
    "):\n",
    "    \n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.RelaxedBernoulli(temperature=gamma_hyper, probs=torch.tensor(0.5, dtype=dtype, device=device))\n",
    "            )\n",
    "    \n",
    "#     rho_ = pyro.sample('rho_', dist.LogNormal(0, 1 / rho_hyper).expand([s]).to_event())\n",
    "#     rho = pyro.deterministic('rho', rho_ / rho_.sum())\n",
    "    rho = pyro.sample('rho', dist.RelaxedOneHotCategorical(temperature=rho_hyper, logits=torch.zeros(s, dtype=dtype, device=device)))\n",
    "    \n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        pi = pyro.sample('pi', dist.RelaxedOneHotCategorical(temperature=pi_hyper, probs=rho))\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1)\n",
    "        alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "\n",
    "        \n",
    "    m = pyro.sample('m', NegativeBinomialReparam(m_hyper_mu, m_hyper_r).expand([n, g]).to_event())\n",
    "\n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ gamma)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "        \n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ).to_event(),\n",
    "#         dist.Binomial(\n",
    "#             probs=p,\n",
    "#             total_count=m\n",
    "#         ).to_event(),\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_sim = partial(\n",
    "    pyro.condition(\n",
    "        model1,\n",
    "        data=as_torch_all(\n",
    "            # NOTHING HERE\n",
    "            dtype=torch.float32,\n",
    "            device=\"cuda\",\n",
    "        ),\n",
    "    ),\n",
    "    s=s,\n",
    "    g=g,\n",
    "    n=n,\n",
    "    **as_torch_all(\n",
    "        gamma_hyper=0.001,\n",
    "        pi_hyper=0.2,\n",
    "        rho_hyper=2.,\n",
    "        m_hyper_mu=10.,\n",
    "        m_hyper_r=1.,\n",
    "        epsilon_hyper=0.01,\n",
    "        alpha_hyper=100.,\n",
    "        dtype=torch.float32,\n",
    "        device=\"cuda\",\n",
    "    ),\n",
    "    dtype=torch.float32,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "trace = pyro.poutine.trace(model1_sim).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not run so the same simulated data is fit for each model.\n",
    "\n",
    "sim1 = pyro.infer.Predictive(model1_sim, num_samples=1)()\n",
    "sim1 = {k: sim1[k].detach().cpu().numpy().squeeze() for k in sim1.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim1['pi'].max(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['pi'].sum(0), sim1['pi'].max(0))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim1['pi'].max(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim1['rho']))\n",
    "plt.plot(np.sort(sim1['pi'].mean(0)))\n",
    "#plt.plot(np.sort(sim1['pi'].max(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sim1['pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(sim1['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(sim1['p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "p_obs = (sim1['y'] + 1) / (sim1['m'] + 2)\n",
    "\n",
    "genotype_score = p_obs * 2 - 1\n",
    "agg = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='complete', distance_threshold=0.05).fit(genotype_score)\n",
    "clust = pd.Series(agg.labels_)\n",
    "clust.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_total = pd.DataFrame(sim1['y']).groupby(clust).sum()\n",
    "m_total = pd.DataFrame(sim1['m']).groupby(clust).sum()\n",
    "clust_genotype = (y_total + 1) / (m_total + 2)\n",
    "\n",
    "additional_haplotypes = 100\n",
    "gamma_init = pd.concat([\n",
    "    clust_genotype, pd.DataFrame(np.ones((additional_haplotypes, clust_genotype.shape[1])) * 0.5)\n",
    "]).reset_index(drop=True)\n",
    "sns.clustermap(gamma_init)\n",
    "#sns.clustermap(clust_genotype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fit = gamma_init.shape[0]\n",
    "pi_init = np.ones((n, s_fit))\n",
    "for i in range(s_fit):\n",
    "    pi_init[i, clust[i]] = (s_fit - 1)\n",
    "pi_init /= pi_init.sum(1, keepdims=True)\n",
    "pi_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_fit = partial(\n",
    "    pyro.condition(\n",
    "        model1,\n",
    "        data=as_torch_all(\n",
    "            m=sim1['m'], y=sim1['y'],\n",
    "            dtype=torch.float32,\n",
    "            device=\"cuda\",\n",
    "        ),\n",
    "    ),\n",
    "    s=s_fit,\n",
    "    g=g,\n",
    "    n=n,\n",
    "    **as_torch_all(\n",
    "        gamma_hyper=1.,\n",
    "        pi_hyper=1.,\n",
    "        rho_hyper=1.,\n",
    "        m_hyper_mu=10.,  # Conditioned out\n",
    "        m_hyper_r=1.,  # Conditioned out\n",
    "        epsilon_hyper=0.01,\n",
    "        alpha_hyper=200.,\n",
    "        dtype=torch.float32,\n",
    "        device=\"cuda\",\n",
    "    ),\n",
    "    dtype=torch.float32,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "_guide = pyro.infer.autoguide.AutoLaplaceApproximation(\n",
    "    model1_fit,\n",
    "    init_loc_fn=pyro.infer.autoguide.initialization.init_to_value(\n",
    "        values={\n",
    "            'gamma': torch.tensor(gamma_init.values, dtype=torch.float32, device=\"cuda\"),\n",
    "            'pi': torch.tensor(pi_init, dtype=torch.float32, device=\"cuda\"),\n",
    "            'rho': torch.tensor(np.ones(s_fit) / s_fit, dtype=torch.float32, device=\"cuda\"),\n",
    "            'alpha': torch.tensor(200. * np.ones(n), dtype=torch.float32, device=\"cuda\"),\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "# _guide = pyro.infer.autoguide.AutoNormal(model1_fit)\n",
    "opt = pyro.optim.Adamax({\"lr\": 1e-1}, {\"clip_norm\": 100.})\n",
    "svi = pyro.infer.SVI(\n",
    "    model1_fit,\n",
    "    _guide,\n",
    "    opt,\n",
    "    loss=pyro.infer.JitTrace_ELBO()\n",
    ")\n",
    "pyro.clear_param_store()\n",
    "\n",
    "n_iter = int(2e3)\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_g = pyro.infer.autoguide.AutoLaplaceApproximation(model1_fit)\n",
    "\n",
    "trace = pyro.poutine.trace(_g).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(n_iter))\n",
    "for i in pbar:\n",
    "    elbo = svi.step()\n",
    "    \n",
    "    if np.isnan(elbo):\n",
    "        break\n",
    "\n",
    "    # Fit tracking\n",
    "    history.append(elbo)\n",
    "    \n",
    "    # Reporting/Breaking\n",
    "    if (i % 1 == 0):\n",
    "        if i > 1:\n",
    "            pbar.set_postfix({\n",
    "                'ELBO': history[-1],\n",
    "                'delta': history[-2] - history[-1],\n",
    "#                 'pi_hyper': pi_hyper_fit,\n",
    "#                 'rho_hyper': rho_hyper_fit,\n",
    "#                 'gamma_hyper': gamma_hyper_fit,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = pyro.infer.Predictive(model1_fit, guide=_guide, num_samples=1)()\n",
    "est1 = {k: est1[k].detach().cpu().numpy().mean(0).squeeze() for k in est1.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If alpha collapses to near 0 in the fit, then\n",
    "# the model needs to be re-adjusted.\n",
    "\n",
    "qqplot(sim1['alpha'], est1['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(sim1['epsilon'], est1['epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Why do we have accuracy collapse?\n",
    "\n",
    "plt.scatter(sim1['alpha'], sim1['epsilon'])\n",
    "plt.scatter(est1['alpha'], est1['epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gamma_init.values[0], est1['gamma'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pi_init[0], est1['pi'][0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Meta-community strain abundance.\n",
    "# If the fit is less even than the\n",
    "# simulation, we should increase\n",
    "# the tempurate of rho, merge\n",
    "# similar strains, or turn down the\n",
    "# temperature of gamma.\n",
    "\n",
    "# THIS ONE CANNOT BE COMPARED BETWEEN MODELS\n",
    "\n",
    "# Strain mean abundance across samples.\n",
    "# If the highest mean-abundance strains are\n",
    "# much more abundant in the fit than\n",
    "# the simulation, we should turn up the\n",
    "# temperature of rho, or down the temperature\n",
    "# of gamma, or merge similar strains.\n",
    "\n",
    "\n",
    "qqplot(sim1['rho'], est1['rho'], label='rho')\n",
    "qqplot(sim1['pi'].mean(0), est1['pi'].mean(0), label='pi')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Strain maximum abundance across samples.\n",
    "# If the fit has more strains with high maximum\n",
    "# abundance than the simulation, then we should\n",
    "# merge similar strains or turn up the temperature\n",
    "# of pi.\n",
    "\n",
    "# Dominant strain abundance in each sample.\n",
    "# If the fit is more of a \"cliff\" than the simulation,\n",
    "# then we should turn up the temperature of pi\n",
    "# or down the temperature of gamma.\n",
    "\n",
    "qqplot(sim1['pi'].max(0), est1['pi'].max(0), label='strains')\n",
    "qqplot(sim1['pi'].max(1), est1['pi'].max(1), label='samples')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_error = np.sqrt((((est1['p_noerr'] - sim1['p_noerr'])**2).mean(0)))\n",
    "fit_error = np.sqrt((((est1['p_noerr'] - sim1['p_noerr'])**2)).mean(1))\n",
    "observation_error = np.sqrt((((sim1['p_noerr'] - np.nan_to_num(sim1['y'] / sim1['m'], nan=0.5))**2).mean(1)))\n",
    "prediction_error = np.sqrt((((est1['p_noerr'] - np.nan_to_num(sim1['y'] / sim1['m'], nan=0.5))**2).mean(1)))\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 1, num=50)\n",
    "#plt.hist(position_error, bins=bins, label='position', alpha=0.75)\n",
    "plt.hist(fit_error, bins=bins, label='fit', alpha=0.5)\n",
    "plt.hist(observation_error, bins=bins, label='observation', alpha=0.5)\n",
    "plt.hist(prediction_error, bins=bins, label='prediction', alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize=(5, 10))\n",
    "art0 = axs[0].scatter(prediction_error, fit_error, c=sim1['pi'].max(1), marker='.')\n",
    "art1 = axs[1].scatter(prediction_error, fit_error, c=est1['pi'].max(1), marker='.')\n",
    "# art1 = axs[1].scatter(prediction_error, sample_error, c=est1['alpha'], marker='.')\n",
    "art2 = axs[2].scatter(prediction_error, fit_error, c=sim1['epsilon'], marker='.')\n",
    "fig.colorbar(art0, ax=axs[0])\n",
    "fig.colorbar(art1, ax=axs[1])\n",
    "fig.colorbar(art2, ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "import pandas as pd\n",
    "\n",
    "strain_dist = pd.DataFrame(cdist(sim1['gamma'] * 2 - 1, est1['gamma'] * 2 - 1, metric='cosine'))\n",
    "\n",
    "best_dist = strain_dist.min(1)\n",
    "\n",
    "plt.scatter(sim1['pi'].sum(0), best_dist, c=sim1['pi'].max(0), marker='.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=1e-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['pi'].max(1), est1['pi'].max(1), marker='.', c=sim1['epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.linspace(0, 1, num=50)\n",
    "# plt.hist(position_error, bins=bins, label='position', alpha=0.75)\n",
    "# plt.hist(sample_error, bins=bins, label='sample', alpha=0.75)\n",
    "# plt.legend()\n",
    "\n",
    "plt.scatter(observation_error, prediction_error, marker='.', alpha=0.5, c=sim1['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(est1['epsilon'], fit_error, marker='.', alpha=0.5, c=sim1['epsilon'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim1['pi'].max(1), fit_error, marker='.', alpha=0.5, c=est1['alpha'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(1 - pdist(est1['gamma'].T, metric='correlation'), bins=np.linspace(-1, 1, num=100))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(est1['pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(est1['gamma'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge close strains\n",
    "# TODO: How accurate are the strain estimates before/after merging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2: Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(\n",
    "    n,\n",
    "    g,\n",
    "    s,\n",
    "    gamma_hyper=as_torch(1.),\n",
    "    rho_hyper=as_torch(1.),\n",
    "    pi_hyper=as_torch(1.),\n",
    "    m_hyper_mu=as_torch(10.),\n",
    "    m_hyper_r=as_torch(1.),\n",
    "    epsilon_hyper=as_torch(0.01),\n",
    "    alpha_hyper=as_torch(100.),\n",
    "    dtype=None,\n",
    "    device=None,\n",
    "):\n",
    "    \n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.RelaxedBernoulli(temperature=gamma_hyper, logits=torch.tensor(0, dtype=dtype, device=device))\n",
    "            )\n",
    "    \n",
    "#     rho_ = pyro.sample('rho_', dist.LogNormal(0, 1 / rho_hyper).expand([s]).to_event())\n",
    "#     rho = pyro.deterministic('rho', rho_ / rho_.sum())\n",
    "#     rho = pyro.sample('rho', dist.Dirichlet(torch.ones(s, dtype=dtype, device=device) * rho_hyper))\n",
    "    rho = pyro.sample('rho', dist.RelaxedOneHotCategorical(temperature=rho_hyper, logits=torch.zeros(s, dtype=dtype, device=device)))\n",
    "    \n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "#         pi = pyro.sample('pi', dist.Dirichlet(rho * pi_hyper))\n",
    "        pi = pyro.sample('pi', dist.RelaxedOneHotCategorical(temperature=pi_hyper, probs=rho))\n",
    "        alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1)\n",
    "        \n",
    "    m = pyro.sample('m', NegativeBinomialReparam(m_hyper_mu, m_hyper_r).expand([n, g]).to_event())\n",
    "\n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ gamma)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "        \n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ).to_event(),\n",
    "    )\n",
    "    return y\n",
    "\n",
    "# def model2(\n",
    "#     n,\n",
    "#     g,\n",
    "#     s,\n",
    "#     gamma_hyper=as_torch(1.),\n",
    "#     rho_hyper=as_torch(1.),\n",
    "#     pi_hyper=as_torch(1.),\n",
    "#     m_hyper_mu=as_torch(10.),\n",
    "#     m_hyper_r=as_torch(1.),\n",
    "#     epsilon_hyper=as_torch(0.01),\n",
    "#     alpha_hyper=as_torch(100.),\n",
    "#     dtype=None,\n",
    "#     device=None,\n",
    "# ):\n",
    "    \n",
    "#     with pyro.plate('position', g, dim=-1):\n",
    "#         with pyro.plate('strain', s, dim=-2):\n",
    "#             gamma = pyro.sample(\n",
    "#                 'gamma', dist.RelaxedBernoulli(temperature=gamma_hyper, probs=torch.tensor(0.5, dtype=dtype, device=device))\n",
    "#             )\n",
    "    \n",
    "# #     rho_ = pyro.sample('rho_', dist.LogNormal(0, 1 / rho_hyper).expand([s]).to_event())\n",
    "# #     rho = pyro.deterministic('rho', rho_ / rho_.sum())\n",
    "#     rho = pyro.sample('rho', dist.RelaxedOneHotCategorical(temperature=rho_hyper, logits=torch.zeros(s, dtype=dtype, device=device)))\n",
    "    \n",
    "#     with pyro.plate('sample', n, dim=-1):\n",
    "#         pi = pyro.sample('pi', dist.RelaxedOneHotCategorical(temperature=pi_hyper, probs=rho))\n",
    "#         epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1)\n",
    "#         alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "\n",
    "        \n",
    "#     m = pyro.sample('m', NegativeBinomialReparam(m_hyper_mu, m_hyper_r).expand([n, g]).to_event())\n",
    "\n",
    "#     p_noerr = pyro.deterministic('p_noerr', pi @ gamma)\n",
    "#     p = pyro.deterministic('p',\n",
    "#         (1 - epsilon / 2) * (p_noerr) +\n",
    "#         (epsilon / 2) * (1 - p_noerr)\n",
    "#     )\n",
    "        \n",
    "#     y = pyro.sample(\n",
    "#         'y',\n",
    "#         dist.BetaBinomial(\n",
    "#             concentration1=alpha * p,\n",
    "#             concentration0=alpha * (1 - p),\n",
    "#             total_count=m\n",
    "#         ).to_event(),\n",
    "# #         dist.Binomial(\n",
    "# #             probs=p,\n",
    "# #             total_count=m\n",
    "# #         ).to_event(),\n",
    "#     )\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, g, s = 200, 500, 100\n",
    "\n",
    "model2_sim = partial(\n",
    "    pyro.condition(\n",
    "        model2,\n",
    "        data=as_torch_all(\n",
    "            # NOTHING HERE\n",
    "            dtype=torch.float32,\n",
    "            device=\"cuda\",\n",
    "        ),\n",
    "    ),\n",
    "    s=s,\n",
    "    g=g,\n",
    "    n=n,\n",
    "    **as_torch_all(\n",
    "        gamma_hyper=0.001,\n",
    "        pi_hyper=0.3,\n",
    "        rho_hyper=3.,\n",
    "        m_hyper_mu=2.,\n",
    "        m_hyper_r=10.,\n",
    "        epsilon_hyper=0.01,\n",
    "        alpha_hyper=100.,\n",
    "        dtype=torch.float32,\n",
    "        device=\"cuda\",\n",
    "    ),\n",
    "    dtype=torch.float32,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "trace = pyro.poutine.trace(model2_sim).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not run so the same simulated data is fit for each model.\n",
    "\n",
    "sim2 = pyro.infer.Predictive(model2_sim, num_samples=1)()\n",
    "sim2 = {k: sim2[k].detach().cpu().numpy().squeeze() for k in sim2.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim2['rho'] * n))\n",
    "plt.plot(np.sort(sim2['pi'].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim2['pi'].max(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim2['pi'].sum(0), sim2['pi'].max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim2['pi'].max(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim2['pi'].max(1), entropy(sim2['pi'], axis=1), marker='.', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(sim2['pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(sim2['gamma'] * 2 - 1, metric='cosine', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def genotype_distance(x, y):\n",
    "    x = x * 2 - 1\n",
    "    y = y * 2 - 1\n",
    "    weight = (x * y) ** 2\n",
    "    dist = ((x - y) / 2) ** 2\n",
    "    wmean_dist = (weight * dist).sum() / (weight.sum())\n",
    "    return wmean_dist\n",
    "\n",
    "p_obs = (sim2['y'] + 1) / (sim2['m'] + 2)\n",
    "sample_genotype_dist_matrix = pairwise_distances(p_obs, metric=genotype_distance, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "#genotype_score = p_obs * 2 - 1\n",
    "\n",
    "# print(\"Calculating distance matrix.\")\n",
    "# sample_genotype_dist_matrix = pdist(p_obs, metric=genotype_distance)\n",
    "# print(\"Done calculating distance matrix.\")\n",
    "# print(\"Clustering samples.\")\n",
    "agg = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage='complete', distance_threshold=0.05).fit(sample_genotype_dist_matrix)\n",
    "print(\"Done clustering samples.\")\n",
    "clust = pd.Series(agg.labels_)\n",
    "clust.value_counts()\n",
    "\n",
    "#sns.clustermap(genotype_score, metric='cosine', row_colors=mpl.cm.viridis(clust.values / clust.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_total = pd.DataFrame(sim2['y']).groupby(clust).sum()\n",
    "m_total = pd.DataFrame(sim2['m']).groupby(clust).sum()\n",
    "clust_genotype = (y_total + 1) / (m_total + 2)\n",
    "\n",
    "additional_haplotypes = 0  # clust_genotype.shape[0]  # Double the number of haplotypes (200% more) from clustering\n",
    "gamma_init = pd.concat([\n",
    "    clust_genotype, pd.DataFrame(np.ones((additional_haplotypes, clust_genotype.shape[1])) * 0.5)\n",
    "]).reset_index(drop=True)\n",
    "#sns.clustermap(gamma_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fit = gamma_init.shape[0]\n",
    "pi_init = np.ones((n, s_fit))\n",
    "for i in range(n):\n",
    "    pi_init[i, clust[i]] = (s_fit - 1)\n",
    "pi_init /= pi_init.sum(1, keepdims=True)\n",
    "\n",
    "#sns.clustermap(pi_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fit = g\n",
    "# s_fit = s\n",
    "\n",
    "model2_fit = partial(\n",
    "    pyro.condition(\n",
    "        model2,\n",
    "        data=as_torch_all(\n",
    "            m=sim2['m'][:, :g_fit], y=sim2['y'][:, :g_fit],\n",
    "#             alpha=np.ones(n)*1000,\n",
    "            dtype=torch.float32,\n",
    "            device=\"cuda\",\n",
    "        ),\n",
    "    ),\n",
    "    s=s_fit,\n",
    "    g=g_fit,\n",
    "    n=n,\n",
    "    **as_torch_all(\n",
    "#         # True values\n",
    "#         gamma_hyper=0.001,\n",
    "#         pi_hyper=0.3,\n",
    "#         rho_hyper=3.,\n",
    "        # Fitting values\n",
    "        gamma_hyper=0.01,\n",
    "        pi_hyper=0.5,\n",
    "        rho_hyper=0.5,\n",
    "        m_hyper_mu=10.,  # Conditioned out\n",
    "        m_hyper_r=1.,  # Conditioned out\n",
    "        epsilon_hyper=0.01,\n",
    "        alpha_hyper=100.,\n",
    "        dtype=torch.float32,\n",
    "        device=\"cuda\",\n",
    "    ),\n",
    "    dtype=torch.float32,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "eps_adjust_probs = lambda x, eps=1e-5: (x + eps) / (x + eps).sum(-1, keepdims=True)\n",
    "\n",
    "_guide = pyro.infer.autoguide.AutoLaplaceApproximation(\n",
    "    model2_fit,\n",
    "    init_loc_fn=pyro.infer.autoguide.initialization.init_to_value(\n",
    "        values={\n",
    "            # Smart-initialize\n",
    "            'gamma': torch.tensor(gamma_init.values[:, :g_fit], dtype=torch.float32, device=\"cuda\"),\n",
    "            'pi': torch.tensor(pi_init, dtype=torch.float32, device=\"cuda\"),\n",
    "            'rho': torch.tensor(np.ones(s_fit) / s_fit, dtype=torch.float32, device=\"cuda\"),\n",
    "            'alpha': torch.tensor(10. * np.ones(n), dtype=torch.float32, device=\"cuda\"),\n",
    "            'epsilon': torch.tensor(1e-1 * np.ones(n), dtype=torch.float32, device=\"cuda\"),\n",
    "            # True-initialize\n",
    "#             'gamma': torch.tensor(eps_adjust_probs(sim2['gamma']), dtype=torch.float32, device=\"cuda\"),\n",
    "#             'pi': torch.tensor(eps_adjust_probs(sim2['pi']), dtype=torch.float32, device=\"cuda\"),\n",
    "#             'rho': torch.tensor(eps_adjust_probs(sim2['rho']), dtype=torch.float32, device=\"cuda\"),\n",
    "#             'alpha': torch.tensor(sim2['alpha'], dtype=torch.float32, device=\"cuda\"),\n",
    "#             'epsilon': torch.tensor(sim2['epsilon'], dtype=torch.float32, device=\"cuda\"),\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "# _guide = pyro.infer.autoguide.AutoNormal(model2_fit)\n",
    "opt = pyro.optim.Adamax({\"lr\": 1e-0}, {\"clip_norm\": 100.})\n",
    "svi = pyro.infer.SVI(\n",
    "    model2_fit,\n",
    "    _guide,\n",
    "    opt,\n",
    "    loss=pyro.infer.JitTrace_ELBO()\n",
    ")\n",
    "pyro.clear_param_store()\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_g = pyro.infer.autoguide.AutoLaplaceApproximation(model2_fit)\n",
    "\n",
    "trace = pyro.poutine.trace(_g).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10000\n",
    "lag = 20\n",
    "\n",
    "try:\n",
    "    pbar = tqdm(range(n_iter))\n",
    "    for i in pbar:\n",
    "        elbo = svi.step()\n",
    "\n",
    "        if np.isnan(elbo):\n",
    "            break\n",
    "\n",
    "        # Fit tracking\n",
    "        history.append(elbo)\n",
    "\n",
    "        # Reporting/Breaking\n",
    "        if (i % 1 == 0):\n",
    "            if i > lag:\n",
    "                delta = history[-2] - history[-1]\n",
    "                delta_lag = (history[-lag] - history[-1]) / lag\n",
    "                if delta_lag < 0:\n",
    "                    print(\"Converged\")\n",
    "                    break\n",
    "                pbar.set_postfix({\n",
    "                    'ELBO': history[-1],\n",
    "                    'delta': delta,\n",
    "                    f'lag{lag}': delta_lag,\n",
    "                })\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "    pass\n",
    "finally:         \n",
    "    plt.plot(history)\n",
    "    est2 = pyro.infer.Predictive(model2_fit, guide=_guide, num_samples=1)()\n",
    "    est2 = {k: est2[k].detach().cpu().numpy().mean(0).squeeze() for k in est2.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If alpha collapses to near 0 in the fit, then\n",
    "# the model needs to be re-adjusted.\n",
    "\n",
    "qqplot(sim2['alpha'], est2['alpha'], marker='.', alpha=0.5, c=sim2['pi'].max(1), vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(sim2['rho'] * n)[::-1], label='true_rho', alpha=0.75)\n",
    "plt.plot(np.sort(sim2['pi'].sum(0))[::-1], label='true_sum', alpha=0.75)\n",
    "plt.plot(np.sort(est2['rho'] * n)[::-1], label='fit_rho', alpha=0.75)\n",
    "plt.plot(np.sort(est2['pi'].sum(0))[::-1], label='fit_sum', alpha=0.75)\n",
    "plt.axhline(0, color='k', lw=1, linestyle='--')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_error = np.sqrt((((est2['p_noerr'] - np.nan_to_num(sim2['y'][:,:g_fit] / sim2['m'][:,:g_fit], nan=0.5))**2).mean(1)))\n",
    "\n",
    "sample_mean_genotype_entropy = (est2['pi'] @ np.expand_dims(binary_entropy(est2['gamma'], axis=1, normalize=True), 1)).squeeze()\n",
    "plt.scatter(sample_mean_genotype_entropy, est2['alpha'], c=prediction_error, marker='.', alpha=0.5)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_sample = (sample_mean_genotype_entropy > 0.3)\n",
    "underfit_sample = (est2['alpha'] < 50)\n",
    "\n",
    "failed_samples = overfit_sample | underfit_sample\n",
    "print(overfit_sample.sum(), underfit_sample.sum(), failed_samples.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxwhere(pd.Series(failed_samples))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_error = np.sqrt((((est2['p_noerr'] - sim2['p_noerr'])**2).mean(0)))\n",
    "fit_error = np.sqrt((((est2['p_noerr'] - sim2['p_noerr'])**2)).mean(1))\n",
    "observation_error = np.sqrt((((sim2['p_noerr'] - np.nan_to_num(sim2['y'] / sim2['m'], nan=0.5))**2).mean(1)))\n",
    "observed_entropy = binary_entropy_counts(sim2['y'], sim2['m'], axis=1)\n",
    "\n",
    "bins = np.linspace(0, 1, num=50)\n",
    "#plt.hist(position_error, bins=bins, label='position', alpha=0.75)\n",
    "plt.hist(fit_error, bins=bins, label='fit', alpha=0.5)\n",
    "plt.hist(observation_error, bins=bins, label='observation', alpha=0.5)\n",
    "plt.hist(prediction_error, bins=bins, label='prediction', alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abundance_per_strain = sim2['pi'].max(0)\n",
    "max_strain_per_sample = sim2['pi'].argmax(1)\n",
    "max_abundance_per_sample = sim2['pi'].max(1)\n",
    "entropy_per_sample = entropy(sim2['pi'], axis=1)\n",
    "total_abundance_per_strain = sim2['pi'].sum(0)\n",
    "\n",
    "\n",
    "plt.scatter(total_abundance_per_strain[max_strain_per_sample], entropy_per_sample, marker='.', alpha=0.5, c=failed_samples)\n",
    "# CONCLUSION: Samples dominated by strains that are not abundant overall are fit much less well.\n",
    "# So are samples with high observation error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sample_mean_genotype_entropy, entropy(est2['pi'], axis=1), marker='.', c=failed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(observation_error, entropy_per_sample, marker='.', alpha=0.5, c=failed_samples)\n",
    "# Observation error counter-intuitivily decreased for \"failed\" samples\n",
    "# at high observation error (due to large epsilon or small alpha)\n",
    "# since the bad fit has less effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(entropy_per_sample, fit_error, marker='.', alpha=0.5, c=failed_samples)\n",
    "# Observation error counter-intuitivily decreased for \"failed\" samples\n",
    "# at high observation error (due to large epsilon or small alpha)\n",
    "# since the bad fit has less effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    total_abundance_per_strain[max_strain_per_sample],\n",
    "    max_abundance_per_strain[max_strain_per_sample],\n",
    "    marker='.',\n",
    "    alpha=0.5,\n",
    "    c=prediction_error\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.colorbar()\n",
    "# CONCLUSION: Badly fit samples are often dominated by low-total-abundance strains without any high abundance samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(entropy(sim2['pi'], axis=1), entropy(est2['pi'], axis=1), c=sample_mean_genotype_entropy, marker='.', alpha=0.5)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(est2['pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(est2['gamma'].T * 2 - 1, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(sim2['epsilon'], est2['epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Why do we have accuracy collapse?\n",
    "\n",
    "plt.scatter(sim2['alpha'], sim2['epsilon'], marker='.', alpha=0.5)\n",
    "plt.scatter(est2['alpha'], est2['epsilon'], marker='.', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gamma_init.values[0], est2['gamma'][0], marker='.', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pi_init[0], est2['pi'][0], marker='.', alpha=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Meta-community strain abundance.\n",
    "# If the fit is less even than the\n",
    "# simulation, we should increase\n",
    "# the tempurate of rho, merge\n",
    "# similar strains, or turn down the\n",
    "# temperature of gamma.\n",
    "\n",
    "# THIS ONE CANNOT BE COMPARED BETWEEN MODELS\n",
    "\n",
    "# Strain mean abundance across samples.\n",
    "# If the highest mean-abundance strains are\n",
    "# much more abundant in the fit than\n",
    "# the simulation, we should turn up the\n",
    "# temperature of rho, or down the temperature\n",
    "# of gamma, or merge similar strains.\n",
    "\n",
    "\n",
    "qqplot(sim2['rho'], est2['rho'], label='rho')\n",
    "qqplot(sim2['pi'].mean(0), est2['pi'].mean(0), label='pi')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Strain maximum abundance across samples.\n",
    "# If the fit has more strains with high maximum\n",
    "# abundance than the simulation, then we should\n",
    "# merge similar strains or turn up the temperature\n",
    "# of pi.\n",
    "\n",
    "# Dominant strain abundance in each sample.\n",
    "# If the fit is more of a \"cliff\" than the simulation,\n",
    "# then we should turn up the temperature of pi\n",
    "# or down the temperature of gamma.\n",
    "\n",
    "qqplot(sim2['pi'].max(0), est2['pi'].max(0), label='strains')\n",
    "qqplot(sim2['pi'].max(1), est2['pi'].max(1), label='samples')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, figsize=(5, 12))\n",
    "art0 = axs[0].scatter(prediction_error, fit_error, c=sim2['pi'].max(1), marker='.')\n",
    "art1 = axs[1].scatter(prediction_error, fit_error, c=est2['pi'].max(1), marker='.')\n",
    "# art1 = axs[1].scatter(prediction_error, sample_error, c=est2['alpha'], marker='.')\n",
    "art2 = axs[2].scatter(prediction_error, fit_error, c=sim2['epsilon'], marker='.')\n",
    "art3 = axs[3].scatter(prediction_error, fit_error, c=sample_mean_genotype_entropy, marker='.')\n",
    "art4 = axs[4].scatter(prediction_error, fit_error, c=failed_samples, marker='.')\n",
    "\n",
    "\n",
    "fig.colorbar(art0, ax=axs[0])\n",
    "fig.colorbar(art1, ax=axs[1])\n",
    "fig.colorbar(art2, ax=axs[2])\n",
    "fig.colorbar(art3, ax=axs[3])\n",
    "fig.colorbar(art4, ax=axs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of haplotype inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Fit to true\n",
    "strain_dist = pd.DataFrame(cdist(sim2['gamma'] * 2 - 1, est2['gamma'] * 2 - 1, metric='cosine'))\n",
    "best_match = strain_dist.idxmin(1)\n",
    "best_fit_dist = strain_dist.min(1)\n",
    "\n",
    "# Init to true\n",
    "strain_dist = pd.DataFrame(cdist(sim2['gamma'] * 2 - 1, gamma_init * 2 - 1, metric='cosine'))\n",
    "best_init_dist = strain_dist.min(1)\n",
    "\n",
    "strain_entropy = binary_entropy(est2['gamma'], normalize=True, axis=1)\n",
    "best_match_entropy = strain_entropy[best_match.values]\n",
    "\n",
    "plt.scatter(best_fit_dist, best_init_dist, c=best_match_entropy, marker='.', alpha=0.5, norm=mpl.colors.PowerNorm(1/5))\n",
    "plt.plot([0, 1], [0, 1], color='k', lw=1, linestyle='--')\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# plt.ylim(top=1e-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim2['pi'].sum(0), best_fit_dist, c=best_match_entropy, marker='.', alpha=0.5, norm=mpl.colors.PowerNorm(1/5))\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim2['pi'].max(1), est2['pi'].max(1), marker='.', c=failed_samples, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.linspace(0, 1, num=50)\n",
    "# plt.hist(position_error, bins=bins, label='position', alpha=0.75)\n",
    "# plt.hist(sample_error, bins=bins, label='sample', alpha=0.75)\n",
    "# plt.legend()\n",
    "\n",
    "plt.scatter(observation_error, prediction_error, marker='.', alpha=0.5, c=failed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim2['epsilon'], fit_error, marker='.', alpha=0.5, c=failed_samples)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sim2['pi'].max(1), fit_error, marker='.', alpha=0.5, c=failed_samples)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(1 - pdist(est2['gamma'].T, metric='correlation'), bins=np.linspace(-1, 1, num=100))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of composition inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sense of how accurate taxon calling is, despite possible permutations\n",
    "# in the fit relative to true.\n",
    "# When two samples have highly similar communities in reality,\n",
    "# they have highly similar communities in the fit.\n",
    "\n",
    "sns.jointplot(\n",
    "    pdist(sim2['pi'][~failed_samples]),\n",
    "    pdist(est2['pi'][~failed_samples]),\n",
    "    kind='hex',\n",
    "   norm=mpl.colors.PowerNorm(1/5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}