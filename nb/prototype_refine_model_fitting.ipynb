{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lib.util import info, idxwhere\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "import arviz as az\n",
    "from pyro.ops.contract import einsum\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "def rss(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def binary_entropy(p):\n",
    "    q = (1 - p)\n",
    "    return -p * np.log2(p) - q * np.log2(q)\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    min_loss = loss_history.min()\n",
    "    plt.plot(loss_history - min_loss)\n",
    "    plt.plot(\n",
    "        np.linspace(0, len(loss_history), num=1000),\n",
    "        np.linspace(len(loss_history), 0, num=1000),\n",
    "        lw=1, linestyle='--', color='grey'\n",
    "        )\n",
    "    plt.title(f'+{min_loss}')\n",
    "    plt.yscale('log')\n",
    "    return plt.gca()\n",
    "\n",
    "def mean_residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    out = np.abs(((frac_obs - expect_frac)))\n",
    "    out[np.isnan(out)] = 0\n",
    "    return (out * m).sum() / m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(\n",
    "    s,\n",
    "    m,\n",
    "    y=None,\n",
    "    gamma_hyper=1.,\n",
    "    pi0=1.,\n",
    "    rho0=1.,\n",
    "    epsilon0=0.01,\n",
    "    alpha0=1000.,\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "):\n",
    "    \n",
    "    # Cast inputs and set device\n",
    "    m, gamma_hyper, pi0, rho0, epsilon0, alpha0 = [\n",
    "        torch.tensor(v, dtype=dtype, device=device)\n",
    "        for v in [m, gamma_hyper, pi0, rho0, epsilon0, alpha0]\n",
    "    ]\n",
    "    if y is not None:\n",
    "        y = torch.tensor(y)\n",
    "    \n",
    "    n, g = m.shape\n",
    "    \n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.Beta(gamma_hyper, gamma_hyper)\n",
    "            )\n",
    "    # gamma.shape == (s, g)\n",
    "    \n",
    "    rho_hyper = pyro.sample('rho_hyper', dist.Gamma(rho0, 1.))\n",
    "    rho = pyro.sample('rho', dist.Dirichlet(torch.ones(s, dtype=dtype, device=device) * rho_hyper))\n",
    "    \n",
    "    epsilon_hyper = pyro.sample('epsilon_hyper', dist.Beta(1., 1 / epsilon0))\n",
    "    alpha_hyper = pyro.sample('alpha_hyper', dist.Gamma(alpha0, 1.))\n",
    "    pi_hyper = pyro.sample('pi_hyper', dist.Gamma(pi0, 1.))\n",
    "    \n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        pi = pyro.sample('pi', dist.Dirichlet(rho * s * pi_hyper))\n",
    "        alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1) \n",
    "    # pi.shape == (n, s)\n",
    "    # alpha.shape == epsilon.shape == (n,)\n",
    "    \n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ gamma)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    # p.shape == (n, g)\n",
    "\n",
    "        \n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ),\n",
    "        obs=y\n",
    "    )\n",
    "    # y.shape == (n, g)\n",
    "    return y\n",
    "\n",
    "def conditioned_model(\n",
    "    model,\n",
    "    data={},\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "    **kwargs,\n",
    "):\n",
    "    data = {\n",
    "        k: torch.tensor(v, dtype=dtype, device=device)\n",
    "        for k, v in data.items()\n",
    "    }\n",
    "    return partial(\n",
    "        pyro.condition(\n",
    "            model,\n",
    "            data=data\n",
    "        ),\n",
    "        dtype=dtype, device=device,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "def find_map(\n",
    "    model,\n",
    "    lag=10,\n",
    "    stop_at=1.0,\n",
    "    max_iter=int(1e5),\n",
    "    learning_rate = 1e-0,\n",
    "    clip_norm=100.,\n",
    "):\n",
    "    guide = pyro.infer.autoguide.AutoLaplaceApproximation(model)\n",
    "    svi = pyro.infer.SVI(\n",
    "        model,\n",
    "        guide,\n",
    "        pyro.optim.Adamax(\n",
    "            optim_args={\"lr\": learning_rate},\n",
    "            clip_args={\"clip_norm\": clip_norm}\n",
    "        ),\n",
    "        loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "    \n",
    "    pyro.clear_param_store()\n",
    "    pbar = tqdm(range(max_iter), position=0, leave=True)\n",
    "    history = []\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                break\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if (i % 1 == 0):\n",
    "                if i < 2:\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                    })\n",
    "                elif i < lag + 1:\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta_1': history[-2] - history[-1],\n",
    "                    })\n",
    "                else:\n",
    "                    delta_lag = (history[-lag] - history[-1]) / lag\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta_1': history[-2] - history[-1],\n",
    "                        f'delta_{lag}': delta_lag\n",
    "                    })\n",
    "                    if delta_lag < stop_at:\n",
    "                        info('Optimization converged')\n",
    "                        break\n",
    "    except KeyboardInterrupt:\n",
    "        info('Optimization interrupted')\n",
    "    pbar.refresh()\n",
    "    \n",
    "    # Gather MAP from parameter-store\n",
    "    mapest = {\n",
    "        k: v.detach().cpu().numpy().squeeze()\n",
    "        for k, v\n",
    "        in pyro.infer.Predictive(\n",
    "            model, guide=guide, num_samples=1\n",
    "        )().items()\n",
    "    }\n",
    "    return mapest, np.array(history)\n",
    "\n",
    "  \n",
    "def mean_residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    out = np.abs(((frac_obs - expect_frac)))\n",
    "    out[np.isnan(out)] = 0\n",
    "    return (out * m).sum() / m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataarray('data/core/100022/gtpro.nc').squeeze().sum('read')\n",
    "data.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_allele_incid = (data > 0).mean('library_id').min('allele')\n",
    "\n",
    "thresh = 0.01\n",
    "\n",
    "plt.hist(minor_allele_incid, bins=100)\n",
    "plt.axvline(thresh, lw=1, linestyle='--', c='k')\n",
    "\n",
    "informative_positions = idxwhere(minor_allele_incid.to_series() > thresh)\n",
    "\n",
    "print(len(informative_positions), (minor_allele_incid.to_series() > thresh).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Samples with >5% of informative positions covered\n",
    "suff_cvrg_samples = (data.sel(position=informative_positions).sum(['allele']) > 0).mean('position') > 0.05\n",
    "npos = 2000\n",
    "npos_out = 2000\n",
    "position_ss_ = np.random.choice(\n",
    "    informative_positions,\n",
    "    size=npos + npos_out,\n",
    "    replace=False\n",
    "    )\n",
    "position_ss, position_ss_out = position_ss_[:npos], position_ss_[npos:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build m, y matrices from data.\n",
    "_data = data.sel(library_id=suff_cvrg_samples, position=position_ss)\n",
    "m = _data.sum('allele')\n",
    "n, g = m.shape\n",
    "y_obs = _data.sel(allele='alt')\n",
    "\n",
    "s = 3000\n",
    "model_fit = conditioned_model(\n",
    "    model,\n",
    "    data=dict(\n",
    "        alpha=np.ones(n) * 100,\n",
    "        epsilon_hyper=0.01,\n",
    "        pi_hyper=1e-1 / s,\n",
    "        rho_hyper=1e0,\n",
    "        y=y_obs.values,\n",
    "    ),\n",
    "    s=s,\n",
    "    m=m.values,\n",
    "    gamma_hyper=1e-2,\n",
    "    dtype=torch.float32,\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapest, history = find_map(model_fit, lag=10, stop_at=10., learning_rate=2e-1, max_iter=int(1e4), clip_norm=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 'scripts/strain_facts.py'\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from lib.util import info, idxwhere\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "import arviz as az\n",
    "from pyro.ops.contract import einsum\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "import argparse\n",
    "\n",
    "def rss(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def binary_entropy(p):\n",
    "    q = (1 - p)\n",
    "    return -p * np.log2(p) - q * np.log2(q)\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    min_loss = loss_history.min()\n",
    "    plt.plot(loss_history - min_loss)\n",
    "    plt.plot(\n",
    "        np.linspace(0, len(loss_history), num=1000),\n",
    "        np.linspace(len(loss_history), 0, num=1000),\n",
    "        lw=1, linestyle='--', color='grey'\n",
    "        )\n",
    "    plt.title(f'+{min_loss}')\n",
    "    plt.yscale('log')\n",
    "    return plt.gca()\n",
    "\n",
    "def mean_residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    out = np.abs(((frac_obs - expect_frac)))\n",
    "    out[np.isnan(out)] = 0\n",
    "    return (out * m).sum() / m.sum()\n",
    "\n",
    "def model(\n",
    "    s,\n",
    "    m,\n",
    "    y=None,\n",
    "    gamma_hyper=1.,\n",
    "    pi0=1.,\n",
    "    rho0=1.,\n",
    "    epsilon0=0.01,\n",
    "    alpha0=1000.,\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "):\n",
    "    \n",
    "    # Cast inputs and set device\n",
    "    m, gamma_hyper, pi0, rho0, epsilon0, alpha0 = [\n",
    "        torch.tensor(v, dtype=dtype, device=device)\n",
    "        for v in [m, gamma_hyper, pi0, rho0, epsilon0, alpha0]\n",
    "    ]\n",
    "    if y is not None:\n",
    "        y = torch.tensor(y)\n",
    "    \n",
    "    n, g = m.shape\n",
    "    \n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.Beta(gamma_hyper, gamma_hyper)\n",
    "            )\n",
    "    # gamma.shape == (s, g)\n",
    "    \n",
    "    rho_hyper = pyro.sample('rho_hyper', dist.Gamma(rho0, 1.))\n",
    "    rho = pyro.sample('rho', dist.Dirichlet(torch.ones(s, dtype=dtype, device=device) * rho_hyper))\n",
    "    \n",
    "    epsilon_hyper = pyro.sample('epsilon_hyper', dist.Beta(1., 1 / epsilon0))\n",
    "    alpha_hyper = pyro.sample('alpha_hyper', dist.Gamma(alpha0, 1.))\n",
    "    pi_hyper = pyro.sample('pi_hyper', dist.Gamma(pi0, 1.))\n",
    "    \n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        pi = pyro.sample('pi', dist.Dirichlet(rho * s * pi_hyper))\n",
    "        alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1) \n",
    "    # pi.shape == (n, s)\n",
    "    # alpha.shape == epsilon.shape == (n,)\n",
    "    \n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ gamma)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    # p.shape == (n, g)\n",
    "\n",
    "        \n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ),\n",
    "        obs=y\n",
    "    )\n",
    "    # y.shape == (n, g)\n",
    "    return y\n",
    "\n",
    "def conditioned_model(\n",
    "    model,\n",
    "    data={},\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "    **kwargs,\n",
    "):\n",
    "    data = {\n",
    "        k: torch.tensor(v, dtype=dtype, device=device)\n",
    "        for k, v in data.items()\n",
    "    }\n",
    "    return partial(\n",
    "        pyro.condition(\n",
    "            model,\n",
    "            data=data\n",
    "        ),\n",
    "        dtype=dtype, device=device,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "def find_map(\n",
    "    model,\n",
    "    lag=10,\n",
    "    stop_at=1.0,\n",
    "    max_iter=int(1e5),\n",
    "    learning_rate = 1e-0,\n",
    "    clip_norm=100.,\n",
    "):\n",
    "    guide = pyro.infer.autoguide.AutoLaplaceApproximation(model)\n",
    "    svi = pyro.infer.SVI(\n",
    "        model,\n",
    "        guide,\n",
    "        pyro.optim.Adamax(\n",
    "            optim_args={\"lr\": learning_rate},\n",
    "            clip_args={\"clip_norm\": clip_norm}\n",
    "        ),\n",
    "        loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "    \n",
    "    pyro.clear_param_store()\n",
    "    pbar = tqdm(range(max_iter), position=0, leave=True)\n",
    "    history = []\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                break\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if (i % 1 == 0):\n",
    "                if i < 2:\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                    })\n",
    "                elif i < lag + 1:\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta_1': history[-2] - history[-1],\n",
    "                    })\n",
    "                else:\n",
    "                    delta_lag = (history[-lag] - history[-1]) / lag\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta_1': history[-2] - history[-1],\n",
    "                        f'delta_{lag}': delta_lag\n",
    "                    })\n",
    "                    if delta_lag < stop_at:\n",
    "                        info('Optimization converged')\n",
    "                        break\n",
    "    except KeyboardInterrupt:\n",
    "        info('Optimization interrupted')\n",
    "    pbar.refresh()\n",
    "    \n",
    "    # Gather MAP from parameter-store\n",
    "    mapest = {\n",
    "        k: v.detach().cpu().numpy().squeeze()\n",
    "        for k, v\n",
    "        in pyro.infer.Predictive(\n",
    "            model, guide=guide, num_samples=1\n",
    "        )().items()\n",
    "    }\n",
    "    return mapest, np.array(history)\n",
    "\n",
    "  \n",
    "def mean_residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    out = np.abs(((frac_obs - expect_frac)))\n",
    "    out[np.isnan(out)] = 0\n",
    "    return (out * m).sum() / m.sum()\n",
    "\n",
    "\n",
    "def parse_args(argv):\n",
    "    p = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    # Input\n",
    "    p.add_argument(\n",
    "        \"pileup\",\n",
    "        help=\"\"\"\n",
    "Single, fully processed, pileup table in NetCDF format with the following dimensions:\n",
    "    * library_id\n",
    "    * position\n",
    "    * read\n",
    "    * allele\n",
    "                        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Shape of the model\n",
    "    p.add_argument(\"--nstrains\", metavar=\"INT\", type=int, default=1000)\n",
    "    p.add_argument(\n",
    "        \"--npos\",\n",
    "        metavar=\"INT\",\n",
    "        default=2000,\n",
    "        type=int,\n",
    "        help=(\"Number of positions to sample for model fitting.\"),\n",
    "    )\n",
    "    \n",
    "    # Data filtering\n",
    "    p.add_argument(\"--incid-thresh\",\n",
    "                   metavar=\"FLOAT\",\n",
    "                   type=float,\n",
    "                   default=0.02,\n",
    "                   help=(\"Minimum fraction of samples that must have the minor allele \"\n",
    "                         \"for the position to be considered 'informative'.\"),\n",
    "    )\n",
    "    p.add_argument(\"--cvrg-thresh\",\n",
    "                   metavar=\"FLOAT\",\n",
    "                   type=float,\n",
    "                   default=0.5,\n",
    "                   help=(\"Minimum fraction of 'informative' positions with counts \"\n",
    "                         \"necessary for sample to be included.\"),\n",
    "    )\n",
    "\n",
    "    # Regularization\n",
    "    p.add_argument(\n",
    "        \"--pi-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e-1,\n",
    "        type=float,\n",
    "        help=(\"Heterogeneity regularization parameter (will be scaled by 1 / s).\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--rho-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e0,\n",
    "        type=float,\n",
    "        help=(\"Diversity regularization parameter.\"),\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--gamma-hyper\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=1e-2,\n",
    "        type=float,\n",
    "        help=(\"Ambiguity regularization parameter.\"),\n",
    "    )\n",
    "    p.add_argument(\"--epsilon-hyper\", metavar=\"FLOAT\", default=0.01, type=float)\n",
    "    p.add_argument(\n",
    "        \"--alpha\",\n",
    "        metavar=\"FLOAT\",\n",
    "        default=100.,\n",
    "        type=float,\n",
    "        help=('Concentration parameter of BetaBinomial observation.'),\n",
    "    )\n",
    "\n",
    "    # Fitting\n",
    "    p.add_argument(\"--random-seed\", default=0, type=int, help=(\"TODO\"))\n",
    "    p.add_argument(\"--max-iter\", default=10000, type=int, help=(\"TODO\"))\n",
    "    p.add_argument(\"--lag\", default=50, type=int, help=(\"TODO\"))\n",
    "    p.add_argument(\"--stop-at\", default=5., type=float, help=(\"TODO\"))\n",
    "    p.add_argument(\"--learning-rate\", default=1e-0, type=float, help=(\"TODO\"))\n",
    "    p.add_argument(\"--clip-norm\", default=100., type=float, help=(\"TODO\"))\n",
    "\n",
    "    # Hardware\n",
    "    p.add_argument(\"--device\", default='cpu', help=(\"PyTorch device name.\"))\n",
    "\n",
    "    # Output\n",
    "    p.add_argument(\n",
    "        \"--outpath\",\n",
    "        metavar=\"PATH\",\n",
    "        help=(\"Path for genotype fraction output.\"),\n",
    "    )\n",
    "    \n",
    "    args = p.parse_args(argv)\n",
    "    \n",
    "    if args.outpath == None:\n",
    "        args.outpath = args.pileup + '_strain-facts.nc'\n",
    "    \n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     warnings.filterwarnings(\n",
    "#         \"ignore\", category=UserWarning, module=\"pymc3.sampling\", lineno=566\n",
    "#     )\n",
    "\n",
    "    args = parse_args(sys.argv[1:])\n",
    "    info(args)\n",
    "\n",
    "    info(f'Setting random seed: {args.random_seed}')\n",
    "    np.random.seed(args.random_seed)\n",
    "    \n",
    "    info('Loading input data.')\n",
    "    data = xr.open_dataarray(args.pileup).squeeze()\n",
    "    info(f'Input data shape: {data.sizes}.')\n",
    "    data = data.sum('read')\n",
    "    \n",
    "    info('Filtering positions.')\n",
    "    minor_allele_incid = (data > 0).mean('library_id').min('allele')\n",
    "    informative_positions = idxwhere(\n",
    "        minor_allele_incid.to_series() > args.incid_thresh\n",
    "    )\n",
    "    npos_available = len(informative_positions)\n",
    "    info(f'Found {npos_available} informative positions with minor '\n",
    "         f'allele incidence of >{args.incid_thresh}')\n",
    "    npos = min(args.npos, npos_available)\n",
    "    info(f'Randomly sampling {npos} positions.')\n",
    "    position_ss = np.random.choice(\n",
    "        informative_positions,\n",
    "        size=npos,\n",
    "        replace=False,\n",
    "        )\n",
    "    \n",
    "    info('Filtering libraries.')\n",
    "    suff_cvrg_samples = idxwhere(\n",
    "        ((data.sel(position=informative_positions).sum(['allele']) > 0)\n",
    "        .mean('position') > args.cvrg_thresh).to_series()\n",
    "    )\n",
    "    nlibs = len(suff_cvrg_samples)\n",
    "    info(f'Found {nlibs} libraries with >{args.cvrg_thresh:0.1%} '\n",
    "         f'of informative positions covered.')\n",
    "    \n",
    "    info('Building conditioned model.')\n",
    "    data_fit = data.sel(library_id=suff_cvrg_samples, position=position_ss)\n",
    "    m = data_fit.sum('allele')\n",
    "    n, g = m.shape\n",
    "    y_obs = data_fit.sel(allele='alt')\n",
    "    s = args.nstrains\n",
    "    model_fit = conditioned_model(\n",
    "        model,\n",
    "        data=dict(\n",
    "            alpha=np.ones(n) * args.alpha,\n",
    "            epsilon_hyper=args.epsilon_hyper,\n",
    "            pi_hyper=args.pi_hyper / s,\n",
    "            rho_hyper=args.rho_hyper,\n",
    "            y=y_obs.values,\n",
    "        ),\n",
    "        s=s,\n",
    "        m=m.values,\n",
    "        gamma_hyper=args.gamma_hyper,\n",
    "        dtype=torch.float32,\n",
    "        device=args.device,\n",
    "    )\n",
    "    \n",
    "    info('Fitting model.')\n",
    "    mapest, history = find_map(\n",
    "        model_fit,\n",
    "        lag=args.lag,\n",
    "        stop_at=args.stop_at,\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_iter=args.max_iter,\n",
    "        clip_norm=args.clip_norm,\n",
    "    )\n",
    "\n",
    "    result = xr.Dataset(\n",
    "        {\n",
    "            'gamma': (['strain', 'position'], mapest['gamma']),\n",
    "            'rho': (['strain'], mapest['rho']),\n",
    "            'alpha_hyper': ([], mapest['alpha_hyper']),\n",
    "            'pi': (['library_id', 'strain'], mapest['pi']),\n",
    "            'epsilon': (['library_id'], mapest['epsilon']),\n",
    "            'rho_hyper': ([], mapest['rho_hyper']),\n",
    "            'epsilon_hyper': ([], mapest['epsilon_hyper']),\n",
    "            'pi_hyper': ([], mapest['pi_hyper']),\n",
    "            'alpha': (['library_id'], mapest['alpha']),\n",
    "            'p_noerr': (['library_id', 'position'], mapest['p_noerr']),\n",
    "            'p': (['library_id', 'position'], mapest['p']),\n",
    "            'y': (['library_id', 'position'], y_obs),\n",
    "            'm': (['library_id', 'position'], m),\n",
    "            'elbo_trace': (['iteration'], history),\n",
    "        },\n",
    "        coords=dict(strain=np.arange(s), position=data_fit.position, library_id=data_fit.library_id),\n",
    "    )\n",
    "    \n",
    "    result.to_netcdf(\n",
    "        args.outpath,\n",
    "        encoding=dict(\n",
    "            gamma=dict(dtype='float32', zlib=True, complevel=6),\n",
    "            pi=dict(dtype='float32', zlib=True, complevel=6),\n",
    "            p_noerr=dict(dtype='float32', zlib=True, complevel=6),\n",
    "            p=dict(dtype='float32', zlib=True, complevel=6),\n",
    "            y=dict(dtype='uint16', zlib=True, complevel=6),\n",
    "            m=dict(dtype='uint16', zlib=True, complevel=6),\n",
    "            elbo_trace=dict(dtype='float32', zlib=True, complevel=6),\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/strain_facts.py --device cuda gtpro.nc --learning-rate 2e-1 --stop-at 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = xr.load_dataset('gtpro.nc_strain-facts.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_fit = pd.DataFrame(mapest['pi'], index=_data.library_id)\n",
    "gamma_fit = pd.DataFrame(mapest['gamma'], columns=_data.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.max(1).sort_values(ascending=False).values)\n",
    "plt.axhline(1.0, c='k', lw=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.max(0).sort_values(ascending=False).values)\n",
    "plt.axhline(1.0, c='k', lw=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.sum(0).sort_values(ascending=False).values)\n",
    "plt.plot((pi_fit > 0.15).sum(0).sort_values(ascending=False).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mapest['alpha'], bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mapest['epsilon'], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(pi_fit, metric='cosine', vmin=0, vmax=1, xticklabels=1, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(\n",
    "    (gamma_fit.T * 2) - 1,\n",
    "    metric='cosine',\n",
    "#     row_cluster=False, \n",
    "    cmap='coolwarm',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((pi_fit.T * m.mean('position')).sum(1), binary_entropy(gamma_fit).mean(1), s=1)\n",
    "plt.ylabel('strain-entropy')\n",
    "plt.xlabel('estimated-total-coverage')\n",
    "plt.xlim(-1, 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "major_allele_rcvrg = (data.max('allele') / data.sum('allele')).fillna(0)\n",
    "per_sample_major_allele_mean_coverage = (data.max('allele') / data.sum('allele')).mean('library_id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(minor_allele_incid, per_sample_major_allele_mean_coverage, s=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bins = np.concatenate([[0], np.linspace(0.5, 1, num=21)])\n",
    "allele_frac_hist = major_allele_rcvrg.to_pandas().T.apply(lambda x: np.histogram(x, bins=bins)[0]).set_index(bins[:-1]).rename_axis(index='bin_low')\n",
    "\n",
    "# sns.clustermap(\n",
    "#     allele_frac_hist**(1/5),\n",
    "#     metric='cosine',\n",
    "#     vmin=0, vmax=7,\n",
    "#     row_cluster=False,\n",
    "#     figsize=(20, 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cvrg_samples = idxwhere(((data.sel(position=informative_positions).sum(['allele']) > 0).mean('position') > 0.5).to_series())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_diversity_samples = idxwhere(pi_fit.max(1).loc[high_cvrg_samples].sort_values() > 0.98)\n",
    "high_diversity_samples = idxwhere(pi_fit.max(1).loc[high_cvrg_samples].sort_values() < 0.5)\n",
    "\n",
    "len(low_diversity_samples), len(high_diversity_samples)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(\n",
    "    allele_frac_hist[low_diversity_samples]**(1/5),\n",
    "    metric='cosine',\n",
    "    vmin=0, vmax=7,\n",
    "    row_cluster=False,\n",
    "    figsize=(20, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(\n",
    "    allele_frac_hist[high_diversity_samples]**(1/5),\n",
    "    metric='cosine',\n",
    "    vmin=0, vmax=7,\n",
    "    row_cluster=False,\n",
    "    figsize=(20, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, sharey=True, sharex=True)\n",
    "\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "high_coverage_libraries_sorted_by_max_strain_fraction = pi_fit.max(1).loc[high_cvrg_samples].sort_values().index\n",
    "\n",
    "\n",
    "for library_id in high_coverage_libraries_sorted_by_max_strain_fraction[:5]:\n",
    "    d = data.sel(library_id=library_id)\n",
    "    d = (d / d.sum('allele')).dropna('position').max('allele')\n",
    "    axs[0].hist(d, bins=np.linspace(0.5, 0.999, num=11), density=False, alpha=0.5, color='black')\n",
    "    \n",
    "for library_id in high_coverage_libraries_sorted_by_max_strain_fraction[-5:]:\n",
    "    d = data.sel(library_id=library_id)\n",
    "    d = (d / d.sum('allele')).dropna('position').max('allele')\n",
    "    axs[1].hist(d, bins=np.linspace(0.5, 0.999, num=11), density=False, alpha=0.5, color='black')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_obs = y_obs.numpy() / m.numpy()\n",
    "frac_obs_ = frac_obs.copy()\n",
    "frac_obs_[np.isnan(frac_obs_)] = 0.5\n",
    "\n",
    "frac_expect = (mapest['p_noerr'].squeeze()) #* m.numpy()\n",
    "\n",
    "print(np.abs(((frac_obs_ - frac_expect) * m.numpy())).sum().sum() / m.numpy().sum())\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "#sns.heatmap(frac_obs[:,:], cmap='coolwarm', cbar=False, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_taxa = (pi_fit.max(0) < 0.01)\n",
    "drop_taxa.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gamma_fit.loc[drop_taxa].T, vmin=0, vmax=1, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build m, y matrices from data, summing over both reads.\n",
    "_data = data[high_cvrg_samples, :].astype('float32')\n",
    "m = torch.tensor(_data.sum(['read', 'allele']).values)\n",
    "n, g = m.shape\n",
    "y_obs = torch.tensor(_data.sum('read').sel(allele='alt').values)\n",
    "\n",
    "\n",
    "# Build fully-conditioned model.\n",
    "s = 1500\n",
    "model_geno = partial(\n",
    "    pyro.condition(\n",
    "        model,\n",
    "        data={\n",
    "#           'alpha_hyper': torch.tensor(300.),\n",
    "          'alpha': torch.ones(n) * 10.,\n",
    "          'epsilon_hyper': torch.tensor(0.01),\n",
    "#           'pi_hyper': torch.tensor(1e-1 / s),\n",
    "#           'rho_hyper': torch.tensor(1e0),\n",
    "#           'epsilon': torch.ones(n) * 0.001,\n",
    "#           'rho': torch.ones(s) / s,\n",
    "           'pi': torch.tensor(mapest['pi']),\n",
    "           'y': y_obs,\n",
    "        }\n",
    "    ),\n",
    "    s=s,\n",
    "    m=m,\n",
    "    gamma_hyper=torch.tensor(1e-0),\n",
    "#     pi0=torch.tensor(1e-1),\n",
    "#    rho0=torch.tensor(1e-1),\n",
    "#    alpha0=torch.tensor(100.),  # These two params have no effect IF we condition\n",
    "#    epsilon0=torch.tensor(0.01),  #  on epsilon_hyper and alpha_hyper\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapest_geno, history_geno = find_map(model_geno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(history_geno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_geno = pd.DataFrame(mapest_geno['gamma'], columns=_data.position) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gamma_geno.loc[~drop_taxa].T, vmin=0, vmax=1, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_h = binary_entropy(pi_fit).sum(1)\n",
    "strain_h = binary_entropy(gamma_geno).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((pi_fit.T * m.mean(1)).sum(1), strain_h, s=1)\n",
    "plt.ylabel('strain-entropy')\n",
    "plt.xlabel('estimated-total-coverage')\n",
    "#plt.xlim(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(m.mean(1), sample_h, s=1)\n",
    "plt.ylabel('sample-entropy')\n",
    "plt.xlabel('sample-mean-coverage')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_h, bins=np.linspace(0, 10, num=50))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(strain_h, bins=np.linspace(0, 1, num=50))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build m, y matrices from data, summing over both reads.\n",
    "_data = data[:, :].astype('float32')\n",
    "m = torch.tensor(_data.sum(['read', 'allele']).values)\n",
    "n, g = m.shape\n",
    "y_obs = torch.tensor(_data.sum('read').sel(allele='alt').values)\n",
    "\n",
    "\n",
    "# Build fully-conditioned model.\n",
    "s = 1500\n",
    "model_frac = partial(\n",
    "    pyro.condition(\n",
    "        model,\n",
    "        data={\n",
    "#           'alpha_hyper': torch.tensor(300.),\n",
    "          'alpha': torch.ones(n) * 10.,\n",
    "          'epsilon_hyper': torch.tensor(0.01),\n",
    "          'pi_hyper': torch.tensor(1e-1 / s),\n",
    "          'rho_hyper': torch.tensor(1e0),\n",
    "#           'epsilon': torch.ones(n) * 0.001,\n",
    "#           'rho': torch.ones(s) / s,\n",
    "           'gamma': torch.tensor(mapest_geno['gamma']),\n",
    "           'y': y_obs,\n",
    "        }\n",
    "    ),\n",
    "    s=s,\n",
    "    m=m,\n",
    "#     gamma_hyper=torch.tensor(1e-0),\n",
    "#     pi0=torch.tensor(1e-1),\n",
    "#    rho0=torch.tensor(1e-1),\n",
    "#    alpha0=torch.tensor(100.),  # These two params have no effect IF we condition\n",
    "#    epsilon0=torch.tensor(0.01),  #  on epsilon_hyper and alpha_hyper\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapest_frac, history_frac = find_map(model_frac)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import pickle\n",
    "# with open('test.pickle', 'rb') as f:\n",
    "#     mapest4 = pickle.load(f)\n",
    "    \n",
    "(mapest4['y'] > 0).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}