{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lib.util import info, idxwhere\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from functools import partial\n",
    "import arviz as az\n",
    "from pyro.ops.contract import einsum\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "def rss(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def binary_entropy(p):\n",
    "    q = (1 - p)\n",
    "    return -p * np.log2(p) - q * np.log2(q)\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    min_loss = loss_history.min()\n",
    "    plt.plot(loss_history - min_loss)\n",
    "    plt.title(f'+{min_loss}')\n",
    "    plt.yscale('log')\n",
    "    return plt.gca()\n",
    "\n",
    "def mean_residual_count(expect_frac, obs_count, m):\n",
    "    frac_obs = obs_count / m\n",
    "    out = np.abs(((frac_obs - expect_frac)))\n",
    "    out[np.isnan(out)] = 0\n",
    "    return (out * m).sum() / m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(\n",
    "    s,\n",
    "    m,\n",
    "    y=None,\n",
    "    gamma_hyper=1.,\n",
    "    pi0=1.,\n",
    "    rho0=1.,\n",
    "    epsilon0=0.01,\n",
    "    alpha0=1000.,\n",
    "    dtype='float32',\n",
    "    device='cpu',\n",
    "):\n",
    "    \n",
    "    m, gamma_hyper, pi0, rho0, epsilon0, alpha0 = [\n",
    "        torch.tensor(v, dtype=dtype, device=device)\n",
    "        for v in [m, gamma_hyper, pi0, rho0, epsilon0, alpha0]\n",
    "    ]\n",
    "    if y is not None:\n",
    "        y = torch.tensor(y)\n",
    "    \n",
    "    n, g = m.shape\n",
    "    \n",
    "    with pyro.plate('position', g, dim=-1):\n",
    "        with pyro.plate('strain', s, dim=-2):\n",
    "            gamma = pyro.sample(\n",
    "                'gamma', dist.Beta(gamma_hyper, gamma_hyper)\n",
    "            )\n",
    "    # gamma.shape == (s, g)\n",
    "    \n",
    "    rho_hyper = pyro.sample('rho_hyper', dist.Gamma(rho0, 1.))\n",
    "    rho = pyro.sample('rho', dist.Dirichlet(torch.ones(s, dtype=dtype, device=device) * rho_hyper))\n",
    "    \n",
    "    epsilon_hyper = pyro.sample('epsilon_hyper', dist.Beta(1., 1 / epsilon0))\n",
    "    alpha_hyper = pyro.sample('alpha_hyper', dist.Gamma(alpha0, 1.))\n",
    "    pi_hyper = pyro.sample('pi_hyper', dist.Gamma(pi0, 1.))\n",
    "    \n",
    "    with pyro.plate('sample', n, dim=-1):\n",
    "        pi = pyro.sample('pi', dist.Dirichlet(rho * s * pi_hyper))\n",
    "        alpha = pyro.sample('alpha', dist.Gamma(alpha_hyper, 1.)).unsqueeze(-1)\n",
    "        epsilon = pyro.sample('epsilon', dist.Beta(1., 1 / epsilon_hyper)).unsqueeze(-1) \n",
    "    # pi.shape == (n, s)\n",
    "    # alpha.shape == epsilon.shape == (n,)\n",
    "\n",
    "    p_noerr = pyro.deterministic('p_noerr', pi @ gamma)\n",
    "    p = pyro.deterministic('p',\n",
    "        (1 - epsilon / 2) * (p_noerr) +\n",
    "        (epsilon / 2) * (1 - p_noerr)\n",
    "    )\n",
    "    # p.shape == (n, g)\n",
    "\n",
    "        \n",
    "    y = pyro.sample(\n",
    "        'y',\n",
    "        dist.BetaBinomial(\n",
    "            concentration1=alpha * p,\n",
    "            concentration0=alpha * (1 - p),\n",
    "            total_count=m\n",
    "        ),\n",
    "        obs=y\n",
    "    )\n",
    "    # y.shape == (n, g)\n",
    "    return y\n",
    "\n",
    "def conditioned_model(\n",
    "    model,\n",
    "    model_params={},\n",
    "    data={},\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    "    **kwargs,\n",
    "):\n",
    "    model_params = {\n",
    "        k: torch.tensor(v, dtype=dtype, device=device)\n",
    "        for k, v in model_params.items()\n",
    "    }\n",
    "    data = {\n",
    "        k: torch.tensor(v, dtype=dtype, device=device)\n",
    "        for k, v in data.items()\n",
    "    }\n",
    "    return partial(\n",
    "        pyro.condition(\n",
    "            model,\n",
    "            data=data\n",
    "        ),\n",
    "        **model_params,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "def find_map(\n",
    "    model,\n",
    "    max_iter=int(1e5),\n",
    "    learning_rate = 1e-0,\n",
    "):\n",
    "    guide = pyro.infer.autoguide.AutoLaplaceApproximation(model)\n",
    "    svi = pyro.infer.SVI(\n",
    "        model,\n",
    "        guide,\n",
    "        pyro.optim.Adamax(\n",
    "            optim_args={\"lr\": learning_rate},\n",
    "            clip_args={\"clip_norm\": 100.}\n",
    "        ),\n",
    "        loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "    \n",
    "    pyro.clear_param_store()\n",
    "    pbar = tqdm(range(max_iter))\n",
    "    history = []\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            elbo = svi.step()\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                break\n",
    "\n",
    "            # Fit tracking\n",
    "            history.append(elbo)\n",
    "\n",
    "            # Reporting/Breaking\n",
    "            if (i % 1 == 0):\n",
    "                if i > 1:\n",
    "                    pbar.set_postfix({\n",
    "                        'ELBO': history[-1],\n",
    "                        'delta': history[-2] - history[-1]\n",
    "                    })\n",
    "    except KeyboardInterrupt:\n",
    "        info('Optimization interrupted')\n",
    "    pbar.refresh()\n",
    "    \n",
    "    # Gather MAP from parameter-store\n",
    "    mapest = {\n",
    "        k: v.detach().numpy().squeeze()\n",
    "        for k, v\n",
    "        in pyro.infer.Predictive(\n",
    "            model, guide=guide, num_samples=1\n",
    "        )().items()\n",
    "    }\n",
    "    return mapest, np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataarray('data/core/102506/gtpro.nc').squeeze().sum('read')\n",
    "data.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_allele_incid = (data > 0).sum('library_id').min('allele')\n",
    "\n",
    "thresh = 1000\n",
    "\n",
    "plt.hist(minor_allele_incid, bins=100)\n",
    "plt.axvline(thresh, lw=1, linestyle='--', c='k')\n",
    "\n",
    "informative_positions = idxwhere(minor_allele_incid.to_series() > thresh)\n",
    "\n",
    "print(len(informative_positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Samples with >25% of positions covered\n",
    "suff_cvrg_samples = (data.sel(position=informative_positions).sum(['allele']) > 0).mean('position') > 0.25\n",
    "npos = 1000\n",
    "npos_out = 1000\n",
    "position_ss_ = np.random.choice(\n",
    "    informative_positions,\n",
    "    size=npos + npos_out,\n",
    "    replace=False\n",
    "    )\n",
    "position_ss, position_ss_out = position_ss_[:npos], position_ss_[npos:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build m, y matrices from data.\n",
    "_data = data.sel(library_id=suff_cvrg_samples, position=position_ss)\n",
    "m = _data.sum('allele').values\n",
    "n, g = m\n",
    "y_obs = _data.sel(allele='alt')\n",
    "\n",
    "s = 3000\n",
    "model_fit = conditioned_model(\n",
    "    model,\n",
    "    model_params=dict(m=m.values, gamma_hyper=1e-2),\n",
    "    data=dict(\n",
    "        alpha=np.ones(n) * 100,\n",
    "        epsilon_hyper=0.01,\n",
    "        pi_hyper=1e-1 / s,\n",
    "        rho_hyper=1e0,\n",
    "        y=y_obs.values,\n",
    "    ),\n",
    "    s=s,\n",
    "    dtype=torch.float32,\n",
    "    device='cpu',\n",
    ")\n",
    "\n",
    "trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapest, history = find_map(model_fit, learning_rate=5e-1, max_iter=int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_fit = pd.DataFrame(mapest['pi'], index=_data.library_id)\n",
    "gamma_fit = pd.DataFrame(mapest['gamma'], columns=_data.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.max(1).sort_values(ascending=False).values)\n",
    "plt.axhline(1.0, c='k', lw=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.max(0).sort_values(ascending=False).values)\n",
    "plt.axhline(1.0, c='k', lw=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi_fit.sum(0).sort_values(ascending=False).values)\n",
    "plt.plot((pi_fit > 0.15).sum(0).sort_values(ascending=False).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mapest['alpha'], bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mapest['epsilon'], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(pi_fit, metric='cosine', vmin=0, vmax=1, xticklabels=1, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.clustermap(\n",
    "    (gamma_fit.T * 2) - 1,\n",
    "    metric='cosine',\n",
    "#     row_cluster=False, \n",
    "    cmap='coolwarm',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((pi_fit.T * m.mean(1)).sum(1), binary_entropy(gamma_fit).mean(1), s=1)\n",
    "plt.ylabel('strain-entropy')\n",
    "plt.xlabel('estimated-total-coverage')\n",
    "plt.xlim(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_allele_rcvrg = (data.max('allele') / data.sum('allele')).fillna(0)\n",
    "per_sample_major_allele_mean_coverage = (data.max('allele') / data.sum('allele')).mean('library_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(minor_allele_incid, per_sample_major_allele_mean_coverage, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.concatenate([[0], np.linspace(0.5, 1, num=21)])\n",
    "allele_frac_hist = major_allele_rcvrg.to_pandas().T.apply(lambda x: np.histogram(x, bins=bins)[0]).set_index(bins[:-1]).rename_axis(index='bin_low')\n",
    "\n",
    "# sns.clustermap(\n",
    "#     allele_frac_hist**(1/5),\n",
    "#     metric='cosine',\n",
    "#     vmin=0, vmax=7,\n",
    "#     row_cluster=False,\n",
    "#     figsize=(20, 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_diversity_samples = idxwhere(pi_fit.max(1).sort_values() > 0.98)\n",
    "high_diversity_samples = idxwhere(pi_fit.max(1).sort_values() < 0.5)\n",
    "\n",
    "len(low_diversity_samples), len(high_diversity_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(\n",
    "    allele_frac_hist[low_diversity_samples]**(1/5),\n",
    "    metric='cosine',\n",
    "    vmin=0, vmax=7,\n",
    "    row_cluster=False,\n",
    "    figsize=(20, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(\n",
    "    allele_frac_hist[high_diversity_samples]**(1/5),\n",
    "    metric='cosine',\n",
    "    vmin=0, vmax=7,\n",
    "    row_cluster=False,\n",
    "    figsize=(20, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, sharey=True, sharex=True)\n",
    "\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "\n",
    "for library_id in high_diversity_samples[:100]:\n",
    "    d = data.sel(library_id=library_id)\n",
    "    d = (d / d.sum('allele')).dropna('position').max('allele')\n",
    "    axs[0].hist(d, bins=np.linspace(0.5, 0.999, num=11), density=False, alpha=0.005, color='black')\n",
    "    \n",
    "for library_id in low_diversity_samples[:100]:\n",
    "    d = data.sel(library_id=library_id)\n",
    "    d = (d / d.sum('allele')).dropna('position').max('allele')\n",
    "    axs[1].hist(d, bins=np.linspace(0.5, 0.999, num=11), density=False, alpha=0.005, color='black')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_obs = y_obs.numpy() / m.numpy()\n",
    "frac_obs_ = frac_obs.copy()\n",
    "frac_obs_[np.isnan(frac_obs_)] = 0.5\n",
    "\n",
    "frac_expect = (mapest['p_noerr'].squeeze()) #* m.numpy()\n",
    "\n",
    "print(np.abs(((frac_obs_ - frac_expect) * m.numpy())).sum().sum() / m.numpy().sum())\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "#sns.heatmap(frac_obs[:,:], cmap='coolwarm', cbar=False, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_taxa = (pi_fit.max(0) < 0.01)\n",
    "drop_taxa.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gamma_fit.loc[drop_taxa].T, vmin=0, vmax=1, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build m, y matrices from data, summing over both reads.\n",
    "_data = data[high_cvrg_samples, :].astype('float32')\n",
    "m = torch.tensor(_data.sum(['read', 'allele']).values)\n",
    "n, g = m.shape\n",
    "y_obs = torch.tensor(_data.sum('read').sel(allele='alt').values)\n",
    "\n",
    "\n",
    "# Build fully-conditioned model.\n",
    "s = 1500\n",
    "model_geno = partial(\n",
    "    pyro.condition(\n",
    "        model,\n",
    "        data={\n",
    "#           'alpha_hyper': torch.tensor(300.),\n",
    "          'alpha': torch.ones(n) * 10.,\n",
    "          'epsilon_hyper': torch.tensor(0.01),\n",
    "#           'pi_hyper': torch.tensor(1e-1 / s),\n",
    "#           'rho_hyper': torch.tensor(1e0),\n",
    "#           'epsilon': torch.ones(n) * 0.001,\n",
    "#           'rho': torch.ones(s) / s,\n",
    "           'pi': torch.tensor(mapest['pi']),\n",
    "           'y': y_obs,\n",
    "        }\n",
    "    ),\n",
    "    s=s,\n",
    "    m=m,\n",
    "    gamma_hyper=torch.tensor(1e-0),\n",
    "#     pi0=torch.tensor(1e-1),\n",
    "#    rho0=torch.tensor(1e-1),\n",
    "#    alpha0=torch.tensor(100.),  # These two params have no effect IF we condition\n",
    "#    epsilon0=torch.tensor(0.01),  #  on epsilon_hyper and alpha_hyper\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapest_geno, history_geno = find_map(model_geno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(history_geno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_geno = pd.DataFrame(mapest_geno['gamma'], columns=_data.position) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gamma_geno.loc[~drop_taxa].T, vmin=0, vmax=1, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_h = binary_entropy(pi_fit).sum(1)\n",
    "strain_h = binary_entropy(gamma_geno).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((pi_fit.T * m.mean(1)).sum(1), strain_h, s=1)\n",
    "plt.ylabel('strain-entropy')\n",
    "plt.xlabel('estimated-total-coverage')\n",
    "#plt.xlim(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(m.mean(1), sample_h, s=1)\n",
    "plt.ylabel('sample-entropy')\n",
    "plt.xlabel('sample-mean-coverage')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_h, bins=np.linspace(0, 10, num=50))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(strain_h, bins=np.linspace(0, 1, num=50))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build m, y matrices from data, summing over both reads.\n",
    "_data = data[:, :].astype('float32')\n",
    "m = torch.tensor(_data.sum(['read', 'allele']).values)\n",
    "n, g = m.shape\n",
    "y_obs = torch.tensor(_data.sum('read').sel(allele='alt').values)\n",
    "\n",
    "\n",
    "# Build fully-conditioned model.\n",
    "s = 1500\n",
    "model_frac = partial(\n",
    "    pyro.condition(\n",
    "        model,\n",
    "        data={\n",
    "#           'alpha_hyper': torch.tensor(300.),\n",
    "          'alpha': torch.ones(n) * 10.,\n",
    "          'epsilon_hyper': torch.tensor(0.01),\n",
    "          'pi_hyper': torch.tensor(1e-1 / s),\n",
    "          'rho_hyper': torch.tensor(1e0),\n",
    "#           'epsilon': torch.ones(n) * 0.001,\n",
    "#           'rho': torch.ones(s) / s,\n",
    "           'gamma': torch.tensor(mapest_geno['gamma']),\n",
    "           'y': y_obs,\n",
    "        }\n",
    "    ),\n",
    "    s=s,\n",
    "    m=m,\n",
    "#     gamma_hyper=torch.tensor(1e-0),\n",
    "#     pi0=torch.tensor(1e-1),\n",
    "#    rho0=torch.tensor(1e-1),\n",
    "#    alpha0=torch.tensor(100.),  # These two params have no effect IF we condition\n",
    "#    epsilon0=torch.tensor(0.01),  #  on epsilon_hyper and alpha_hyper\n",
    ")\n",
    "\n",
    "# trace = pyro.poutine.trace(model_fit).get_trace()\n",
    "# trace.compute_log_prob()\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapest_frac, history_frac = find_map(model_frac)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import pickle\n",
    "# with open('test.pickle', 'rb') as f:\n",
    "#     mapest4 = pickle.load(f)\n",
    "    \n",
    "(mapest4['y'] > 0).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}